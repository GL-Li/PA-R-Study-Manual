<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 11 Introduction to modeling | Predictive Analytics for Actuaries</title>
  <meta name="description" content=" 11 Introduction to modeling | Predictive Analytics for Actuaries" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content=" 11 Introduction to modeling | Predictive Analytics for Actuaries" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 11 Introduction to modeling | Predictive Analytics for Actuaries" />
  
  
  

<meta name="author" content="Sam Castillo" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="images/artificial_actuary_logo_favicon.png" type="image/x-icon" />
<link rel="prev" href="visualization.html"/>
<link rel="next" href="generalized-linear-models-glms.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Exam PA Study Manual</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="join-the-online-course.html"><a href="join-the-online-course.html"><i class="fa fa-check"></i><b>1</b> Join the online course</a></li>
<li class="chapter" data-level="2" data-path="how-to-use-this-book.html"><a href="how-to-use-this-book.html"><i class="fa fa-check"></i><b>2</b> How to use this book</a></li>
<li class="chapter" data-level="3" data-path="how-to-contribute-to-this-book.html"><a href="how-to-contribute-to-this-book.html"><i class="fa fa-check"></i><b>3</b> How to Contribute to this book</a></li>
<li class="chapter" data-level="4" data-path="the-exam.html"><a href="the-exam.html"><i class="fa fa-check"></i><b>4</b> The exam</a></li>
<li class="chapter" data-level="5" data-path="prometric-demo.html"><a href="prometric-demo.html"><i class="fa fa-check"></i><b>5</b> Prometric Demo</a></li>
<li class="chapter" data-level="6" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>6</b> Introduction</a></li>
<li class="chapter" data-level="7" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>7</b> Getting started</a><ul>
<li class="chapter" data-level="7.1" data-path="getting-started.html"><a href="getting-started.html#download-the-data"><i class="fa fa-check"></i><b>7.1</b> Download the data</a></li>
<li class="chapter" data-level="7.2" data-path="getting-started.html"><a href="getting-started.html#download-islr"><i class="fa fa-check"></i><b>7.2</b> Download ISLR</a></li>
<li class="chapter" data-level="7.3" data-path="getting-started.html"><a href="getting-started.html#new-users"><i class="fa fa-check"></i><b>7.3</b> New users</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="r-programming.html"><a href="r-programming.html"><i class="fa fa-check"></i><b>8</b> R programming</a><ul>
<li class="chapter" data-level="8.1" data-path="r-programming.html"><a href="r-programming.html#notebook-chunks"><i class="fa fa-check"></i><b>8.1</b> Notebook chunks</a></li>
<li class="chapter" data-level="8.2" data-path="r-programming.html"><a href="r-programming.html#basic-operations"><i class="fa fa-check"></i><b>8.2</b> Basic operations</a></li>
<li class="chapter" data-level="8.3" data-path="r-programming.html"><a href="r-programming.html#lists"><i class="fa fa-check"></i><b>8.3</b> Lists</a></li>
<li class="chapter" data-level="8.4" data-path="r-programming.html"><a href="r-programming.html#functions"><i class="fa fa-check"></i><b>8.4</b> Functions</a></li>
<li class="chapter" data-level="8.5" data-path="r-programming.html"><a href="r-programming.html#data-frames"><i class="fa fa-check"></i><b>8.5</b> Data frames</a></li>
<li class="chapter" data-level="8.6" data-path="r-programming.html"><a href="r-programming.html#pipes"><i class="fa fa-check"></i><b>8.6</b> Pipes</a></li>
<li class="chapter" data-level="8.7" data-path="r-programming.html"><a href="r-programming.html#the-soas-code-doesnt-use-pipes-or-dplyr-so-can-i-skip-learning-this"><i class="fa fa-check"></i><b>8.7</b> The SOA’s code doesn’t use pipes or dplyr, so can I skip learning this?</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="data-manipulation.html"><a href="data-manipulation.html"><i class="fa fa-check"></i><b>9</b> Data manipulation</a><ul>
<li class="chapter" data-level="9.1" data-path="data-manipulation.html"><a href="data-manipulation.html#look-at-the-data"><i class="fa fa-check"></i><b>9.1</b> Look at the data</a></li>
<li class="chapter" data-level="9.2" data-path="data-manipulation.html"><a href="data-manipulation.html#transform-the-data"><i class="fa fa-check"></i><b>9.2</b> Transform the data</a></li>
<li class="chapter" data-level="9.3" data-path="data-manipulation.html"><a href="data-manipulation.html#exercises"><i class="fa fa-check"></i><b>9.3</b> Exercises</a></li>
<li class="chapter" data-level="9.4" data-path="data-manipulation.html"><a href="data-manipulation.html#answers-to-exercises"><i class="fa fa-check"></i><b>9.4</b> Answers to exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i><b>10</b> Visualization</a><ul>
<li class="chapter" data-level="10.1" data-path="visualization.html"><a href="visualization.html#create-a-plot-object-ggplot"><i class="fa fa-check"></i><b>10.1</b> Create a plot object (ggplot)</a></li>
<li class="chapter" data-level="10.2" data-path="visualization.html"><a href="visualization.html#add-a-plot"><i class="fa fa-check"></i><b>10.2</b> Add a plot</a></li>
<li class="chapter" data-level="10.3" data-path="visualization.html"><a href="visualization.html#data-manipulation-chaining"><i class="fa fa-check"></i><b>10.3</b> Data manipulation chaining</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html"><i class="fa fa-check"></i><b>11</b> Introduction to modeling</a><ul>
<li class="chapter" data-level="11.1" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#modeling-vocabulary"><i class="fa fa-check"></i><b>11.1</b> Modeling vocabulary</a></li>
<li class="chapter" data-level="11.2" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#modeling-notation"><i class="fa fa-check"></i><b>11.2</b> Modeling notation</a></li>
<li class="chapter" data-level="11.3" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>11.3</b> Ordinary Least Squares (OLS)</a></li>
<li class="chapter" data-level="11.4" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#regression-vs.-classification"><i class="fa fa-check"></i><b>11.4</b> Regression vs. classification</a></li>
<li class="chapter" data-level="11.5" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#regression-metrics"><i class="fa fa-check"></i><b>11.5</b> Regression metrics</a></li>
<li class="chapter" data-level="11.6" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#example"><i class="fa fa-check"></i><b>11.6</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html"><i class="fa fa-check"></i><b>12</b> Generalized linear Models (GLMs)</a><ul>
<li class="chapter" data-level="12.0.1" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#assumptions-of-ols"><i class="fa fa-check"></i><b>12.0.1</b> Assumptions of OLS</a></li>
<li class="chapter" data-level="12.0.2" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#assumptions-of-glms"><i class="fa fa-check"></i><b>12.0.2</b> Assumptions of GLMs</a></li>
<li class="chapter" data-level="12.1" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#advantages-and-disadvantages"><i class="fa fa-check"></i><b>12.1</b> Advantages and disadvantages</a></li>
<li class="chapter" data-level="12.2" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#glms-for-regression"><i class="fa fa-check"></i><b>12.2</b> GLMs for regression</a></li>
<li class="chapter" data-level="12.3" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#interpretation-of-coefficients"><i class="fa fa-check"></i><b>12.3</b> Interpretation of coefficients</a><ul>
<li class="chapter" data-level="12.3.1" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#identity-link"><i class="fa fa-check"></i><b>12.3.1</b> Identity link</a></li>
<li class="chapter" data-level="12.3.2" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#log-link"><i class="fa fa-check"></i><b>12.3.2</b> Log link</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#other-links"><i class="fa fa-check"></i><b>12.4</b> Other links</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="glms-for-classification.html"><a href="glms-for-classification.html"><i class="fa fa-check"></i><b>13</b> GLMs for classification</a><ul>
<li class="chapter" data-level="13.1" data-path="glms-for-classification.html"><a href="glms-for-classification.html#binary-target"><i class="fa fa-check"></i><b>13.1</b> Binary target</a></li>
<li class="chapter" data-level="13.2" data-path="glms-for-classification.html"><a href="glms-for-classification.html#count-target"><i class="fa fa-check"></i><b>13.2</b> Count target</a></li>
<li class="chapter" data-level="13.3" data-path="glms-for-classification.html"><a href="glms-for-classification.html#link-functions"><i class="fa fa-check"></i><b>13.3</b> Link functions</a></li>
<li class="chapter" data-level="13.4" data-path="glms-for-classification.html"><a href="glms-for-classification.html#interpretation-of-coefficients-1"><i class="fa fa-check"></i><b>13.4</b> Interpretation of coefficients</a><ul>
<li class="chapter" data-level="13.4.1" data-path="glms-for-classification.html"><a href="glms-for-classification.html#logit"><i class="fa fa-check"></i><b>13.4.1</b> Logit</a></li>
<li class="chapter" data-level="13.4.2" data-path="glms-for-classification.html"><a href="glms-for-classification.html#probit-cauchit-cloglog"><i class="fa fa-check"></i><b>13.4.2</b> Probit, Cauchit, Cloglog</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="glms-for-classification.html"><a href="glms-for-classification.html#example-1"><i class="fa fa-check"></i><b>13.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="classification-metrics.html"><a href="classification-metrics.html"><i class="fa fa-check"></i><b>14</b> Classification metrics</a><ul>
<li class="chapter" data-level="14.1" data-path="classification-metrics.html"><a href="classification-metrics.html#area-under-the-roc-curve-auc"><i class="fa fa-check"></i><b>14.1</b> Area Under the ROC Curve (AUC)</a></li>
<li class="chapter" data-level="14.2" data-path="classification-metrics.html"><a href="classification-metrics.html#additional-reading"><i class="fa fa-check"></i><b>14.2</b> Additional reading</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html"><i class="fa fa-check"></i><b>15</b> Additional GLM topics</a><ul>
<li class="chapter" data-level="15.1" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#residuals"><i class="fa fa-check"></i><b>15.1</b> Residuals</a><ul>
<li class="chapter" data-level="15.1.1" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#raw-residuals"><i class="fa fa-check"></i><b>15.1.1</b> Raw residuals</a></li>
<li class="chapter" data-level="15.1.2" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#deviance-residuals"><i class="fa fa-check"></i><b>15.1.2</b> Deviance residuals</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#example-2"><i class="fa fa-check"></i><b>15.2</b> Example</a></li>
<li class="chapter" data-level="15.3" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#log-transforms-of-continuous-predictors"><i class="fa fa-check"></i><b>15.3</b> Log transforms of continuous predictors</a></li>
<li class="chapter" data-level="15.4" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#reference-levels"><i class="fa fa-check"></i><b>15.4</b> Reference levels</a></li>
<li class="chapter" data-level="15.5" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#interactions"><i class="fa fa-check"></i><b>15.5</b> Interactions</a></li>
<li class="chapter" data-level="15.6" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#offsets"><i class="fa fa-check"></i><b>15.6</b> Offsets</a></li>
<li class="chapter" data-level="15.7" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#tweedie-regression"><i class="fa fa-check"></i><b>15.7</b> Tweedie regression</a></li>
<li class="chapter" data-level="15.8" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#combinations-of-link-functions-and-target-distributions"><i class="fa fa-check"></i><b>15.8</b> Combinations of Link Functions and Target Distributions</a><ul>
<li class="chapter" data-level="15.8.1" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-log-link"><i class="fa fa-check"></i><b>15.8.1</b> Gaussian Response with Log Link</a></li>
<li class="chapter" data-level="15.8.2" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-inverse-link"><i class="fa fa-check"></i><b>15.8.2</b> Gaussian Response with Inverse Link</a></li>
<li class="chapter" data-level="15.8.3" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-identity-link"><i class="fa fa-check"></i><b>15.8.3</b> Gaussian Response with Identity Link</a></li>
<li class="chapter" data-level="15.8.4" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-log-link-and-negative-values"><i class="fa fa-check"></i><b>15.8.4</b> Gaussian Response with Log Link and Negative Values</a></li>
<li class="chapter" data-level="15.8.5" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gamma-response-with-log-link"><i class="fa fa-check"></i><b>15.8.5</b> Gamma Response with Log Link</a></li>
<li class="chapter" data-level="15.8.6" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gamma-with-inverse-link"><i class="fa fa-check"></i><b>15.8.6</b> Gamma with Inverse Link</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html"><i class="fa fa-check"></i><b>16</b> GLM variable selection</a><ul>
<li class="chapter" data-level="16.1" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#stepwise-subset-selection"><i class="fa fa-check"></i><b>16.1</b> Stepwise subset selection</a></li>
<li class="chapter" data-level="16.2" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#penalized-linear-models"><i class="fa fa-check"></i><b>16.2</b> Penalized Linear Models</a></li>
<li class="chapter" data-level="16.3" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#ridge-regression"><i class="fa fa-check"></i><b>16.3</b> Ridge Regression</a></li>
<li class="chapter" data-level="16.4" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#lasso"><i class="fa fa-check"></i><b>16.4</b> Lasso</a></li>
<li class="chapter" data-level="16.5" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#elastic-net"><i class="fa fa-check"></i><b>16.5</b> Elastic Net</a></li>
<li class="chapter" data-level="16.6" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#advantages-and-disadvantages-1"><i class="fa fa-check"></i><b>16.6</b> Advantages and disadvantages</a></li>
<li class="chapter" data-level="16.7" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#example-ridge-regression"><i class="fa fa-check"></i><b>16.7</b> Example: Ridge Regression</a></li>
<li class="chapter" data-level="16.8" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#example-the-lasso"><i class="fa fa-check"></i><b>16.8</b> Example: The Lasso</a></li>
<li class="chapter" data-level="16.9" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#references"><i class="fa fa-check"></i><b>16.9</b> References</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="tree-based-models.html"><a href="tree-based-models.html"><i class="fa fa-check"></i><b>17</b> Tree-based models</a><ul>
<li class="chapter" data-level="17.1" data-path="tree-based-models.html"><a href="tree-based-models.html#decision-trees"><i class="fa fa-check"></i><b>17.1</b> Decision Trees</a><ul>
<li class="chapter" data-level="17.1.1" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-2"><i class="fa fa-check"></i><b>17.1.1</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="tree-based-models.html"><a href="tree-based-models.html#ensemble-learning"><i class="fa fa-check"></i><b>17.2</b> Ensemble learning</a><ul>
<li class="chapter" data-level="17.2.1" data-path="tree-based-models.html"><a href="tree-based-models.html#bagging"><i class="fa fa-check"></i><b>17.2.1</b> Bagging</a></li>
<li class="chapter" data-level="17.2.2" data-path="tree-based-models.html"><a href="tree-based-models.html#boosting"><i class="fa fa-check"></i><b>17.2.2</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="tree-based-models.html"><a href="tree-based-models.html#random-forests"><i class="fa fa-check"></i><b>17.3</b> Random Forests</a><ul>
<li class="chapter" data-level="17.3.1" data-path="tree-based-models.html"><a href="tree-based-models.html#example-3"><i class="fa fa-check"></i><b>17.3.1</b> Example</a></li>
<li class="chapter" data-level="17.3.2" data-path="tree-based-models.html"><a href="tree-based-models.html#variable-importance"><i class="fa fa-check"></i><b>17.3.2</b> Variable Importance</a></li>
<li class="chapter" data-level="17.3.3" data-path="tree-based-models.html"><a href="tree-based-models.html#partial-dependence"><i class="fa fa-check"></i><b>17.3.3</b> Partial dependence</a></li>
<li class="chapter" data-level="17.3.4" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-3"><i class="fa fa-check"></i><b>17.3.4</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="tree-based-models.html"><a href="tree-based-models.html#gradient-boosted-trees"><i class="fa fa-check"></i><b>17.4</b> Gradient Boosted Trees</a><ul>
<li class="chapter" data-level="17.4.1" data-path="tree-based-models.html"><a href="tree-based-models.html#adaboost"><i class="fa fa-check"></i><b>17.4.1</b> AdaBoost</a></li>
<li class="chapter" data-level="17.4.2" data-path="tree-based-models.html"><a href="tree-based-models.html#gradient-boosting"><i class="fa fa-check"></i><b>17.4.2</b> Gradient Boosting</a></li>
<li class="chapter" data-level="17.4.3" data-path="tree-based-models.html"><a href="tree-based-models.html#notation"><i class="fa fa-check"></i><b>17.4.3</b> Notation</a></li>
<li class="chapter" data-level="17.4.4" data-path="tree-based-models.html"><a href="tree-based-models.html#parameters"><i class="fa fa-check"></i><b>17.4.4</b> Parameters</a></li>
<li class="chapter" data-level="17.4.5" data-path="tree-based-models.html"><a href="tree-based-models.html#example-4"><i class="fa fa-check"></i><b>17.4.5</b> Example</a></li>
<li class="chapter" data-level="17.4.6" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-4"><i class="fa fa-check"></i><b>17.4.6</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="tree-based-models.html"><a href="tree-based-models.html#exercises-1"><i class="fa fa-check"></i><b>17.5</b> Exercises</a><ul>
<li class="chapter" data-level="17.5.1" data-path="tree-based-models.html"><a href="tree-based-models.html#rf-with-randomforest"><i class="fa fa-check"></i><b>17.5.1</b> 1. RF with <code>randomForest</code></a></li>
<li class="chapter" data-level="17.5.2" data-path="tree-based-models.html"><a href="tree-based-models.html#rf-tuning-with-caret"><i class="fa fa-check"></i><b>17.5.2</b> 2. RF tuning with <code>caret</code></a></li>
<li class="chapter" data-level="17.5.3" data-path="tree-based-models.html"><a href="tree-based-models.html#tuning-a-gbm-with-caret"><i class="fa fa-check"></i><b>17.5.3</b> 3. Tuning a GBM with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>18</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="18.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>18.1</b> Principal Component Analysis (PCA)</a><ul>
<li class="chapter" data-level="18.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-pca-on-us-arrests"><i class="fa fa-check"></i><b>18.1.1</b> Example: PCA on US Arrests</a></li>
<li class="chapter" data-level="18.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-pca-on-cancel-cells"><i class="fa fa-check"></i><b>18.1.2</b> Example: PCA on Cancel Cells</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering"><i class="fa fa-check"></i><b>18.2</b> Clustering</a><ul>
<li class="chapter" data-level="18.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>18.2.1</b> K-Means Clustering</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#hierarchical-clustering"><i class="fa fa-check"></i><b>18.3</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="18.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-clustering-cancel-cells"><i class="fa fa-check"></i><b>18.3.1</b> Example: Clustering Cancel Cells</a></li>
<li class="chapter" data-level="18.3.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#references-1"><i class="fa fa-check"></i><b>18.3.2</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="references-2.html"><a href="references-2.html"><i class="fa fa-check"></i><b>19</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predictive Analytics for Actuaries</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-modeling" class="section level1">
<h1><span class="header-section-number"> 11</span> Introduction to modeling</h1>
<p>About 40-50% of the exam grade is based on modeling. The goal is to be able to predict an unknown quantity. In actuarial applications, this tends to be claims that occur in the future, death, injury, accidents, policy lapse, hurricanes, or some other insurable event.</p>
<div id="modeling-vocabulary" class="section level2">
<h2><span class="header-section-number">11.1</span> Modeling vocabulary</h2>
<p>Modeling notation is sloppy because there are many words that mean the same thing.</p>
<p>The number of observations will be denoted by <span class="math inline">\(n\)</span>. When we refer to the size of a data set, we are referring to <span class="math inline">\(n\)</span>. Each row of the data is called an <em>observation</em> or <em>record</em>. Observations tend to be people, cars, buildings, or other insurable things. These are always independent in that they do not influence one another. Because the Prometric computers have limited power, <span class="math inline">\(n\)</span> tends to be less than 100,000.</p>
<p>Each observation has known attributes called <em>variables</em>, <em>features</em>, or <em>predictors</em>. We use <span class="math inline">\(p\)</span> to refer the number of input variables that are used in the model.</p>
<p>The <em>target</em>, <em>response</em>, <em>label</em>, <em>dependent variable</em>, or <em>outcome</em> variable is the unknown quantity that is being predicted. We use <span class="math inline">\(Y\)</span> for this. This can be either a whole number, in which case we are performing <em>regression</em>, or a category, in which case we are performing <em>classification</em>.</p>
<p>For example, say that you are a health insurance company that wants to set the premiums for a group of people. The premiums for people who are likely to incur high health costs need to be higher than those who are likely to be low-cost. Older people tend to use more of their health benefits than younger people, but there are always exceptions for those who are very physically active and healthy. Those who have an unhealthy Body Mass Index (BMI) tend to have higher costs than those who have a healthy BMI, but this has less of an impact on younger people. <strong>In short, we want to be able to predict a person’s future health costs by taking into account many of their attributes at once</strong>.</p>
<p>This can be done in the <code>health_insurance</code> data by fitting a model to predict the annual health costs of a person. The target variable is <code>y = charges</code>, and the predictor variables are <code>age</code>, <code>sex</code>, <code>bmi</code>, <code>children</code>, <code>smoker</code> and <code>region</code>. These six variables mean that <span class="math inline">\(p = 6\)</span>. The data is collected from 1,338 patients, which means that <span class="math inline">\(n = 1,338\)</span>.</p>
</div>
<div id="modeling-notation" class="section level2">
<h2><span class="header-section-number">11.2</span> Modeling notation</h2>
<p>Scalar numbers are denoted by ordinary variables (i.e., <span class="math inline">\(x = 2\)</span>, <span class="math inline">\(z = 4\)</span>), and vectors are denoted by bold-faced letters</p>
<p><span class="math display">\[\mathbf{a} = \begin{pmatrix} a_1 \\ a_2 \\ a_3 \end{pmatrix}\]</span></p>
<p>We organize these variables into matrices. Take an example with <span class="math inline">\(p\)</span> = 2 columns and 3 observations. The matrix is said to be <span class="math inline">\(3 \times 2\)</span> (read as “3-by-2”) matrix.</p>
<p><span class="math display">\[
\mathbf{X} = \begin{pmatrix}x_{11} &amp; x_{21}\\
x_{21} &amp; x_{22}\\
x_{31} &amp; x_{32}
\end{pmatrix}
\]</span></p>
<p>In the health care costs example, <span class="math inline">\(y_1\)</span> would be the costs of the first patient, <span class="math inline">\(y_2\)</span> the costs of the second patient, and so forth. The variables <span class="math inline">\(x_{11}\)</span> and <span class="math inline">\(x_{12}\)</span> might represent the first patient’s age and sex respectively, where <span class="math inline">\(x_{i1}\)</span> is the patient’s age, and <span class="math inline">\(x_{i2} = 1\)</span> if the ith patient is male and 0 if female.</p>
<p>Modeling is about using <span class="math inline">\(X\)</span> to predict <span class="math inline">\(Y\)</span>. We call this “y-hat”, or simply the <em>prediction</em>. This is based on a function of the data <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[\hat{Y} = f(X)\]</span></p>
<p>This is almost never going to happen perfectly, and so there is always an error term, <span class="math inline">\(\epsilon\)</span>. This can be made smaller, but is never exactly zero.</p>
<p><span class="math display">\[
\hat{Y} + \epsilon = f(X) + \epsilon
\]</span></p>
<p>In other words, <span class="math inline">\(\epsilon = y - \hat{y}\)</span>. We call this the <em>residual</em>. When we predict a person’s health care costs, this is the difference between the predicted costs (which we had created the year before) and the actual costs that the patient experienced (of that current year).</p>
<p>Another way of saying this is in terms of expected value: the model <span class="math inline">\(f(X)\)</span> estimates the expected value of the target <span class="math inline">\(E[Y|X]\)</span>. That is, once we condition on the data <span class="math inline">\(X\)</span>, we can make a guess as to what we expect <span class="math inline">\(Y\)</span> to be “close to”. There are many ways of measuring “closeness”, as we will see.</p>
</div>
<div id="ordinary-least-squares-ols" class="section level2">
<h2><span class="header-section-number">11.3</span> Ordinary Least Squares (OLS)</h2>
<p>Also known as <em>simple linear regression</em>, OLS predicts the target as a weighted sum of the variables.</p>
<p>We find a <span class="math inline">\(\mathbf{\beta}\)</span> so that</p>
<p><span class="math display">\[
\hat{Y} = E[Y] =  \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p
\]</span></p>
<p>Each <span class="math inline">\(y_i\)</span> is a <em>linear combination</em> of <span class="math inline">\(x_{i1}, ..., x_{ip}\)</span>, plus a constant <span class="math inline">\(\beta_0\)</span> which is called the <em>intercept</em> term.</p>
<p>In the one-dimensional case, this creates a line connecting the points. In higher dimensions, this creates a hyper-plane.</p>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-71-1.png" width="672" /></p>
<p>The red line shows the <em>expected value</em> of the target, as the target <span class="math inline">\(\hat{Y}\)</span> is actually a random variable. For each of the data points, the model assumes a Gaussian distribution. If there is just a single predictor, <span class="math inline">\(x\)</span>, then the mean is <span class="math inline">\(\beta_0 + \beta_1 x\)</span>.</p>
<p><img src="images/conditional_response.jpg" width="200%" style="display: block; margin: auto;" /></p>
<p>The question then is <strong>how can we choose the best values of</strong> <span class="math inline">\(\beta?\)</span> First of all, we need to define what we mean by “best”. Ideally, we will choose these values which will create close predictions of <span class="math inline">\(Y\)</span> on new, unseen data.</p>
<p>To solve for <span class="math inline">\(\mathbf{\beta}\)</span>, we first need to define a <em>loss function</em>. This allows us to compare how well a model is fitting the data. The most commonly used loss function is the residual sum of squares (RSS), also called the <em>squared error loss</em> or the L2 norm. When RSS is small, then the predictions are close to the actual values and the model is a good fit. When RSS is large, the model is a poor fit.</p>
<p><span class="math display">\[
\text{RSS} = \sum_i(y_i - \hat{y})^2
\]</span></p>
<p>When you replace <span class="math inline">\(\hat{y_i}\)</span> in the above equation with <span class="math inline">\(\beta_0 + \beta_1 x_1 + ... + \beta_p x_p\)</span>, take the derivative with respect to <span class="math inline">\(\beta\)</span>, set equal to zero, and solve, we can find the optimal values. This turns the problem of statistics into a problem of numeric optimization, which computers can do quickly.</p>
<p>You might be asking: why does this need to be the squared error? Why not the absolute error, or the cubed error? Technically, these could be used as well but the betas would not be the maximum likelihood parameters. In fact, using the absolute error results in the model predicting the <em>median</em> as opposed to the <em>mean</em>. Two reasons why RSS is popular are:</p>
<ul>
<li>It provides the same solution if we assume that the distribution of <span class="math inline">\(Y|X\)</span> is Gaussian and maximize the likelihood function. This method is used for GLMs, in the next chapter.</li>
<li>It is computationally easier, and computers used to have a difficult time optimizing for MAE</li>
</ul>
<blockquote>
<p>What does it mean when a log transform is applied to <span class="math inline">\(Y\)</span>? I remember from my statistics course on regression that this was done.</p>
</blockquote>
<p>This is done so that the variance is closer to being constant. For example, if the units are in dollars, then it is very common for the values to fluctuate more for higher values than for lower values. Consider a stock price, for instance. If the stock is $50 per share, then it will go up or down less than if it is $1000 per share. The log of 50, however, is about 3.9 and the log of 1000 is only 6.9, and so this difference is smaller. In other words, the variance is smaller.</p>
<p>Transforming the target means that instead of the model predicting <span class="math inline">\(E[Y]\)</span>, it predicts <span class="math inline">\(E[log(Y)]\)</span>. A common mistake is to then the take the exponent in an attempt to “undo” this transform, but <span class="math inline">\(e^{E[log(Y)]}\)</span> is not the same as <span class="math inline">\(E[Y]\)</span>.</p>
</div>
<div id="regression-vs.-classification" class="section level2">
<h2><span class="header-section-number">11.4</span> Regression vs. classification</h2>
<p>Regression modeling is when the target is a number. Binary classification is when there are two outcomes, such as “Yes/No”, “True/False”, or “0/1”. Multi-class regression is when there are more than two categories such as “Red, Yellow, Green” or “A, B, C, D, E”. There are many other types of regression that are not covered on this exam such as ordinal regression, where the outcome is an ordered category, or time-series regression, where the data is time-dependent.</p>
</div>
<div id="regression-metrics" class="section level2">
<h2><span class="header-section-number">11.5</span> Regression metrics</h2>
<p>For any model, the goal is always to reduce an error metric. This is a way of measuring how well the model can explain the target. The phrases “reducing error”, “improving performance”, or “making a better fit” are synonymous with reducing the error. The word “better” means “lower error” and “worse” means “higher error”.</p>
<p>The choice of error metric has a big difference on the outcome. When explaining a model to a businessperson, using simpler metrics such as R-Squared and Accuracy is convenient. When training the model, however, using a more nuanced metric is almost always better.</p>
<p>These are the regression metrics that are most likely to appear on Exam PA. Memorizing these formulas for AIC and BIC is not necessary as they are in the R documentation by typing <code>?AIC</code> or <code>?BIC</code> into the R console.</p>
<p><img src="images/regression_metrics.png" width="1300%" style="display: block; margin: auto;" /></p>
</div>
<div id="example" class="section level2">
<h2><span class="header-section-number">11.6</span> Example</h2>
<p>In our health insurance data, we can predict a person’s health costs based on their age, body mass index, and gender. Intuitively, we expect that these costs would increase as a person’s age increases, would be different for men than for women, and would be higher for those who have a less healthy BMI. We create a linear model using <code>bmi</code>, <code>age</code>, and <code>sex</code> as an inputs.</p>
<p>The <code>formula</code> controls which variables are included. There are a few shortcuts for using R formulas.</p>
<table>
<colgroup>
<col width="43%" />
<col width="56%" />
</colgroup>
<thead>
<tr class="header">
<th>Formula</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>charges</code> ~ <code>bmi</code> + <code>age</code></td>
<td>Use <code>age</code> and <code>bmi</code> to predict <code>charges</code></td>
</tr>
<tr class="even">
<td><code>charges</code> ~ <code>bmi</code> + <code>age</code> + <code>bmi</code>*<code>age</code></td>
<td>Use <code>age</code>,<code>bmi</code> as well as an interaction to predict <code>charges</code></td>
</tr>
<tr class="odd">
<td><code>charges</code> ~ (<code>bmi &gt; 20</code>) + <code>age</code></td>
<td>Use an indicator variable for <code>bmi &gt; 20</code> <code>age</code> to predict <code>charges</code></td>
</tr>
<tr class="even">
<td>log(<code>charges</code>) ~ log(<code>bmi</code>) + log(<code>age</code>)</td>
<td>Use the logs of <code>age</code> and <code>bmi</code> to predict log(<code>charges</code>)</td>
</tr>
<tr class="odd">
<td><code>charges</code> ~ .</td>
<td>Use all variables to predict <code>charges</code></td>
</tr>
</tbody>
</table>
<blockquote>
<p>While you can use formulas to create new variables, the exam questions tend to have you do this in the data itself. For example, if taking the log transform of a <code>bmi</code>, you would add a column <code>log_bmi</code> to the data and remove the original <code>bmi</code> column.</p>
</blockquote>
<p>Below we fit a simple linear model to predict charges.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb170-1" title="1"><span class="kw">library</span>(ExamPAData)</a>
<a class="sourceLine" id="cb170-2" title="2"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb170-3" title="3"></a>
<a class="sourceLine" id="cb170-4" title="4">model &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> health_insurance, <span class="dt">formula =</span> charges <span class="op">~</span><span class="st"> </span>bmi <span class="op">+</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>sex)</a></code></pre></div>
<p>The <code>summary</code> function gives details about the model. First, the <code>Estimate</code>, gives you the coefficients. The <code>Std. Error</code> is the error of the estimate for the coefficient. Higher standard error means greater uncertainty. This is relative to the average value of that variable. The <code>p value</code> tells you how “big” this error really is based on standard deviations. A small p-value (<code>Pr (&gt;|t|))</code>) means that we can safely reject the null hypothesis that says the coefficient is equal to zero.</p>
<p>The little <code>*</code>, <code>**</code>, <code>***</code> tell you the significance level. A variable with a <code>***</code> means that the probability of getting a coefficient of that size given that the data was randomly generated is less than 0.001. The <code>**</code> has a significance level of 0.01, and <code>*</code> of 0.05.</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb171-1" title="1"><span class="kw">summary</span>(model)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = charges ~ bmi + age + sex, data = health_insurance)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -14974  -7073  -5072   6953  47348 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -6986.82    1761.04  -3.967 7.65e-05 ***
## bmi           327.54      51.37   6.377 2.49e-10 ***
## age           243.19      22.28  10.917  &lt; 2e-16 ***
## sexmale      1344.46     622.66   2.159    0.031 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 11370 on 1334 degrees of freedom
## Multiple R-squared:  0.1203, Adjusted R-squared:  0.1183 
## F-statistic: 60.78 on 3 and 1334 DF,  p-value: &lt; 2.2e-16</code></pre>
<blockquote>
<p>For this exam, variable selection tends to be based on the 0.05 significance level (single star <code>*</code>).</p>
</blockquote>
<p>When evaluating model performance, you should not rely on the <code>summary</code> alone as this is based on the training data. To look at performance, test the model on validation data. This can be done by either using a hold out set, or using cross-validation, which is even better.</p>
<p>Let’s create an 80% training set and 20% testing set. You don’t need to worry about understanding this code as the exam will always give this to you.</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb173-1" title="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb173-2" title="2"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb173-3" title="3"><span class="co">#create a train/test split</span></a>
<a class="sourceLine" id="cb173-4" title="4">index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> health_insurance<span class="op">$</span>charges, </a>
<a class="sourceLine" id="cb173-5" title="5">                             <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list =</span> F) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.numeric</span>()</a>
<a class="sourceLine" id="cb173-6" title="6">train &lt;-<span class="st">  </span>health_insurance <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(index)</a>
<a class="sourceLine" id="cb173-7" title="7">test &lt;-<span class="st"> </span>health_insurance <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>index)</a></code></pre></div>
<p>Train the model on the <code>train</code> and test on <code>test</code>.</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb174-1" title="1">model &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> train, <span class="dt">formula =</span> charges <span class="op">~</span><span class="st"> </span>bmi <span class="op">+</span><span class="st"> </span>age)</a>
<a class="sourceLine" id="cb174-2" title="2">pred =<span class="st"> </span><span class="kw">predict</span>(model, test)</a></code></pre></div>
<p>Let’s look at the Root Mean Squared Error (RMSE).</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb175-1" title="1">get_rmse &lt;-<span class="st"> </span><span class="cf">function</span>(y, y_hat){</a>
<a class="sourceLine" id="cb175-2" title="2">  <span class="kw">sqrt</span>(<span class="kw">mean</span>((y <span class="op">-</span><span class="st"> </span>y_hat)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb175-3" title="3">}</a>
<a class="sourceLine" id="cb175-4" title="4"></a>
<a class="sourceLine" id="cb175-5" title="5"><span class="kw">get_rmse</span>(pred, test<span class="op">$</span>charges)</a></code></pre></div>
<pre><code>## [1] 11421.96</code></pre>
<p>And the Mean Absolute Error as well.</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb177-1" title="1">get_mae &lt;-<span class="st"> </span><span class="cf">function</span>(y, y_hat){</a>
<a class="sourceLine" id="cb177-2" title="2">  <span class="kw">sqrt</span>(<span class="kw">mean</span>(<span class="kw">abs</span>(y <span class="op">-</span><span class="st"> </span>y_hat)))</a>
<a class="sourceLine" id="cb177-3" title="3">}</a>
<a class="sourceLine" id="cb177-4" title="4"></a>
<a class="sourceLine" id="cb177-5" title="5"><span class="kw">get_mae</span>(pred, test<span class="op">$</span>charges)</a></code></pre></div>
<pre><code>## [1] 94.32336</code></pre>
<p>The above metrics do not tell us if this is a good model or not by themselves. We need a comparison. The fastest check is to compare against a prediction of the mean. In other words, all values of the <code>y_hat</code> are the average of <code>charges</code>, which is about $13,000.</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb179-1" title="1"><span class="kw">get_rmse</span>(<span class="kw">mean</span>(test<span class="op">$</span>charges), test<span class="op">$</span>charges)</a></code></pre></div>
<pre><code>## [1] 12574.97</code></pre>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb181-1" title="1"><span class="kw">get_mae</span>(<span class="kw">mean</span>(test<span class="op">$</span>charges), test<span class="op">$</span>charges)</a></code></pre></div>
<pre><code>## [1] 96.63604</code></pre>
<p>The RMSE and MAE are both <strong>higher</strong> (worse) when using just the mean, which is what we expect. <strong>If you ever fit a model and get an error which is worse than the average prediction, something must be wrong.</strong></p>
<p>The next test is to see if any assumptions have been violated.</p>
<p>First, is there a pattern in the residuals? If there is, this means that the model is missing key information. For the model below, this is a <strong>yes</strong>, <strong>which means that this is a bad model</strong>. Because this is just for illustration, we are going to continue using it.</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb183-1" title="1"><span class="kw">plot</span>(model, <span class="dt">which =</span> <span class="dv">1</span>)</a></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-81"></span>
<img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-81-1.png" alt="Residuals vs. Fitted" width="672" />
<p class="caption">
Figure 11.1: Residuals vs. Fitted
</p>
</div>
<p>The normal QQ shows how well the quantiles of the predictions fit to a theoretical normal distribution. If this is true, then the graph is a straight 45-degree line. In this model, you can definitely see that this is not the case. If this were a good model, this distribution would be closer to normal.</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb184-1" title="1"><span class="kw">plot</span>(model, <span class="dt">which =</span> <span class="dv">2</span>)</a></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-82"></span>
<img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-82-1.png" alt="Normal Q-Q" width="672" />
<p class="caption">
Figure 11.2: Normal Q-Q
</p>
</div>
<p>Once you have chosen your model, you should re-train over the entire data set. This is to make the coefficients more stable because <code>n</code> is larger. Below you can see that the standard error is lower after training over the entire data set.</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb185-1" title="1">all_data &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> health_insurance, </a>
<a class="sourceLine" id="cb185-2" title="2">               <span class="dt">formula =</span> charges <span class="op">~</span><span class="st"> </span>bmi <span class="op">+</span><span class="st"> </span>age)</a>
<a class="sourceLine" id="cb185-3" title="3">testing &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> test, </a>
<a class="sourceLine" id="cb185-4" title="4">              <span class="dt">formula =</span> charges <span class="op">~</span><span class="st"> </span>bmi <span class="op">+</span><span class="st"> </span>age)</a></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">full_data_std_error</th>
<th align="right">test_data_std_error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">1744.1</td>
<td align="right">3824.2</td>
</tr>
<tr class="even">
<td align="left">bmi</td>
<td align="right">51.4</td>
<td align="right">111.1</td>
</tr>
<tr class="odd">
<td align="left">age</td>
<td align="right">22.3</td>
<td align="right">47.8</td>
</tr>
</tbody>
</table>
<p>All interpretations should be based on the model which was trained on the entire data set. Obviously, this only makes a difference if you are interpreting the precise values of the coefficients. If you are just looking at which variables are included, or at the size and sign of the coefficients, then this would probably not make a difference.</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb186-1" title="1"><span class="kw">coefficients</span>(model)</a></code></pre></div>
<pre><code>## (Intercept)         bmi         age 
##  -4526.5284    286.8283    228.4372</code></pre>
<p>Translating the above into an equation we have</p>
<p><span class="math display">\[\hat{y_i} = -4,526 + 287 \space\text{bmi} + 228\space \text{age}\]</span></p>
<p>For example, if a patient has <code>bmi = 27.9</code> and <code>age = 19</code> then predicted value is</p>
<p><span class="math display">\[\hat{y_1} = 4,526 + (287)(27.9) + (228)(19) = 16,865\]</span></p>
<p>This model structure implies that each of the variables <span class="math inline">\(x_1, ..., x_p\)</span> each change the predicted <span class="math inline">\(\hat{y}\)</span>. If <span class="math inline">\(x_{ij}\)</span> increases by one unit, then <span class="math inline">\(y_i\)</span> increases by <span class="math inline">\(\beta_j\)</span> units, regardless of what happens to all of the other variables. This is one of the main assumptions of linear models: <em>variable independence</em>. If the variables are correlated, say, then this assumption will be violated.</p>
<table>
<thead>
<tr class="header">
<th>Readings</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ISLR 2.1 What is statistical learning?</td>
<td></td>
</tr>
<tr class="even">
<td>ISLR 2.2 Assessing model accuracy</td>
<td></td>
</tr>
</tbody>
</table>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="visualization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="generalized-linear-models-glms.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/sdcastillo/PA-R-Study-Manual/edit/master/05-linear-models.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Exam-PA-Study-Manual.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
