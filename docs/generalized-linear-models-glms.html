<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 10 Generalized linear models (GLMs) | Exam PA Study Manual</title>
  <meta name="description" content=" 10 Generalized linear models (GLMs) | Exam PA Study Manual" />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content=" 10 Generalized linear models (GLMs) | Exam PA Study Manual" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="sdcastillo/PA-R-Study-Manual" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 10 Generalized linear models (GLMs) | Exam PA Study Manual" />
  
  
  

<meta name="author" content="Sam Castillo" />


<meta name="date" content="2020-01-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="images/artificial_actuary_logo_favicon.png" type="image/x-icon" />
<link rel="prev" href="introduction-to-modeling.html"/>
<link rel="next" href="logistic-regression.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Exam PA Study Manual</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> What’s in this book</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i><b>1.1</b> About the author</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-exam.html"><a href="the-exam.html"><i class="fa fa-check"></i><b>2</b> The exam</a></li>
<li class="chapter" data-level="3" data-path="prometric-demo.html"><a href="prometric-demo.html"><i class="fa fa-check"></i><b>3</b> Prometric Demo</a></li>
<li class="chapter" data-level="4" data-path="you-already-know-what-learning-is.html"><a href="you-already-know-what-learning-is.html"><i class="fa fa-check"></i><b>4</b> You already know what learning is</a></li>
<li class="chapter" data-level="5" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>5</b> Getting started</a><ul>
<li class="chapter" data-level="5.1" data-path="getting-started.html"><a href="getting-started.html#download-the-data"><i class="fa fa-check"></i><b>5.1</b> Download the data</a></li>
<li class="chapter" data-level="5.2" data-path="getting-started.html"><a href="getting-started.html#download-islr"><i class="fa fa-check"></i><b>5.2</b> Download ISLR</a></li>
<li class="chapter" data-level="5.3" data-path="getting-started.html"><a href="getting-started.html#new-users"><i class="fa fa-check"></i><b>5.3</b> New users</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="r-programming.html"><a href="r-programming.html"><i class="fa fa-check"></i><b>6</b> R programming</a><ul>
<li class="chapter" data-level="6.1" data-path="r-programming.html"><a href="r-programming.html#notebook-chunks"><i class="fa fa-check"></i><b>6.1</b> Notebook chunks</a></li>
<li class="chapter" data-level="6.2" data-path="r-programming.html"><a href="r-programming.html#basic-operations"><i class="fa fa-check"></i><b>6.2</b> Basic operations</a></li>
<li class="chapter" data-level="6.3" data-path="r-programming.html"><a href="r-programming.html#lists"><i class="fa fa-check"></i><b>6.3</b> Lists</a></li>
<li class="chapter" data-level="6.4" data-path="r-programming.html"><a href="r-programming.html#functions"><i class="fa fa-check"></i><b>6.4</b> Functions</a></li>
<li class="chapter" data-level="6.5" data-path="r-programming.html"><a href="r-programming.html#data-frames"><i class="fa fa-check"></i><b>6.5</b> Data frames</a></li>
<li class="chapter" data-level="6.6" data-path="r-programming.html"><a href="r-programming.html#pipes"><i class="fa fa-check"></i><b>6.6</b> Pipes</a></li>
<li class="chapter" data-level="6.7" data-path="r-programming.html"><a href="r-programming.html#the-soas-code-doesnt-use-pipes-or-dplyr-so-can-i-skip-learning-this"><i class="fa fa-check"></i><b>6.7</b> The SOA’s code doesn’t use pipes or dplyr, so can I skip learning this?</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-manipulation.html"><a href="data-manipulation.html"><i class="fa fa-check"></i><b>7</b> Data manipulation</a><ul>
<li class="chapter" data-level="7.1" data-path="data-manipulation.html"><a href="data-manipulation.html#look-at-the-data"><i class="fa fa-check"></i><b>7.1</b> Look at the data</a></li>
<li class="chapter" data-level="7.2" data-path="data-manipulation.html"><a href="data-manipulation.html#transform-the-data"><i class="fa fa-check"></i><b>7.2</b> Transform the data</a></li>
<li class="chapter" data-level="7.3" data-path="data-manipulation.html"><a href="data-manipulation.html#exercises"><i class="fa fa-check"></i><b>7.3</b> Exercises</a></li>
<li class="chapter" data-level="7.4" data-path="data-manipulation.html"><a href="data-manipulation.html#answers-to-exercises"><i class="fa fa-check"></i><b>7.4</b> Answers to exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i><b>8</b> Visualization</a><ul>
<li class="chapter" data-level="8.1" data-path="visualization.html"><a href="visualization.html#create-a-plot-object-ggplot"><i class="fa fa-check"></i><b>8.1</b> Create a plot object (ggplot)</a></li>
<li class="chapter" data-level="8.2" data-path="visualization.html"><a href="visualization.html#add-a-plot"><i class="fa fa-check"></i><b>8.2</b> Add a plot</a></li>
<li class="chapter" data-level="8.3" data-path="visualization.html"><a href="visualization.html#data-manipulation-chaining"><i class="fa fa-check"></i><b>8.3</b> Data manipulation chaining</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html"><i class="fa fa-check"></i><b>9</b> Introduction to Modeling</a><ul>
<li class="chapter" data-level="9.1" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#model-notation"><i class="fa fa-check"></i><b>9.1</b> Model Notation</a></li>
<li class="chapter" data-level="9.2" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>9.2</b> Ordinary least squares (OLS)</a></li>
<li class="chapter" data-level="9.3" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#example"><i class="fa fa-check"></i><b>9.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html"><i class="fa fa-check"></i><b>10</b> Generalized linear models (GLMs)</a><ul>
<li class="chapter" data-level="10.1" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#the-generalized-linear-model"><i class="fa fa-check"></i><b>10.1</b> The generalized linear model</a></li>
<li class="chapter" data-level="10.2" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#interpretation"><i class="fa fa-check"></i><b>10.2</b> Interpretation</a></li>
<li class="chapter" data-level="10.3" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#residuals"><i class="fa fa-check"></i><b>10.3</b> Residuals</a></li>
<li class="chapter" data-level="10.4" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#example-1"><i class="fa fa-check"></i><b>10.4</b> Example</a></li>
<li class="chapter" data-level="10.5" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#combinations-of-link-and-response-family-examples"><i class="fa fa-check"></i><b>10.5</b> Combinations of Link and Response Family Examples</a><ul>
<li class="chapter" data-level="10.5.1" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#gaussian-response-with-log-link"><i class="fa fa-check"></i><b>10.5.1</b> Gaussian Response with Log Link</a></li>
<li class="chapter" data-level="10.5.2" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#gaussian-response-with-inverse-link"><i class="fa fa-check"></i><b>10.5.2</b> Gaussian Response with Inverse Link</a></li>
<li class="chapter" data-level="10.5.3" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#gaussian-response-with-identity-link"><i class="fa fa-check"></i><b>10.5.3</b> Gaussian Response with Identity Link</a></li>
<li class="chapter" data-level="10.5.4" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#gaussian-response-with-log-link-and-negative-values"><i class="fa fa-check"></i><b>10.5.4</b> Gaussian Response with Log Link and Negative Values</a></li>
<li class="chapter" data-level="10.5.5" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#gamma-response-with-log-link"><i class="fa fa-check"></i><b>10.5.5</b> Gamma Response with Log Link</a></li>
<li class="chapter" data-level="10.5.6" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#gamma-with-inverse-link"><i class="fa fa-check"></i><b>10.5.6</b> Gamma with Inverse Link</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#log-transforms-of-continuous-predictors"><i class="fa fa-check"></i><b>10.6</b> Log transforms of continuous predictors</a></li>
<li class="chapter" data-level="10.7" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#reference-levels"><i class="fa fa-check"></i><b>10.7</b> Reference levels</a></li>
<li class="chapter" data-level="10.8" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#interactions"><i class="fa fa-check"></i><b>10.8</b> Interactions</a></li>
<li class="chapter" data-level="10.9" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#poisson-regression"><i class="fa fa-check"></i><b>10.9</b> Poisson Regression</a></li>
<li class="chapter" data-level="10.10" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#offsets"><i class="fa fa-check"></i><b>10.10</b> Offsets</a></li>
<li class="chapter" data-level="10.11" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#tweedie-regression"><i class="fa fa-check"></i><b>10.11</b> Tweedie regression</a></li>
<li class="chapter" data-level="10.12" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#stepwise-subset-selection"><i class="fa fa-check"></i><b>10.12</b> Stepwise subset selection</a></li>
<li class="chapter" data-level="10.13" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#advantages-and-disadvantages"><i class="fa fa-check"></i><b>10.13</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>11</b> Logistic Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="logistic-regression.html"><a href="logistic-regression.html#model-form"><i class="fa fa-check"></i><b>11.1</b> Model form</a></li>
<li class="chapter" data-level="11.2" data-path="logistic-regression.html"><a href="logistic-regression.html#example-2"><i class="fa fa-check"></i><b>11.2</b> Example</a></li>
<li class="chapter" data-level="11.3" data-path="logistic-regression.html"><a href="logistic-regression.html#classification-metrics"><i class="fa fa-check"></i><b>11.3</b> Classification metrics</a><ul>
<li class="chapter" data-level="11.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#area-under-the-roc-curv-auc"><i class="fa fa-check"></i><b>11.3.1</b> Area Under the ROC Curv (AUC)</a></li>
<li class="chapter" data-level="11.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#additional-reading"><i class="fa fa-check"></i><b>11.3.2</b> Additional reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="penalized-linear-models.html"><a href="penalized-linear-models.html"><i class="fa fa-check"></i><b>12</b> Penalized Linear Models</a><ul>
<li class="chapter" data-level="12.1" data-path="penalized-linear-models.html"><a href="penalized-linear-models.html#ridge-regression"><i class="fa fa-check"></i><b>12.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="12.2" data-path="penalized-linear-models.html"><a href="penalized-linear-models.html#lasso"><i class="fa fa-check"></i><b>12.2</b> Lasso</a></li>
<li class="chapter" data-level="12.3" data-path="penalized-linear-models.html"><a href="penalized-linear-models.html#elastic-net"><i class="fa fa-check"></i><b>12.3</b> Elastic Net</a></li>
<li class="chapter" data-level="12.4" data-path="penalized-linear-models.html"><a href="penalized-linear-models.html#advantages-and-disadvantages-1"><i class="fa fa-check"></i><b>12.4</b> Advantages and disadvantages</a></li>
<li class="chapter" data-level="12.5" data-path="penalized-linear-models.html"><a href="penalized-linear-models.html#example-ridge-regression"><i class="fa fa-check"></i><b>12.5</b> Example: Ridge Regression</a></li>
<li class="chapter" data-level="12.6" data-path="penalized-linear-models.html"><a href="penalized-linear-models.html#example-the-lasso"><i class="fa fa-check"></i><b>12.6</b> Example: The Lasso</a></li>
<li class="chapter" data-level="12.7" data-path="penalized-linear-models.html"><a href="penalized-linear-models.html#references"><i class="fa fa-check"></i><b>12.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="tree-based-models.html"><a href="tree-based-models.html"><i class="fa fa-check"></i><b>13</b> Tree-based models</a><ul>
<li class="chapter" data-level="13.1" data-path="tree-based-models.html"><a href="tree-based-models.html#decision-trees"><i class="fa fa-check"></i><b>13.1</b> Decision Trees</a><ul>
<li class="chapter" data-level="13.1.1" data-path="tree-based-models.html"><a href="tree-based-models.html#model-form-1"><i class="fa fa-check"></i><b>13.1.1</b> Model form</a></li>
<li class="chapter" data-level="13.1.2" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-2"><i class="fa fa-check"></i><b>13.1.2</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="tree-based-models.html"><a href="tree-based-models.html#ensemble-learning"><i class="fa fa-check"></i><b>13.2</b> Ensemble learning</a><ul>
<li class="chapter" data-level="13.2.1" data-path="tree-based-models.html"><a href="tree-based-models.html#bagging"><i class="fa fa-check"></i><b>13.2.1</b> Bagging</a></li>
<li class="chapter" data-level="13.2.2" data-path="tree-based-models.html"><a href="tree-based-models.html#boosting"><i class="fa fa-check"></i><b>13.2.2</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="tree-based-models.html"><a href="tree-based-models.html#random-forests"><i class="fa fa-check"></i><b>13.3</b> Random Forests</a><ul>
<li class="chapter" data-level="13.3.1" data-path="tree-based-models.html"><a href="tree-based-models.html#model-form-2"><i class="fa fa-check"></i><b>13.3.1</b> Model form</a></li>
<li class="chapter" data-level="13.3.2" data-path="tree-based-models.html"><a href="tree-based-models.html#example-3"><i class="fa fa-check"></i><b>13.3.2</b> Example</a></li>
<li class="chapter" data-level="13.3.3" data-path="tree-based-models.html"><a href="tree-based-models.html#variable-importance"><i class="fa fa-check"></i><b>13.3.3</b> Variable Importance</a></li>
<li class="chapter" data-level="13.3.4" data-path="tree-based-models.html"><a href="tree-based-models.html#partial-dependence"><i class="fa fa-check"></i><b>13.3.4</b> Partial dependence</a></li>
<li class="chapter" data-level="13.3.5" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-3"><i class="fa fa-check"></i><b>13.3.5</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="tree-based-models.html"><a href="tree-based-models.html#gradient-boosted-trees"><i class="fa fa-check"></i><b>13.4</b> Gradient Boosted Trees</a><ul>
<li class="chapter" data-level="13.4.1" data-path="tree-based-models.html"><a href="tree-based-models.html#parameters"><i class="fa fa-check"></i><b>13.4.1</b> Parameters</a></li>
<li class="chapter" data-level="13.4.2" data-path="tree-based-models.html"><a href="tree-based-models.html#example-4"><i class="fa fa-check"></i><b>13.4.2</b> Example</a></li>
<li class="chapter" data-level="13.4.3" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-4"><i class="fa fa-check"></i><b>13.4.3</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="tree-based-models.html"><a href="tree-based-models.html#exercises-1"><i class="fa fa-check"></i><b>13.5</b> Exercises</a><ul>
<li class="chapter" data-level="13.5.1" data-path="tree-based-models.html"><a href="tree-based-models.html#rf-with-randomforest"><i class="fa fa-check"></i><b>13.5.1</b> 1. RF with <code>randomForest</code></a></li>
<li class="chapter" data-level="13.5.2" data-path="tree-based-models.html"><a href="tree-based-models.html#rf-tuning-with-caret"><i class="fa fa-check"></i><b>13.5.2</b> 2. RF tuning with <code>caret</code></a></li>
<li class="chapter" data-level="13.5.3" data-path="tree-based-models.html"><a href="tree-based-models.html#tuning-a-gbm-with-caret"><i class="fa fa-check"></i><b>13.5.3</b> 3. Tuning a GBM with <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="tree-based-models.html"><a href="tree-based-models.html#answers-to-exercises-1"><i class="fa fa-check"></i><b>13.6</b> Answers to Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>14</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="14.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#principal-componant-analysis-pca"><i class="fa fa-check"></i><b>14.1</b> Principal Componant Analysis (PCA)</a><ul>
<li class="chapter" data-level="14.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-pca-on-us-arrests"><i class="fa fa-check"></i><b>14.1.1</b> Example: PCA on US Arrests</a></li>
<li class="chapter" data-level="14.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-pca-on-cancel-cells"><i class="fa fa-check"></i><b>14.1.2</b> Example: PCA on Cancel Cells</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering"><i class="fa fa-check"></i><b>14.2</b> Clustering</a><ul>
<li class="chapter" data-level="14.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>14.2.1</b> K-Means Clustering</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#hierarchical-clustering"><i class="fa fa-check"></i><b>14.3</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="14.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-clustering-cancel-cells"><i class="fa fa-check"></i><b>14.3.1</b> Example: Clustering Cancel Cells</a></li>
<li class="chapter" data-level="14.3.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#references-1"><i class="fa fa-check"></i><b>14.3.2</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="practice-exams.html"><a href="practice-exams.html"><i class="fa fa-check"></i><b>15</b> Practice Exams</a></li>
<li class="chapter" data-level="16" data-path="references-2.html"><a href="references-2.html"><i class="fa fa-check"></i><b>16</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Exam PA Study Manual</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="generalized-linear-models-glms" class="section level1">
<h1><span class="header-section-number"> 10</span> Generalized linear models (GLMs)</h1>
<p>The linear model that we have considered up to this point, what we called “OLS”, assumes that the response is a linear combination of the predictor variables. For an error term <span class="math inline">\(\epsilon_i \sim N(0,\sigma^2)\)</span>, this is assumes that</p>
<p><span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p + \epsilon
\]</span></p>
<p>In matrix notation, if <span class="math inline">\(X\)</span> is the matrix made up of columns <span class="math inline">\(X_1, ..., X_p\)</span>, then</p>
<p><span class="math display">\[
\mathbf{Y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}
\]</span></p>
<p>Another way of saying this is that “after we adjust for the data, the error is normally distributed and the variance is constant.” If <span class="math inline">\(I\)</span> is an n-by-in identity matrix, and <span class="math inline">\(\sigma^2 I\)</span> is the covariance matrix, then</p>
<p><span class="math display">\[
\mathbf{Y|X} \sim N( \mathbf{X \beta}, \mathbf{\sigma^2} I)
\]</span></p>
<p>Because this notation is getting too cumbersome, we’re going to stop using bold letters to denote matrices and just use non-bold characters. From now on, <span class="math inline">\(\mathbf{X}\)</span> is the same as <span class="math inline">\(X\)</span>.</p>
<p>These assumptions can be expressed in two parts:</p>
<ol style="list-style-type: decimal">
<li><p>A <em>random component</em>: The response variable <span class="math inline">\(Y|X\)</span> is normally distributed with mean <span class="math inline">\(\mu = \mu(X) = E(Y|X)\)</span></p></li>
<li><p>A link between the response and the covariates (also known as the systemic component) <span class="math inline">\(\mu(X) = X\beta\)</span></p></li>
</ol>
<p>In words, this is saying that each observation follows a normal distribution which has a mean that is equal to the linear predictor.</p>
<div id="the-generalized-linear-model" class="section level2">
<h2><span class="header-section-number">10.1</span> The generalized linear model</h2>
<p>Just as the name implies, GLMs are more <em>general</em> in that they are more flexible. We relax these two assumptions by saying that the model is defined by</p>
<ol style="list-style-type: decimal">
<li><p>A random component: <span class="math inline">\(Y|X \sim \text{some exponential family distribution}\)</span></p></li>
<li><p>A link: between the random component and covariates:</p></li>
</ol>
<p><span class="math display">\[g(\mu(X)) = X\beta\]</span>
where <span class="math inline">\(g\)</span> is called the <em>link function</em> and <span class="math inline">\(\mu = E[Y|X]\)</span>.</p>
<p>In words, this is saying that each observation follows <em>some type of exonential distrubution</em> (Gamma, Inverse Gaussian, Poisson, etc.) and that distribution has a mean which is related to the linear predictor through the link function. Additionally, there is a <em>dispersion</em> parameter, but that is more more info that is needed here. For an explanation, see <a href="https://www.casact.org/pubs/monographs/papers/05-Goldburd-Khare-Tevet.pdf">Ch. 2.2 of CAS Monograph 5</a>.</p>
<p>The possible combinations of link functions and distribution families are summarized nicely on <a href="https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function">Wikipedia</a>.</p>
<div class="figure"><span id="fig:unnamed-chunk-14"></span>
<img src="images/glm_links.png" alt="Distribution-Link Function Combinations" width="804" />
<p class="caption">
Figure 10.1: Distribution-Link Function Combinations
</p>
</div>
<p>For this exam, a common question is to ask candiates to choose the best distribution and link function. There is no all-encompasing answer, but a few suggestions are</p>
<ul>
<li>If <span class="math inline">\(Y\)</span> is counting something, such as the number of claims, number of accidents, or some other discrete and positive counting sequence, use the Poisson;</li>
<li>If <span class="math inline">\(Y\)</span> contains negative values, then do not use the Exponential, Gamma, or Inverse Gaussian as these are strictly positive. Conversely, if <span class="math inline">\(Y\)</span> is only positive, such as the price of a policy (price is always &gt; 0), or the claim costs, then these are good choices;</li>
<li>If <span class="math inline">\(Y\)</span> is binary, the the binomial response with either a Probit or Logit link. The Logit is more common.</li>
<li>If <span class="math inline">\(Y\)</span> has more than two categories, the multinomial distribution with either the Probit or Logic link (See Logistic Regression)</li>
</ul>
</div>
<div id="interpretation" class="section level2">
<h2><span class="header-section-number">10.2</span> Interpretation</h2>
<p>The exam will always ask you to interpret the GLM. These questions can usually be answered by inverting the link function and interpreting the coefficients. In the case of the log link, simply take the exponent of the coefficients and each of these represents a “relativity” factor.</p>
<p><span class="math display">\[
log(\mathbf{\hat{y}}) = \mathbf{X} \mathbf{\beta} \Rightarrow \mathbf{\hat{y}} = e^{\mathbf{X} \mathbf{\beta}}
\]</span></p>
<p>For a single observation <span class="math inline">\(y_i\)</span>, this is</p>
<p><span class="math display">\[
\text{exp}(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip}) = \\
e^{\beta_0} e^{\beta_1 x_{i1}}e^{\beta_2 x_{i2}} ...  e^{\beta_p x_{ip}} = 
R_0 R_2 R_3 ... R_{p}
\]</span></p>
<p>Where <span class="math inline">\(R_k\)</span> is the <em>relativity</em> of the kth variable. This terminology is from insurance ratemaking, where actuaries need to be able to explain the impact of each variable in pricing insurance. The data science community does not use this language.</p>
<p>For binary outcomes with logit or probit link, there is no easy interpretation. This has come up in at least one past sample exam, and the solution was to create “psuedo” observations and observe how changing each <span class="math inline">\(x_k\)</span> would change the predicted value. Due to the time requirements, this is unlikely to come up on an exam. So if you are asked to use a logit or probit link, saying that the result is not easy to interpret should suffice.</p>
</div>
<div id="residuals" class="section level2">
<h2><span class="header-section-number">10.3</span> Residuals</h2>
<p>The word “residual” by itself actually means the “raw residual” in GLM language. This is the difference in actual vs. predicted values.</p>
<p><span class="math display">\[\text{Raw Residual} = y_i - \hat{y_i}\]</span></p>
<p>This are not meaningful for GLMs with non-Gaussian response families because the distribution changes depending on the response family chosen. To adjust for this, we need the concept of <em>deviance residual</em>.</p>
<p>To paraphrase from this paper from the University of Oxford:</p>
<p>www.stats.ox.ac.uk/pub/bdr/IAUL/ModellingLecture5.pdf</p>
<p>Deviance is a way of assessing the adequacy of a model by comparing it with a more general
model with the maximum number of parameters that can be estimated. It is referred to
as the saturated model. In the saturated model there is basically one parameter per
observation. The deviance assesses the goodness of fit for the model by looking at the
difference between the log-likelihood functions of the saturated model and the model
under investigation, i.e. <span class="math inline">\(l(b_{sat},y) - l(b,y)\)</span>. Here sat <span class="math inline">\(b_{sat}\)</span> denotes the maximum likelihood
estimator of the parameter vector of the saturated model, <span class="math inline">\(\beta_{sat}\)</span> , and <span class="math inline">\(b\)</span> is the maximum
likelihood estimator of the parameters of the model under investigation, <span class="math inline">\(\beta\)</span>. The maximum likelihood estimator is the estimator that maximises the likelihood function. <strong>The deviance is defined as</strong></p>
<p><span class="math display">\[D = 2[l(b_{sat},y) - l(b,y)]\]</span>
The deviance residual uses the deviance of the ith observation <span class="math inline">\(d_i\)</span> and then takes the square root and applies the same sign (aka, the + or - part) of the raw residual.</p>
<p><span class="math display">\[\text{Deviance Residual} = \text{sign}(y_i - \hat{y_i})\sqrt{d_i}\]</span></p>
</div>
<div id="example-1" class="section level2">
<h2><span class="header-section-number">10.4</span> Example</h2>
<p>Just as with OLS, there is a <code>formula</code> and <code>data argument</code>. In addition, we need to specify the response distribution and link function.</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb184-1" title="1">model =<span class="st"> </span><span class="kw">glm</span>(<span class="dt">formula =</span> charges <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>sex <span class="op">+</span><span class="st"> </span>smoker, </a>
<a class="sourceLine" id="cb184-2" title="2">            <span class="dt">family =</span> <span class="kw">Gamma</span>(<span class="dt">link =</span> <span class="st">&quot;log&quot;</span>),</a>
<a class="sourceLine" id="cb184-3" title="3">            <span class="dt">data =</span> health_insurance)</a></code></pre></div>
<p>We see that <code>age</code>, <code>sex</code>, and <code>smoker</code> are all significant (p &lt;0.01). Reading off the coefficient signs, we see that claims</p>
<ul>
<li>Increase as age increases</li>
<li>Are higher for women</li>
<li>Are higher for smokers</li>
</ul>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb185-1" title="1">model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">tidy</span>()</a></code></pre></div>
<pre><code>## # A tibble: 4 x 5
##   term        estimate std.error statistic   p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   7.82     0.0600     130.   0.       
## 2 age           0.0290   0.00134     21.6  3.40e- 89
## 3 sexmale      -0.0468   0.0377      -1.24 2.15e-  1
## 4 smokeryes     1.50     0.0467      32.1  3.25e-168</code></pre>
<p>Below you can see graph of deviance residuals vs. the predicted values.</p>
<p><strong>If this were a perfect model, all of these below assumptions would be met:</strong></p>
<ul>
<li>Scattered around zero?</li>
<li>Constant variance?</li>
<li>No obvious pattern?</li>
</ul>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb187-1" title="1"><span class="kw">plot</span>(model, <span class="dt">which =</span> <span class="dv">3</span>)</a></code></pre></div>
<p><img src="05-linear-models_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>The quantile-quantile (QQ) plot shows the quantiles of the deviance residuals (i.e., after adjusting for the Gamma distribution) against theoretical Gaussian quantiles.</p>
<p><strong>In a perfect model, all of these assumptions would be met:</strong></p>
<ul>
<li>Points lie on a straight line?<br />
</li>
<li>Tails are not significantly above or below line? Some tail deviation is ok.</li>
<li>No sudden “jumps”? This indicates many <span class="math inline">\(Y\)</span>’s which have the same value, such as insurance claims which all have the exact value of $100.00 or $0.00.</li>
</ul>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb188-1" title="1"><span class="kw">plot</span>(model, <span class="dt">which =</span> <span class="dv">2</span>)</a></code></pre></div>
<p><img src="05-linear-models_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
<div id="combinations-of-link-and-response-family-examples" class="section level2">
<h2><span class="header-section-number">10.5</span> Combinations of Link and Response Family Examples</h2>
<p>What is an example of when to use a log link with a guassian response? What about a Gamma family with an inverse link? What about an inverse Gaussian response and an inverse square link? As these questions illustrate, there are many combinations of link and response family. In the real world, a model rarely fits perfectly, and so often these choices come down to the judgement of the modeler - which model is the best fit and meets the business objectives?</p>
<p>However, there is one way that we can know for certain which link and response family is the best, and that is if we generate the data ourselves.</p>
<p>Recall that a GLM has two parts:</p>
<ol style="list-style-type: decimal">
<li><p>A <strong>random component</strong>: <span class="math inline">\(Y|X \sim \text{some exponential family distribution}\)</span></p></li>
<li><p>A <strong>link function</strong>: between the random component and the covariates: <span class="math inline">\(g(\mu(X)) = X\beta\)</span> where <span class="math inline">\(\mu = E[Y|X]\)</span></p></li>
</ol>
<p><strong>Following this recipe, we can simulate data from any combination of link function and response family. This helps us to understand the GLM framework very clearly.</strong></p>
<div id="gaussian-response-with-log-link" class="section level3">
<h3><span class="header-section-number">10.5.1</span> Gaussian Response with Log Link</h3>
<p>We create a function that takes in data <span class="math inline">\(x\)</span> and returns a guassian random variable that has mean equal to the inverse link, which in the case of a log link is the exponent. We add 10 to <span class="math inline">\(x\)</span> so that the values will always be positive, as will be described later on.</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb189-1" title="1">sim_norm &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb189-2" title="2">  <span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> <span class="kw">exp</span>(<span class="dv">10</span> <span class="op">+</span><span class="st"> </span>x), <span class="dt">sd =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb189-3" title="3">}</a></code></pre></div>
<p>The values of <span class="math inline">\(X\)</span> do not need to be normal. The above assumption is merely that the mean of the response <span class="math inline">\(Y\)</span> is related to <span class="math inline">\(X\)</span> through the link function, <code>mean = exp(10 + x)</code>, and that the distribution is normal. This has been accomplished with <code>rnorm</code> already. For illustration, here we use <span class="math inline">\(X\)</span>’s from a uniform distribution.</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb190-1" title="1">data &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">runif</span>(<span class="dv">1000</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb190-2" title="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> x <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">map_dbl</span>(sim_norm))</a></code></pre></div>
<p>We already know what the answer is: a gaussian response with a log link. We fit a GLM and see a perfect fit.</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb191-1" title="1">glm &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">family =</span> <span class="kw">gaussian</span>(<span class="dt">link =</span> <span class="st">&quot;log&quot;</span>), <span class="dt">data =</span> data)</a>
<a class="sourceLine" id="cb191-2" title="2"></a>
<a class="sourceLine" id="cb191-3" title="3"><span class="kw">summary</span>(glm)</a></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x, family = gaussian(link = &quot;log&quot;), data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.0004  -0.6964   0.0005   0.7266   3.0718  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.000e+01  2.195e-06 4554982   &lt;2e-16 ***
## x           1.000e+00  3.085e-06  324117   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 1.067056)
## 
##     Null deviance: 1.2235e+11  on 999  degrees of freedom
## Residual deviance: 1.0649e+03  on 998  degrees of freedom
## AIC: 2906.8
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb193-1" title="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb193-2" title="2"><span class="kw">plot</span>(glm, <span class="dt">cex =</span> <span class="fl">0.4</span>)</a></code></pre></div>
<p><img src="05-linear-models_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
</div>
<div id="gaussian-response-with-inverse-link" class="section level3">
<h3><span class="header-section-number">10.5.2</span> Gaussian Response with Inverse Link</h3>
<p>The same steps are repeated except the link function is now the inverse, <code>mean = 1/x</code>. We see that some values of <span class="math inline">\(Y\)</span> are negative, which is ok.</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb194-1" title="1">sim_norm &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb194-2" title="2">  <span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> <span class="dv">1</span><span class="op">/</span>x, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb194-3" title="3">}</a>
<a class="sourceLine" id="cb194-4" title="4"></a>
<a class="sourceLine" id="cb194-5" title="5">data &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">runif</span>(<span class="dv">10000</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb194-6" title="6"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> x <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">map_dbl</span>(sim_norm))</a>
<a class="sourceLine" id="cb194-7" title="7"><span class="kw">summary</span>(data)</a></code></pre></div>
<pre><code>##        x                   y           
##  Min.   :0.0001064   Min.   :  -1.957  
##  1st Qu.:0.2532864   1st Qu.:   1.259  
##  Median :0.5028875   Median :   2.334  
##  Mean   :0.5018599   Mean   :  10.214  
##  3rd Qu.:0.7507694   3rd Qu.:   4.232  
##  Max.   :0.9998552   Max.   :9394.790</code></pre>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb196-1" title="1">glm &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">family =</span> <span class="kw">gaussian</span>(<span class="dt">link =</span> <span class="st">&quot;inverse&quot;</span>), <span class="dt">data =</span> data)</a>
<a class="sourceLine" id="cb196-2" title="2"></a>
<a class="sourceLine" id="cb196-3" title="3"><span class="kw">summary</span>(glm)</a></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x, family = gaussian(link = &quot;inverse&quot;), data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.5186  -0.6747   0.0105   0.6883   3.3764  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 3.596e-08  2.587e-08    1.39    0.165    
## x           9.998e-01  2.072e-04 4824.13   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 1.007483)
## 
##     Null deviance: 203905704  on 9999  degrees of freedom
## Residual deviance:     10073  on 9998  degrees of freedom
## AIC: 28457
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb198-1" title="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb198-2" title="2"><span class="kw">plot</span>(glm, <span class="dt">cex =</span> <span class="fl">0.4</span>)</a></code></pre></div>
<p><img src="05-linear-models_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
<div id="gaussian-response-with-identity-link" class="section level3">
<h3><span class="header-section-number">10.5.3</span> Gaussian Response with Identity Link</h3>
<p>And now the link is the identity, <code>mean = x</code>.</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb199-1" title="1">sim_norm &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb199-2" title="2">  <span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> x, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb199-3" title="3">}</a>
<a class="sourceLine" id="cb199-4" title="4"></a>
<a class="sourceLine" id="cb199-5" title="5">data &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">rnorm</span>(<span class="dv">10000</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb199-6" title="6"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> x <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">map_dbl</span>(sim_norm))</a>
<a class="sourceLine" id="cb199-7" title="7"></a>
<a class="sourceLine" id="cb199-8" title="8">glm &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">family =</span> <span class="kw">gaussian</span>(<span class="dt">link =</span> <span class="st">&quot;identity&quot;</span>), <span class="dt">data =</span> data)</a>
<a class="sourceLine" id="cb199-9" title="9"></a>
<a class="sourceLine" id="cb199-10" title="10"><span class="kw">summary</span>(glm)</a></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x, family = gaussian(link = &quot;identity&quot;), data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -4.4461  -0.6853   0.0129   0.6794   3.6661  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.01393    0.01010   1.379    0.168    
## x            0.98236    0.01005  97.727   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 1.020901)
## 
##     Null deviance: 19957  on 9999  degrees of freedom
## Residual deviance: 10207  on 9998  degrees of freedom
## AIC: 28590
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb201-1" title="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb201-2" title="2"><span class="kw">plot</span>(glm, <span class="dt">cex =</span> <span class="fl">0.4</span>)</a></code></pre></div>
<p><img src="05-linear-models_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
</div>
<div id="gaussian-response-with-log-link-and-negative-values" class="section level3">
<h3><span class="header-section-number">10.5.4</span> Gaussian Response with Log Link and Negative Values</h3>
<p>By Gaussian response we say that the <em>mean</em> of the response is Gaussian. The range of a normal random variable is <span class="math inline">\((-\infty, +\infty)\)</span>, which means that negative values are always possible. Now, if the mean is a large positive number, than negative values are much less likely but still possible: about 95% of the observations will be within 2 standard deviations of the mean.</p>
<p>We see below that there are some <span class="math inline">\(Y\)</span> values which are negative.</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb202-1" title="1">sim_norm &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb202-2" title="2">  <span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> <span class="kw">exp</span>(x), <span class="dt">sd =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb202-3" title="3">}</a>
<a class="sourceLine" id="cb202-4" title="4"></a>
<a class="sourceLine" id="cb202-5" title="5">data &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">runif</span>(<span class="dv">1000</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb202-6" title="6"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> x <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">map_dbl</span>(sim_norm))</a>
<a class="sourceLine" id="cb202-7" title="7"><span class="kw">summary</span>(data)</a></code></pre></div>
<pre><code>##        x                   y          
##  Min.   :0.0000122   Min.   :-1.8709  
##  1st Qu.:0.2354295   1st Qu.: 0.9815  
##  Median :0.5055838   Median : 1.6969  
##  Mean   :0.4968540   Mean   : 1.7275  
##  3rd Qu.:0.7611768   3rd Qu.: 2.4854  
##  Max.   :0.9993455   Max.   : 5.0233</code></pre>
<p>We can also see this from the histogram.</p>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb204-1" title="1">data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_density</span>( <span class="dt">fill =</span> <span class="dv">1</span>, <span class="dt">alpha =</span> <span class="fl">0.3</span>)</a></code></pre></div>
<p><img src="05-linear-models_files/figure-html/unnamed-chunk-26-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>If we try to fit a GLM with a log link, there is an error.</p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb205-1" title="1">glm &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">family =</span> <span class="kw">gaussian</span>(<span class="dt">link =</span> <span class="st">&quot;log&quot;</span>), <span class="dt">data =</span> data)</a></code></pre></div>
<p><code>Error in eval(family$initialize) : cannot find valid starting values: please specify some</code></p>
<p>This is because the domain of the natural logarithm only includes positive numbers, and we just tried to take the log of negative numbers.</p>
<p>Our initial reaction might be to add some constant to each <span class="math inline">\(Y\)</span>, say 10 for instance, so that they are all positive. This does produce a model which is a good fit.</p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb206-1" title="1">glm &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">+</span><span class="st"> </span><span class="dv">10</span> <span class="op">~</span><span class="st"> </span>x, <span class="dt">family =</span> <span class="kw">gaussian</span>(<span class="dt">link =</span> <span class="st">&quot;log&quot;</span>), <span class="dt">data =</span> data)</a>
<a class="sourceLine" id="cb206-2" title="2"><span class="kw">summary</span>(glm)</a></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y + 10 ~ x, family = gaussian(link = &quot;log&quot;), data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.1527  -0.6538  -0.0336   0.6753   3.3219  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 2.394232   0.005463  438.25   &lt;2e-16 ***
## x           0.134685   0.009158   14.71   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 0.987688)
## 
##     Null deviance: 1198.70  on 999  degrees of freedom
## Residual deviance:  985.71  on 998  degrees of freedom
## AIC: 2829.5
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb208-1" title="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb208-2" title="2"><span class="kw">plot</span>(glm, <span class="dt">cex =</span> <span class="fl">0.4</span>)</a></code></pre></div>
<p><img src="05-linear-models_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>We see that on average, the predictions are 10 higher than the target. This is no surprise since <span class="math inline">\(E[Y + 10] = E[Y] + 10\)</span>.</p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb209-1" title="1">y &lt;-<span class="st"> </span>data<span class="op">$</span>y </a>
<a class="sourceLine" id="cb209-2" title="2">y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(glm, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</a>
<a class="sourceLine" id="cb209-3" title="3"><span class="kw">mean</span>(y_hat) <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(y)</a></code></pre></div>
<pre><code>## [1] 9.99995</code></pre>
<p>But we see that the actual predictions are bad. If we were to loot at the R-squared, MAE, RMSE, or any other metric it would tell us the same story. This is because our GLM assumption is <strong>not</strong> that <span class="math inline">\(Y\)</span> is related to the link function of <span class="math inline">\(X\)</span>, but that the <strong>mean</strong> of <span class="math inline">\(Y\)</span> is.</p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb211-1" title="1"><span class="kw">tibble</span>(<span class="dt">y =</span> y, <span class="dt">y_hat =</span> y_hat <span class="op">-</span><span class="st"> </span><span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(y, y_hat)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</a></code></pre></div>
<p><img src="05-linear-models_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>One solution is to adjust the <span class="math inline">\(X\)</span> which the model is based on. Add a constant term to <span class="math inline">\(X\)</span> so that the mean of <span class="math inline">\(Y\)</span> is larger, and hence <span class="math inline">\(Y\)</span> is non zero. While is a viable approach in the case of only one predictor variable, with more predictors this would not be easy to do.</p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb212-1" title="1">data &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">runif</span>(<span class="dv">1000</span>) <span class="op">+</span><span class="st"> </span><span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb212-2" title="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> x <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">map_dbl</span>(sim_norm))</a>
<a class="sourceLine" id="cb212-3" title="3"><span class="kw">summary</span>(data)</a></code></pre></div>
<pre><code>##        x               y        
##  Min.   :10.00   Min.   :22028  
##  1st Qu.:10.25   1st Qu.:28291  
##  Median :10.52   Median :36893  
##  Mean   :10.51   Mean   :38160  
##  3rd Qu.:10.77   3rd Qu.:47441  
##  Max.   :11.00   Max.   :59842</code></pre>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb214-1" title="1">glm &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">family =</span> <span class="kw">gaussian</span>(<span class="dt">link =</span> <span class="st">&quot;log&quot;</span>), <span class="dt">data =</span> data)</a>
<a class="sourceLine" id="cb214-2" title="2"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb214-3" title="3"><span class="kw">plot</span>(glm, <span class="dt">cex =</span> <span class="fl">0.4</span>)</a></code></pre></div>
<p><img src="05-linear-models_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>A better approach may be to use an inverse link even though the data was generated from a log link. This is a good illustration of the saying “all models are wrong, but some are useful” in that the statistical assumption of the model is not correct but the model still works.</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb215-1" title="1">data &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">runif</span>(<span class="dv">1000</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb215-2" title="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> x <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">map_dbl</span>(sim_norm))</a>
<a class="sourceLine" id="cb215-3" title="3">glm &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">family =</span> <span class="kw">gaussian</span>(<span class="dt">link =</span> <span class="st">&quot;inverse&quot;</span>), <span class="dt">data =</span> data)</a>
<a class="sourceLine" id="cb215-4" title="4"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb215-5" title="5"><span class="kw">plot</span>(glm, <span class="dt">cex =</span> <span class="fl">0.4</span>)</a></code></pre></div>
<p><img src="05-linear-models_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb216-1" title="1"><span class="kw">summary</span>(glm)</a></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x, family = gaussian(link = &quot;inverse&quot;), data = data)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -3.10622  -0.63739  -0.00542   0.63167   3.06741  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.94957    0.03515   27.02   &lt;2e-16 ***
## x           -0.58667    0.04360  -13.46   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 0.9967997)
## 
##     Null deviance: 1218.78  on 999  degrees of freedom
## Residual deviance:  994.82  on 998  degrees of freedom
## AIC: 2838.7
## 
## Number of Fisher Scoring iterations: 6</code></pre>
</div>
<div id="gamma-response-with-log-link" class="section level3">
<h3><span class="header-section-number">10.5.5</span> Gamma Response with Log Link</h3>
<p>The gamma distribution with rate parameter <span class="math inline">\(\alpha\)</span> and scale parameter <span class="math inline">\(\theta\)</span> is density.</p>
<p><span class="math display">\[f(y) = \frac{(y/\theta)^\alpha}{x \Gamma(\alpha)}e^{-x/\theta}\]</span></p>
<p>The mean is <span class="math inline">\(\alpha\theta\)</span>.</p>
<p>Let’s use a gamma with shape 2 and scale 0.5, which has mean 1.</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb218-1" title="1">gammas &lt;-<span class="st"> </span><span class="kw">rgamma</span>(<span class="dv">1000</span>, <span class="dt">shape=</span><span class="dv">2</span>, <span class="dt">scale =</span> <span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb218-2" title="2"><span class="kw">mean</span>(gammas)</a></code></pre></div>
<pre><code>## [1] 0.9873887</code></pre>
<p>We then generate random gamma values. Because the mean now depends on two paramters instead of one, which was just <span class="math inline">\(\mu\)</span> in the Guassian case, we need to use a slightly different approach to simulate the random values. The link function here is seen in <code>exp(x)</code>.</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb220-1" title="1"><span class="co">#random component</span></a>
<a class="sourceLine" id="cb220-2" title="2">x &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1000</span>, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb220-3" title="3"></a>
<a class="sourceLine" id="cb220-4" title="4"><span class="co">#relate Y to X with a log link function</span></a>
<a class="sourceLine" id="cb220-5" title="5">y &lt;-<span class="st"> </span>gammas<span class="op">*</span><span class="kw">exp</span>(x)</a>
<a class="sourceLine" id="cb220-6" title="6"></a>
<a class="sourceLine" id="cb220-7" title="7">data &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> x, <span class="dt">y  =</span> y)</a>
<a class="sourceLine" id="cb220-8" title="8"><span class="kw">summary</span>(data)</a></code></pre></div>
<pre><code>##        x                 y            
##  Min.   : 0.2452   Min.   :0.000e+00  
##  1st Qu.:27.0464   1st Qu.:4.946e+11  
##  Median :51.0196   Median :1.057e+22  
##  Mean   :51.0666   Mean   :2.531e+41  
##  3rd Qu.:75.3442   3rd Qu.:4.239e+32  
##  Max.   :99.9213   Max.   :2.693e+43</code></pre>
<p>As expected, the residual plots are all perfect because the model is perfect.</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb222-1" title="1">glm &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">family =</span> <span class="kw">Gamma</span>(<span class="dt">link =</span> <span class="st">&quot;log&quot;</span>), <span class="dt">data =</span> data)</a>
<a class="sourceLine" id="cb222-2" title="2"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb222-3" title="3"><span class="kw">plot</span>(glm, <span class="dt">cex =</span> <span class="fl">0.4</span>)</a></code></pre></div>
<p><img src="05-linear-models_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>If we had tried using an inverse instead of the log, the residual plots would look much worse.</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb223-1" title="1">glm &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">family =</span> <span class="kw">Gamma</span>(<span class="dt">link =</span> <span class="st">&quot;inverse&quot;</span>), <span class="dt">data =</span> data)</a>
<a class="sourceLine" id="cb223-2" title="2"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb223-3" title="3"><span class="kw">plot</span>(glm, <span class="dt">cex =</span> <span class="fl">0.4</span>)</a></code></pre></div>
<pre><code>## Warning in sqrt(crit * p * (1 - hh)/hh): NaNs produced

## Warning in sqrt(crit * p * (1 - hh)/hh): NaNs produced</code></pre>
<p><img src="05-linear-models_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
</div>
<div id="gamma-with-inverse-link" class="section level3">
<h3><span class="header-section-number">10.5.6</span> Gamma with Inverse Link</h3>
<p>With the inverse link, the mean has a factor <code>1/(x + 1)</code>. Note that we need to add 1 to x to avoid dividing by zero.</p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb225-1" title="1"><span class="co">#relate Y to X with a log link function</span></a>
<a class="sourceLine" id="cb225-2" title="2">y &lt;-<span class="st"> </span>gammas<span class="op">*</span><span class="dv">1</span><span class="op">/</span>(x <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb225-3" title="3"></a>
<a class="sourceLine" id="cb225-4" title="4">data &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> x, <span class="dt">y  =</span> y)</a>
<a class="sourceLine" id="cb225-5" title="5"><span class="kw">summary</span>(data)</a></code></pre></div>
<pre><code>##        x                 y            
##  Min.   : 0.2452   Min.   :0.0005277  
##  1st Qu.:27.0464   1st Qu.:0.0084840  
##  Median :51.0196   Median :0.0167640  
##  Mean   :51.0666   Mean   :0.0485119  
##  3rd Qu.:75.3442   3rd Qu.:0.0365838  
##  Max.   :99.9213   Max.   :1.7125489</code></pre>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb227-1" title="1">glm &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">family =</span> <span class="kw">Gamma</span>(<span class="dt">link =</span> <span class="st">&quot;inverse&quot;</span>), <span class="dt">data =</span> data)</a>
<a class="sourceLine" id="cb227-2" title="2"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb227-3" title="3"><span class="kw">plot</span>(glm, <span class="dt">cex =</span> <span class="fl">0.4</span>)</a></code></pre></div>
<p><img src="05-linear-models_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
</div>
</div>
<div id="log-transforms-of-continuous-predictors" class="section level2">
<h2><span class="header-section-number">10.6</span> Log transforms of continuous predictors</h2>
<p>When a log link is used, taking the natural logs of continuous variables allows for the scale of each predictor to match the scale of the thing that they are predicting, the log of the mean of the response. In addition, when the distribution of the continuous variable is skewed, taking the log helps to make it more symmetric.</p>
<p>After taking the log of a predictor, the interpretation becomes a <em>power transform</em> of the original variable.</p>
<p>For <span class="math inline">\(\mu\)</span> the mean response,</p>
<p><span class="math display">\[log(\mu) = \beta_0 + \beta_1 log(X)\]</span>
To solve for <span class="math inline">\(\mu\)</span>, take the exonent of both sides</p>
<p><span class="math display">\[\mu = e^{\beta_1} e^{\beta_1 log(X)} = e^{\beta_0} X^{\beta_1}\]</span></p>
</div>
<div id="reference-levels" class="section level2">
<h2><span class="header-section-number">10.7</span> Reference levels</h2>
<p>When a categorical variable is used in a GLM, the model actually uses indicator variables for each level. The default reference level is the order of the R factors. For the <code>sex</code> variable, the order is <code>female</code> and then <code>male</code>. This means that the base level is <code>female</code> by default.</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb228-1" title="1">health_insurance<span class="op">$</span>sex <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.factor</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">levels</span>()</a></code></pre></div>
<pre><code>## [1] &quot;female&quot; &quot;male&quot;</code></pre>
<p>Why does this matter? Statistically, the coefficients are most stable when there are more observations.</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb230-1" title="1">health_insurance<span class="op">$</span>sex <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.factor</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summary</span>()</a></code></pre></div>
<pre><code>## female   male 
##    662    676</code></pre>
<p>There is already a function to do this in the <code>tidyverse</code> called <code>fct_infreq</code>. Let’s quickly fix the <code>sex</code> column so that these factor levels are in order of frequency.</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb232-1" title="1">health_insurance &lt;-<span class="st"> </span>health_insurance <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb232-2" title="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">sex =</span> <span class="kw">fct_infreq</span>(sex))</a></code></pre></div>
<p>Now <code>male</code> is the base level.</p>
<div class="sourceCode" id="cb233"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb233-1" title="1">health_insurance<span class="op">$</span>sex <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.factor</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">levels</span>()</a></code></pre></div>
<pre><code>## [1] &quot;male&quot;   &quot;female&quot;</code></pre>
</div>
<div id="interactions" class="section level2">
<h2><span class="header-section-number">10.8</span> Interactions</h2>
<p>An interaction occurs when the effect of a variable on the response is different depending on the level of other variables in the model.</p>
<p>Consider this model:</p>
<p>Let <span class="math inline">\(x_2\)</span> be an indicator variable, which is 1 for some records and 0 otherwise.</p>
<p><span class="math display">\[\hat{y_i} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2\]</span></p>
<p>There are now two different linear models dependong on whether <code>x_1</code> is 0 or 1.</p>
<p>When <span class="math inline">\(x_1 = 0\)</span>,</p>
<p><span class="math display">\[\hat{y_i} = \beta_0  + \beta_2 x_2\]</span></p>
<p>and when <span class="math inline">\(x_1 = 1\)</span></p>
<p><span class="math display">\[\hat{y_i} = \beta_0 + \beta_1 + \beta_2 x_2 + \beta_3 x_2\]</span>
By rewriting this we can see that the intercept changes from <span class="math inline">\(\beta_0\)</span> to <span class="math inline">\(\beta_0^*\)</span> and the slope changes from <span class="math inline">\(\beta_1\)</span> to <span class="math inline">\(\beta_1^*\)</span></p>
<p><span class="math display">\[
(\beta_0 + \beta_1) + (\beta_2 + \beta_3 ) x_2 \\
 = \beta_0^* + \beta_1^* x_2
\]</span></p>
<p>The SOA’s modules give an example with the using age and gender as below. This is not a very strong interaction, as the slopes are almost identical across <code>gender</code>.</p>
<div class="sourceCode" id="cb235"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb235-1" title="1">interactions <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb235-2" title="2"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(age, actual, <span class="dt">color =</span> gender)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb235-3" title="3"><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb235-4" title="4"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Age vs. Actual by Gender&quot;</span>, </a>
<a class="sourceLine" id="cb235-5" title="5">       <span class="dt">subtitle =</span> <span class="st">&quot;Interactions imply different slopes&quot;</span>,</a>
<a class="sourceLine" id="cb235-6" title="6">       <span class="dt">caption=</span> <span class="st">&quot;data: interactions&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-43"></span>
<img src="05-linear-models_files/figure-html/unnamed-chunk-43-1.png" alt="Example of weak interaction" width="672" />
<p class="caption">
Figure 10.2: Example of weak interaction
</p>
</div>
<p>Here is a clearer example from the <code>auto_claim</code> data. The lines show the slope of a linear model, assuming that only <code>BLUEBOOK</code> and <code>CAR_TYPE</code> were predictors in the model. You can see that the slope for Sedans and Sports Cars is higher than for Vans and Panel Trucks.</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb236-1" title="1">auto_claim <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb236-2" title="2"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="kw">log</span>(CLM_AMT), <span class="kw">log</span>(BLUEBOOK), <span class="dt">color =</span> CAR_TYPE)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb236-3" title="3"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb236-4" title="4"><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> F) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb236-5" title="5"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Kelly Bluebook Value vs Claim Amount&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-44"></span>
<img src="05-linear-models_files/figure-html/unnamed-chunk-44-1.png" alt="Example of strong interaction" width="672" />
<p class="caption">
Figure 10.3: Example of strong interaction
</p>
</div>
<p>Any time that the effect that one variable has on the response is different depending on the value of other variables we say that there is an interaction. We can also use an hypothesis test with a GLM to check this. Simply include an interaction term and see if the coefficient is zero at the desired significance level.</p>
</div>
<div id="poisson-regression" class="section level2">
<h2><span class="header-section-number">10.9</span> Poisson Regression</h2>
<p>When counting something, numbers can only be positive and increase by increments of 1. Statistically, the name for this is a Poisson Process, which is a model for a serious of discrete events where the average time between events is known, called the “rate” <span class="math inline">\(\lambda\)</span>, but the exact timing of events is unknown. We could just fit a single rate for all observations, but this would often be a simplification. For a time interval of length <span class="math inline">\(m\)</span>, the expected number of events is <span class="math inline">\(\lambda m\)</span>.</p>
<p>By using a GLM, we can fit a different rate for each observation. Because the response is a count, the appropriate response distribution is the Poisson.</p>
<p><span class="math display">\[Y_i|X_i \sim \text{Poisson}(\lambda_i m_i)\]</span></p>
<p>When all observations have the same exposure, <span class="math inline">\(m = 1\)</span>. When the mean of the data is far from the variance, an additional parameter known as the <em>dispersion parameter</em> is used. A classic example is when modeling insurance claim counts which have a lot of zero claims. Then the model is said to be an “over-dispersed Poisson” or “zero-inflated” model.</p>
</div>
<div id="offsets" class="section level2">
<h2><span class="header-section-number">10.10</span> Offsets</h2>
<p>In certain situations, it is convenient to include a constant term in the linear predictor. This is the same as including a variable that has a coefficient equal to 1. We call this an <em>offset</em>.</p>
<p><span class="math display">\[g(\mu) = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p + \text{offset}\]</span></p>
</div>
<div id="tweedie-regression" class="section level2">
<h2><span class="header-section-number">10.11</span> Tweedie regression</h2>
<p>While this topic is briefly mentioned on the modules, the only R libraries which support Tweedie Regression (<code>statmod</code> and <code>tweedie</code>) are not on the syllabus, and so there is no way that the SOA could ask you to build a tweedie model. This means that you can be safely skip this section.</p>
</div>
<div id="stepwise-subset-selection" class="section level2">
<h2><span class="header-section-number">10.12</span> Stepwise subset selection</h2>
<p>In theory, we could test all possible combinations of variables and interaction terms. This includes all <span class="math inline">\(p\)</span> models with one predictor, all p-choose-2 models with two predictors, all p-choose-3 models with three predictors, and so forth. Then we take whichever model has the best performance as the final model.</p>
<p>This “brute force” approach is statistically ineffective: the more variables which are searched, the higher the chance of finding models that overfit.</p>
<p>A subtler method, known as <em>stepwise selection</em>, reduces the chances of overfitting by only looking at the most promising models.</p>
<p><strong>Forward Stepwise Selection:</strong></p>
<ol style="list-style-type: decimal">
<li>Start with no predictors in the model;</li>
<li>Evaluate all <span class="math inline">\(p\)</span> models which use only one predictor and choose the one with the best performance (highest <span class="math inline">\(R^2\)</span> or lowest <span class="math inline">\(\text{RSS}\)</span>);</li>
<li>Repeat the process when adding one additional predictor, and continue until there is a model with one predictor, a model with two predictors, a model with three predictors, and so forth until there are <span class="math inline">\(p\)</span> models;</li>
<li>Select the single best model which has the best <span class="math inline">\(\text{AIC}\)</span>,<span class="math inline">\(\text{BIC}\)</span>, or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
<p><strong>Backward Stepwise Selection:</strong></p>
<ol style="list-style-type: decimal">
<li>Start with a model that contains all predictors;</li>
<li>Create a model which removes all predictors;</li>
<li>Choose the best model which removes all-but-one predictor;</li>
<li>Choose the best model which removes all-but-two predictors;</li>
<li>Continue until there are <span class="math inline">\(p\)</span> models;</li>
<li>Select the single best model which has the best <span class="math inline">\(\text{AIC}\)</span>,<span class="math inline">\(\text{BIC}\)</span>, or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
<p><strong>Both Forward &amp; Backward Selection:</strong></p>
<p>A hybrid approach is to consider use both forward and backward selection. This is done by creating two lists of variables at each step, one from forward and one from backward selection. Then variables from <em>both</em> lists are tested to see if adding or subtracting from the current model would improve the fit or not. ISLR does not mention this directly, however, by default the <code>stepAIC</code> function uses a default of <code>both</code>.</p>
<blockquote>
<p><strong>Tip</strong>: Always load the <code>MASS</code> library before <code>dplyr</code> or <code>tidyverse</code>. Otherwise there will be conflicts as there are functions named <code>select()</code> and <code>filter()</code> in both. Alternatively, specify the library in the function call with <code>dplyr::select()</code>.</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th>Readings</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://www.casact.org/pubs/monographs/papers/05-Goldburd-Khare-Tevet.pdf">CAS Monograph 5 Chapter 2</a></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div id="advantages-and-disadvantages" class="section level2">
<h2><span class="header-section-number">10.13</span> Advantages and disadvantages</h2>
<p>There is usually at least one question on the PA exam which asks you to “list some of the advantages and disadvantages of using this particular model”, and so here is one such list. It is unlikely that the grader will take off points for including too many comments and so a good strategy is to include everything that comes to mind.</p>
<p><strong>GLM Advantages</strong></p>
<ul>
<li>Easy to interpret</li>
<li>Can easily be deployed in spreadsheet format</li>
<li>Handles skewed data through different response distributions</li>
<li>Models the average response which leads to stable predictions on new data</li>
<li>Handles continuous and categorical data</li>
</ul>
<p><strong>GLM Disadvantages</strong></p>
<ul>
<li>Does not select features (without stepwise selection)</li>
<li>Strict assumptions around distribution shape, randomness of error terms, and variable correlations</li>
<li>Unable to detect non-linearity directly (although this can manually be addressed through feature engineering)</li>
<li>Sensitive to outliers</li>
<li>Low predictive power</li>
</ul>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-to-modeling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logistic-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/sdcastillo/PA-R-Study-Manual/edit/master/05-linear-models.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["Exam-PA-Study-Manual.pdf", "Exam-PA-Study-Manual.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
