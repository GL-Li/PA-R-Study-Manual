[
["index.html", "Predictive-Analytics-MAS-R-Study-Manual Chapter 1 Who is this book for", " Predictive-Analytics-MAS-R-Study-Manual Sam Castillo Brian Fannin 2019-10-19 Chapter 1 Who is this book for This book will help you to pass the SOA’s Predictive Analytics Exam and give you insights and useful code snippets that will translate into your every day work flow. Additionally, material is also covered which is on the CAS exams MAS-I and MAS-II. "],
["how-to-use-this-book.html", "Chapter 2 How to Use this book", " Chapter 2 How to Use this book This is like any of the other actuarial exam that you have taken. The format is a 5-hr and 15 minute project that uses RStudio, Excel, and Word. There are three essential skills needed to pass. These are R competency; Business writing ability; Statistics, predictive analytics, and machine learning knowledge. If you already have some of these skills, then a good strategy is to focus on your weak areas. If you are relatively new to all three, great! This means that you will be able to learn a lot of useful skills by reading this manual. "],
["about-the-authors.html", "Chapter 3 About the authors", " Chapter 3 About the authors Sam Castillo has 4+ years of R programming experience while working as an actuarial predictive modeler for the past 3 years. He passed the predictive analytics exam in June of 2019. Brian Fannin etc, etc "],
["intro.html", "Chapter 4 Getting Started 4.1 Installing R and Rstudio 4.2 Setting Up 4.3 R basic syntax 4.4 Data Types 4.5 Rmd file overview 4.6 Read in a data file from local computer in csv format", " Chapter 4 Getting Started 4.1 Installing R and Rstudio Download R Download RStudio 4.2 Setting Up 4.2.1 Using the R version of packages from the SOA (include helper script) You want to use the exact same code that will be on the Prometric Computers. An R library is a software package that provides added flexibility. R is open-source, which means that it is maintained by a community of developers on a volunteer basis. To change the library that R uses, use the libPaths function. #.libPaths(&quot;C:/Users/sam.castillo/Desktop/PA/library/PAlibrary&quot;) 4.2.2 Setting the R Version This book uses Rv3.5. To switch between R versions, Tools -&gt; Global Options -&gt; R Version -&gt; Restart Rstudio 4.3 R basic syntax There are already hundreds of great tutorials online. This will only be a minimal version. 4.4 Data Types Each object in R has a different type. numeric character factors booleans As well as classes matrix data frame tibble (just another name for a data frame) list (most important) 4.4.1 Objects 4.4.2 Functions 4.4.3 Just Copy Someone Elese’s Tutorials 4.5 Rmd file overview 4.6 Read in a data file from local computer in csv format "],
["data-manipulation.html", "Chapter 5 Data Manipulation 5.1 Exercises", " Chapter 5 Data Manipulation About two hours in this exam will be spent just on data manipulation. A common complaint from exam takers who are new to R programming is that this takes too long. Putting in extra practice in this area is garanteed to give you a better score because it will free up time that you can use elsewhere. Each time that you want to “do something” to the data, most of the time there is a function that will do this. 5.0.1 Look at the data The head() or glimpse() functions both suffice. The diamonds data set has one record for each type of diamond. library(tidyverse) mtcars %&gt;% head() ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 mtcars %&gt;% glimpse() ## Observations: 32 ## Variables: 11 ## $ mpg &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.... ## $ cyl &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, ... ## $ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 1... ## $ hp &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, ... ## $ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.9... ## $ wt &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3... ## $ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 2... ## $ vs &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, ... ## $ am &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ... ## $ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, ... ## $ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, ... 5.0.2 Count the data One of the most useful data science tools is counting things. The function count() gives the number of records by a categorical feature. mtcars %&gt;% count(cyl) ## # A tibble: 3 x 2 ## cyl n ## &lt;dbl&gt; &lt;int&gt; ## 1 4 11 ## 2 6 7 ## 3 8 14 Two categories can be counted at once. This creates a table with all combinations of cut and color and shows the number of records in each category. mtcars %&gt;% count(cyl, vs) ## # A tibble: 5 x 3 ## cyl vs n ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 4 0 1 ## 2 4 1 10 ## 3 6 0 3 ## 4 6 1 4 ## 5 8 0 14 5.0.3 Look at summary statistics The summary() function is best. mtcars %&gt;% summary() ## mpg cyl disp hp ## Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 ## 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 ## Median :19.20 Median :6.000 Median :196.3 Median :123.0 ## Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 ## 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 ## Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 ## drat wt qsec vs ## Min. :2.760 Min. :1.513 Min. :14.50 Min. :0.0000 ## 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 ## Median :3.695 Median :3.325 Median :17.71 Median :0.0000 ## Mean :3.597 Mean :3.217 Mean :17.85 Mean :0.4375 ## 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 ## Max. :4.930 Max. :5.424 Max. :22.90 Max. :1.0000 ## am gear carb ## Min. :0.0000 Min. :3.000 Min. :1.000 ## 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:2.000 ## Median :0.0000 Median :4.000 Median :2.000 ## Mean :0.4062 Mean :3.688 Mean :2.812 ## 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:4.000 ## Max. :1.0000 Max. :5.000 Max. :8.000 5.0.4 Find the dimensions of the data The dim function tell us that there are 32 rows and 11 columns. mtcars %&gt;% dim() ## [1] 32 11 5.0.5 Queries Queries are very similar to SQL. They begin with a “SELECT”, use “GROUP BY” to aggregate, and have a “WHERE” to remove records. Unlike SQL, the ordering of these does not matter. “SELECT” can come after a “WHERE”. dplyr to SQL translation** select() -&gt; SELECT mutate() -&gt; user-defined columns summarize() -&gt; aggregated columns left_join() -&gt; LEFT JOIN filter() -&gt; WHERE group_by() -&gt; GROUP BY filter() -&gt; HAVING arrange() -&gt; ORDER BY mtcars %&gt;% select(mpg, cyl) ## mpg cyl ## Mazda RX4 21.0 6 ## Mazda RX4 Wag 21.0 6 ## Datsun 710 22.8 4 ## Hornet 4 Drive 21.4 6 ## Hornet Sportabout 18.7 8 ## Valiant 18.1 6 ## Duster 360 14.3 8 ## Merc 240D 24.4 4 ## Merc 230 22.8 4 ## Merc 280 19.2 6 ## Merc 280C 17.8 6 ## Merc 450SE 16.4 8 ## Merc 450SL 17.3 8 ## Merc 450SLC 15.2 8 ## Cadillac Fleetwood 10.4 8 ## Lincoln Continental 10.4 8 ## Chrysler Imperial 14.7 8 ## Fiat 128 32.4 4 ## Honda Civic 30.4 4 ## Toyota Corolla 33.9 4 ## Toyota Corona 21.5 4 ## Dodge Challenger 15.5 8 ## AMC Javelin 15.2 8 ## Camaro Z28 13.3 8 ## Pontiac Firebird 19.2 8 ## Fiat X1-9 27.3 4 ## Porsche 914-2 26.0 4 ## Lotus Europa 30.4 4 ## Ford Pantera L 15.8 8 ## Ferrari Dino 19.7 6 ## Maserati Bora 15.0 8 ## Volvo 142E 21.4 4 iris %&gt;% select(Sepal.Length, Species) %&gt;% group_by(Species) %&gt;% summarise(TotalLength = sum(Sepal.Length)) ## # A tibble: 3 x 2 ## Species TotalLength ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 250. ## 2 versicolor 297. ## 3 virginica 329. Just like in SQL, many different aggregate functions can be used such as “SUM”, “MEAN”, “MIN”, “MAX”, and so forth. iris %&gt;% select(Sepal.Length, Species) %&gt;% group_by(Species) %&gt;% summarise(TotalLength = sum(Sepal.Length), MaxLength = max(Sepal.Length), MeanLength = mean(Sepal.Length)) ## # A tibble: 3 x 4 ## Species TotalLength MaxLength MeanLength ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 250. 5.8 5.01 ## 2 versicolor 297. 7 5.94 ## 3 virginica 329. 7.9 6.59 5.1 Exercises Get the DWSimpson actuarial salary data and run queries on it. How many FSAs are represented? What is the average salary for a Life/Health Actuarial Analyst with 3-5 exams passed? What is the average difference in salary between an ASA and an ACAS? Create a new column, called social_life, which is equal to n_exams/years_experience. What is the median social_life by industry? Create a new column using case_when which is equal to 0 when n_exams is less than 3 and n_exams^2 otherwise. "],
["visualization.html", "Chapter 6 Visualization 6.1 Step 1. Put the data in a pivotable format 6.2 Step 2. Create a plot object (ggplot) 6.3 Step 3: Add a plot", " Chapter 6 Visualization The creator’s of the exam do not expect candidates to be expert data scientists. Being able to create basic one-dimensional graphs and to interpret results is most important. Creating graphs in R is easy. The most popular way to do this is with the ggplot library. Three-Steps: 6.1 Step 1. Put the data in a pivotable format Excel users will know this as “pivot-table format”, or the way that a table is organized so it can be put into a pivot table. This is also known as “tidy format”. There is one-row per record and one column per variable. 6.1.1 Example of “wide” or “matrix” format The data below contains counts on a survey which asked people about their religion and annual income. religion is stored in the rows income is spread across the columns This is difficult to work with because there are a lot of columns. ## # A tibble: 6 x 11 ## religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Agnostic 27 34 60 81 76 137 ## 2 Atheist 12 27 37 52 35 70 ## 3 Buddhist 27 21 30 34 33 58 ## 4 Catholic 418 617 732 670 638 1116 ## 5 Don’t k~ 15 14 15 11 10 35 ## 6 Evangel~ 575 869 1064 982 881 1486 ## # ... with 4 more variables: `$75-100k` &lt;dbl&gt;, `$100-150k` &lt;dbl&gt;, ## # `&gt;150k` &lt;dbl&gt;, `Don&#39;t know/refused` &lt;dbl&gt; 6.1.2 Example of “pivotable”, “long”, or “tidy” format Here is the same data only in a long format. You don’t need to know how to switch between the two for now, but only that the long format is what is needed to create graphs. 6.2 Step 2. Create a plot object (ggplot) The first step is to create a blank canvas that holds the columns that are needed. Let’s say that the goal is to graph income and count. We put these into a ggplot object called p. The aesthetic argument, aes, means that the x-axis will have income and the y-axis will have count. p &lt;- data %&gt;% ggplot(aes(x = income, y = count)) If we look at p, we see that it is nothing but white space with axis for count and income. p 6.3 Step 3: Add a plot We add an xy plot. p + geom_point() We can also create a bar plot. p + geom_bar(stat = &quot;identity&quot;) Creating histograms is even easier. Just specify the column that you want to graph as the x column. No y is needed because a histogram is one-dimensional. Take a x to be a random variable from a gamma distribution. gamma = tibble(x = rgamma(1000, shape = 1, rate = 2)) p &lt;- gamma %&gt;% ggplot(aes(x = x)) p + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can graph a density instead of a histogram by using geom_density instead of geom_hist. p + geom_density(fill = &quot;grey&quot;) "],
["modeling.html", "Chapter 7 Modeling 7.1 What is machine learning 7.2 Statistical Definitions 7.3 Linear Models 7.4 Tree-based models", " Chapter 7 Modeling 7.1 What is machine learning All of use are already familiar with how to learn - by learning from our mistakes. By repeating what is successful and avoiding what results in failure, we “learn” by doing, by experience, or trial-and-error. Some methods work well for learning, but other methods do not. We all know that memorizing answers without understanding concepts is an ineffective method, and that doing many practice problems is better than doing only a few. These ideas apply to how computers learn as much as they do to how humans learn. Take the example of preparing for an actuarial exam. We can clearly state our objective: get as many correct answers as possible! Imagine a table with one row per question. Question A = 2; B = 3; A + B = ? A = 2; B = 10; A*B = ? What is the meaning of life? We want to correctly predict the solution to every problem. \\[Y = \\text{Problem Solution}, \\hat{Y} = \\text{Your Answer}\\] Question Solution YourAnswer A = 2; B = 3; A + B = ? 5 6 A = 2; B = 10; A*B = ? 10 10 What is the meaning of life? 42 69 Said another way, we are trying to minimize the error, the percentage of incorrect problems. When \\(Y = \\hat{Y}\\), we answer the question correctly. When \\(Y \\neq \\hat{Y}\\), we answer incorrectly. \\[\\text{Exam Score} = \\sum{(Y = \\hat{Y})}\\] df %&gt;% mutate(Correct = (YourAnswer == Solution)*1) %&gt;% kableExtra::kable(&quot;html&quot;) Question Solution YourAnswer Correct A = 2; B = 3; A + B = ? 5 6 0 A = 2; B = 10; A*B = ? 10 10 1 What is the meaning of life? 42 69 0 The “data” are the exam questions. We want to learn the patterns from the data to create a “model” for answering new questions. \\[X = \\text{Exam Questions}\\] The “training data” are the practice problems. The SOA suggests 100 hours per hour of exam, which means that actuaries prepare hundreds of problems before the real exam. Often we see the same problems multiple times, and can then peak by looking at the solution. This can result in us “overfitting” our model if we are not careful. The more practice problems that we do, the larger the training data set, and the better the model becomes. As we see more examples, our mental “model” improves. When we see new problems, ones which have not appeared in the practice exams, we often have a difficult time. Problems which we have seen before are easier, and we have more confidence in our answers. To speed up the training time, we can “downsample” the data by skipping problems randomly. For example, we can only do odd-numbered problems. This insured that we still get the same proportion of each type of question while doing fewer problems. One way to be extra-confident in our answers to to do the problem multiple ways. In modeling, using the averages of different models produces the best results. If we use the wrong loss function our model will fail. If instead of looking at “Correct Answers” when practicing, we do not bother to look at the answer but instead only look at “Number of Study Hours”, the model will not perform well on the real exam. The brain will “learn” to maximize study time, by taking longer time to relax and waste time, rather than aiming to get the correct answer. 7.2 Statistical Definitions The input columns are in a matrix format called \\(X\\). The target variable is a vector \\(y\\). By convenction, \\(n\\) is the number of rows (aka observations) of \\(X\\) and \\(p\\) is the number of columns (aka variables). Take an example with \\(p\\) = 2 columns and 3 observations. The matrix \\(X\\) looks like x1 x2 x11 x21 x12 x22 x13 x23 In matrix notation, you can say that \\(X\\) is a matrix that has columns \\(x_1 = (x_{11}, x_{12}, x_{13}), x_3 = (x_{21}, x_{22}, x_{23})\\) \\[X = [x1,x2,x3] \\] The target is \\(y = (y_1, y_2, y_3)\\). Machine learning is about using \\(X\\) to predict \\(y\\). We call this “y-hat”, or simply the prediction. \\[\\hat{y} = f(X)\\] This is almost never going to happen perfectly, and so there is always an error term, \\(\\epsilon\\). This can be made smaller, but is never exactly zero. \\[\\hat{y} + \\epsilon = f(X) + \\epsilon\\] In other words, \\(\\epsilon = y - \\hat{y}\\). ** Quiz:** The accuracy of a model is a metric used in classification problems (where the outcome is descrete). In the above example, what is the accuracy of the data set? #ncorrect / total = 1/3 = 33% The goal is choose a model \\(f(.)\\) which approximates \\(y\\). The simplest way of doing this is with a linear model. Choose four numbers \\(\\beta = (\\beta_0, \\beta_1, \\beta_2\\) and say that \\(\\hat{y}\\) is a linear combination of \\(x_1, x_2\\) using \\(\\beta\\) as the weights. \\[f(X) = X\\beta = (\\beta_0 + \\beta_1 x_{11} + \\beta_2 x_22, \\beta_0 + \\beta_2 x_{21} + \\beta_3 x_32, \\beta_0 + \\beta_1 x_{31} + \\beta_3 x_32)\\] – bias variance decomposition – flexibility vs interpretability 7.3 Linear Models – glms - assumptions - penalization (ridge vs. lasso) – training/testing data sets – cross validation – glm examples using - ungrouped data w/gaussian log link - ungrouped data w/gamma log link - ungrouped data with binary /log link - grouped data w/gausian log link… - ungrouped data w/penalization 7.4 Tree-based models Niemerg’s “Forest from the trees” + 100 page ML book "],
["a-mini-exam-example.html", "Chapter 8 A Mini-Exam Example 8.1 Project Statement", " Chapter 8 A Mini-Exam Example The easiest way to understand this exam is to look at an example. 8.1 Project Statement ABC Health Insurance company is building a model to predict medical claims. Using only age and sex information from the prior year, build a model to predict claims for the next year. 8.1.1 #1 - Describe the data (1 point) library(tidyverse) data &lt;- read_csv(&quot;C:/Users/sam.castillo/Desktop/R Manual Data/health_insurance.csv&quot;) %&gt;% select(age, sex, charges) %&gt;% #put this into an r library rename(claims = charges) ## Parsed with column specification: ## cols( ## age = col_double(), ## sex = col_character(), ## bmi = col_double(), ## children = col_double(), ## smoker = col_character(), ## region = col_character(), ## charges = col_double() ## ) data %&gt;% summary() ## age sex claims ## Min. :18.00 Length:1338 Min. : 1122 ## 1st Qu.:27.00 Class :character 1st Qu.: 4740 ## Median :39.00 Mode :character Median : 9382 ## Mean :39.21 Mean :13270 ## 3rd Qu.:51.00 3rd Qu.:16640 ## Max. :64.00 Max. :63770 data %&gt;% dim() ## [1] 1338 3 The data consists of 1,338 policies with age and sex information. The objective is to predict future claims. 8.1.2 #2 - Create a histogram of the claims and comment on the shape (1 point) The distribution of claims is strictly positive and right skewed. data %&gt;% ggplot(aes(claims)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 8.1.3 #3 - Fit a linear model (1 point) model = lm(claims ~ age + sex, data = data) 8.1.4 #4 - Describe the relationship between age, sex, and claim costs (1 point) We fit a linear model to the claim costs using age and sex as predictor variables. The coefficient of age is 258, indicating that the claim costs increase by $258 for every one-unit increase in the policyholder’s age. The cofficient of 1538 on sexmale indicates that on average, men have $1538 higher claims than women do. coefficients(model) ## (Intercept) age sexmale ## 2343.6249 258.8651 1538.8314 8.1.5 #5 - Write a summary of steps 1-4 in non-technical language (1 point) ABC Health is interested in predicting the future claims for a group of policyholders. We began by collecting data on 1,538 policy holders which recorded their age, sex, and annual claims. We then created a histogram of the claim costs. A linear model which shows that claim costs increase as age increases, and are higher for men on average. "],
["references.html", "References", " References "],
["references-1.html", "References", " References "],
["practice-exam.html", "Chapter 9 Practice Exam", " Chapter 9 Practice Exam This is a practice exam that is based on the December 2018 PA exam. This is easier than the actual exam. "],
["prior-exams.html", "Chapter 10 Prior Exams", " Chapter 10 Prior Exams There are the exams and solutions published by the SOA. "],
["references-2.html", "References", " References "]
]
