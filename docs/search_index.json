[
["introduction-to-modeling.html", " 4 Introduction to modeling 4.1 Modeling vocabulary 4.2 Modeling notation 4.3 Ordinary Least Squares (OLS) 4.4 R^2 Statistic 4.5 Correlation 4.6 Regression vs. classification 4.7 Regression metrics 4.8 Example: Health Costs", " 4 Introduction to modeling About 40-50% of the exam grade is based on modeling. The goal is to be able to predict an unknown quantity. In actuarial applications, this tends to be claims that occur in the future, death, injury, accidents, policy lapse, hurricanes, or some other insurable event. The next few chapters will cover the following learning objectives. 4.1 Modeling vocabulary Modeling notation is sloppy because there are many words that mean the same thing. The number of observations will be denoted by \\(n\\). When we refer to the size of a data set, we are referring to \\(n\\). Each row of the data is called an observation or record. Observations tend to be people, cars, buildings, or other insurable things. These are always independent in that they do not influence one another. Because the Prometric computers have limited power, \\(n\\) tends to be less than 100,000. Each observation has known attributes called variables, features, or predictors. We use \\(p\\) to refer the number of input variables that are used in the model. The target, response, label, dependent variable, or outcome variable is the unknown quantity that is being predicted. We use \\(Y\\) for this. This can be either a whole number, in which case we are performing regression, or a category, in which case we are performing classification. For example, say that you are a health insurance company that wants to set the premiums for a group of people. The premiums for people who are likely to incur high health costs need to be higher than those who are likely to be low-cost. Older people tend to use more of their health benefits than younger people, but there are always exceptions for those who are very physically active and healthy. Those who have an unhealthy Body Mass Index (BMI) tend to have higher costs than those who have a healthy BMI, but this has less of an impact on younger people. In short, we want to be able to predict a person’s future health costs by taking into account many of their attributes at once. This can be done in the health_insurance data by fitting a model to predict the annual health costs of a person. The target variable is y = charges, and the predictor variables are age, sex, bmi, children, smoker and region. These six variables mean that \\(p = 6\\). The data is collected from 1,338 patients, which means that \\(n = 1,338\\). 4.2 Modeling notation Scalar numbers are denoted by ordinary variables (i.e., \\(x = 2\\), \\(z = 4\\)), and vectors are denoted by bold-faced letters \\[\\mathbf{a} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{pmatrix}\\] We organize these variables into matrices. Take an example with \\(p\\) = 2 columns and 3 observations. The matrix is said to be \\(3 \\times 2\\) (read as “3-by-2”) matrix. \\[ \\mathbf{X} = \\begin{pmatrix}x_{11} &amp; x_{21}\\\\ x_{21} &amp; x_{22}\\\\ x_{31} &amp; x_{32} \\end{pmatrix} \\] In the health care costs example, \\(y_1\\) would be the costs of the first patient, \\(y_2\\) the costs of the second patient, and so forth. The variables \\(x_{11}\\) and \\(x_{12}\\) might represent the first patient’s age and sex respectively, where \\(x_{i1}\\) is the patient’s age, and \\(x_{i2} = 1\\) if the ith patient is male and 0 if female. Modeling is about using \\(X\\) to predict \\(Y\\). We call this “y-hat”, or simply the prediction. This is based on a function of the data \\(X\\). \\[\\hat{Y} = f(X)\\] This is almost never going to happen perfectly, and so there is always an error term, \\(\\epsilon\\). This can be made smaller, but is never exactly zero. \\[ \\hat{Y} + \\epsilon = f(X) + \\epsilon \\] In other words, \\(\\epsilon = y - \\hat{y}\\). We call this the residual. When we predict a person’s health care costs, this is the difference between the predicted costs (which we had created the year before) and the actual costs that the patient experienced (of that current year). Another way of saying this is in terms of expected value: the model \\(f(X)\\) estimates the expected value of the target \\(E[Y|X]\\). That is, once we condition on the data \\(X\\), we can make a guess as to what we expect \\(Y\\) to be “close to”. There are many ways of measuring “closeness”, as we will see. 4.3 Ordinary Least Squares (OLS) Also known as simple linear regression, OLS predicts the target as a weighted sum of the variables. We find a \\(\\mathbf{\\beta}\\) so that \\[ \\hat{Y} = E[Y] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p \\] Each \\(y_i\\) is a linear combination of \\(x_{i1}, ..., x_{ip}\\), plus a constant \\(\\beta_0\\) which is called the intercept term. In the one-dimensional case, this creates a line connecting the points. In higher dimensions, this creates a hyper-plane. ## Warning: package &#39;purrr&#39; was built under R version 3.6.3 The red line shows the expected value of the target, as the target \\(\\hat{Y}\\) is actually a random variable. For each of the data points, the model assumes a Gaussian distribution. If there is just a single predictor, \\(x\\), then the mean is \\(\\beta_0 + \\beta_1 x\\). The question then is how can we choose the best values of \\(\\beta?\\) First of all, we need to define what we mean by “best”. Ideally, we will choose these values which will create close predictions of \\(Y\\) on new, unseen data. To solve for \\(\\mathbf{\\beta}\\), we first need to define a loss function. This allows us to compare how well a model is fitting the data. The most commonly used loss function is the residual sum of squares (RSS), also called the squared error loss or the L2 norm. When RSS is small, then the predictions are close to the actual values and the model is a good fit. When RSS is large, the model is a poor fit. \\[\\text{RSS} = \\sum_i(y_i - \\hat{y})^2\\] When you replace \\(\\hat{y_i}\\) in the above equation with \\(\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p\\), take the derivative with respect to \\(\\beta\\), set equal to zero, and solve, we can find the optimal values. This turns the problem of statistics into a problem of numeric optimization, which computers can do quickly. You will also see the term Root Mean Squared Error (RMSE) which is just the average of the square root of the \\(\\text{RSS}\\), or just Mean Squared Error (MSE). You might be asking: why does this need to be the squared error? Why not the absolute error, or the cubed error? Technically, these could be used as well but the betas would not be the maximum likelihood parameters. In fact, using the absolute error results in the model predicting the median as opposed to the mean. Two reasons why RSS is popular are: It provides the same solution if we assume that the distribution of \\(Y|X\\) is Gaussian and maximize the likelihood function. This method is used for GLMs, in the next chapter. It is computationally easier, and computers used to have a difficult time optimizing for MAE What does it mean when a log transform is applied to \\(Y\\)? I remember from my statistics course on regression that this was done. This is done so that the variance is closer to being constant. For example, if the units are in dollars, then it is very common for the values to fluctuate more for higher values than for lower values. Consider a stock price, for instance. If the stock is $50 per share, then it will go up or down less than if it is $1000 per share. The log of 50, however, is about 3.9 and the log of 1000 is only 6.9, and so this difference is smaller. In other words, the variance is smaller. Transforming the target means that instead of the model predicting \\(E[Y]\\), it predicts \\(E[log(Y)]\\). A common mistake is to then the take the exponent in an attempt to “undo” this transform, but \\(e^{E[log(Y)]}\\) is not the same as \\(E[Y]\\). 4.4 R^2 Statistic One of the most common ways of measuring model fit, which you may be familiar with from a course on linear regression, is the “R-Squared” statistic. The RSS provides an absolute measure of fit, because the number can be any positive value, but it’s not always clear what a “good” RSS is because it’s measured in units of \\(Y\\). The \\(R^2\\) statistic provides an alternative measure of fit. It takes the proportion of variance explained - so that it’s always a value between 0 and 1, and is independent of the scale of \\(Y\\). \\[R^2 = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}\\] Where \\(\\text{TSS} = \\sum(y_i - \\hat{y})^2\\) is the total sum of squares. TSS measures the total variance in the response \\(Y\\) and can be thought of as the amount of variability inherent in the response before the regression is performed. In contrast, RSS measures the amount of variability that is left unexplained after performing the regression. Hench, \\(\\text{TSS} - \\text{RSS}\\) measures the amount of variability in the response that is explained (or removed) be performing the regression, and R^2 measures the proportion of variability in \\(Y\\) that can be explained using \\(X\\). A value near 1 indicates that a large proportion of the variability in the response has been explained by the regression. A number near 0 indicates that the regression did not explain much of the variability in the response; this might occur because the linear model is wrong. The \\(R^2\\) statistic has an interpretational advantage over the RSE. In actuarial applications, it is useful to use an absolute measure of model fit, such as RSS, to train the model, and then use \\(R^2\\) when you are explaining it to your clients so that it is easier to communicate. This chapter was based on Chapter 3, Linear Regression, of An Introduction to Statistical Learning. 4.5 Correlation Correlation does not imply causation. This is a common saying. Just because two things are correlated does not necessarily mean that one casues the other. Just because most actuaries work remotely when there it is cold and snowing does not mean that cold and snow cause anti-social, introverted work habits. A more likely explanation is that actuaries are concerned about driving safely on icy roads and avoiding being involved in a car accident. 4.5.1 Pearson’s correlation Pearson correlation: Measures a linear dependence between two variables \\(X\\) and \\(Y\\). This is the most commonly used correlation method. The correlation is defined by \\(r\\), \\[r = Cor(X,Y) = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2}\\sqrt{(y_i - \\bar{y})^2}}\\] and this is also a measure of the linear relationship between two vectors, \\(X\\) and \\(Y\\). This suggests that we might be able to use \\(r = Cor(X,Y)\\) instead of \\(R^2\\) to assess the model fit. In the case of simple linear regression, where there is only one predictor variable, it is tree that \\(R^2 = r^2\\); however, this relationship does not extend automatically when there are more than one predictor variable. This is because \\(X\\) becomes a matrix instead of a single vector. 4.5.2 Spearman (rank) correlation Spearman correlation: Computes the correlation between the rank of x and the rank of y variables. \\[rho = \\frac{\\sum(x&#39; - m_{x&#39;})(y&#39;_i - m_{y&#39;})}{\\sqrt{\\sum(x&#39; - m_{x&#39;})^2 \\sum(y&#39; - m_{y&#39;})^2}}\\] Where \\(x′=rank(x)\\) and \\(y′=rank(y)\\) Most questions on Exam PA will ask you about Pearson’s correlation. One advantage to Spearman over Pearson is that Spearman works for ordinal variables. See Chapter 6 for the difference between ordinal and numeric variables. 4.6 Regression vs. classification Regression modeling is when the target is a number. Binary classification is when there are two outcomes, such as “Yes/No”, “True/False”, or “0/1”. Multi-class regression is when there are more than two categories such as “Red, Yellow, Green” or “A, B, C, D, E”. There are many other types of regression that are not covered on this exam such as ordinal regression, where the outcome is an ordered category, or time-series regression, where the data is time-dependent. 4.7 Regression metrics For any model, the goal is always to reduce an error metric. This is a way of measuring how well the model can explain the target. The phrases “reducing error”, “improving performance”, or “making a better fit” are synonymous with reducing the error. The word “better” means “lower error” and “worse” means “higher error”. The choice of error metric has a big difference on the outcome. When explaining a model to a businessperson, using simpler metrics such as R-Squared and Accuracy is convenient. When training the model, however, using a more nuanced metric is almost always better. These are the regression metrics that are most likely to appear on Exam PA. Memorizing these formulas for AIC and BIC is not necessary as they are in the R documentation by typing ?AIC or ?BIC into the R console. Don’t forget the most important metric: “usefullness!”. A model which has high predictive accuracy but which does not meet the needs of the business problem has low usefulness. A model which is easy to explain to the PA exam graders has high usefulness. “Some candidates did not consider both predictive power and applicability to the business problem, and others gave justifications based on one of these but then chose a model based on the other.” - SOA PA 6/18/20, Task 12 Solution “Which of the three models would you recommend for this analysis? Do not base your recommendation solely on the mean squared errors (RMSE) from each model.” - SOA PA 6/13/19, Task 9 Project Statement 4.7.1 Example: SOA PA 6/18/20, Task 4 (3 points) Investigate correlations. Create a correlation coefficient matrix for all the numeric variables in the dataset. Among these pairwise correlations, determine which correlations concern you in building GLM and tree models. The response may differ by model. State a method other than principal components analysis (PCA) that can be used to handle the correlated variables. Do not implement this method. I compare the correlations of the linear variables below. Two variables are correlated when there is a tendency of one variable to increase as the other variable increases and to decrease when that variable decreases. I see that edu_years and age have a negative correlation Interest rate and CPI have a very high correlation of 0.59 Employment and CPI also have a high correlation of 0.98 One of the assumptions of GLMs is that the predictor variables are not correlated. These correlations between irate and employment may cause problems when fitting the model. Decision trees, on the other hand, handle correlations well and would not be impacted. GLMs perform poorly because there are separate coefficients for each of the correlated variables which have an offsetting impact. This results in an unstable model which can have its coefficients change drastically depending on the training split. The p-values for the correlated variables would likely be large. Trees are not impacted strongly by this because only one of the variables will be selected at each split. In other words, the feature selection attribute of trees will only choose one of the correlated variables at a time. One method other than PCA for handling the correlated variables is to use one of the variables and delete the redundant ones. (This is the method in the SOA’s solution.) Use stepwise selection such as Step AIC or Step BIC to remove a subset of the correlated variables automatically. That will work because once one variable is included in the model, adding another correlated variable would not lead to improvement in the AIC or BIC and so this model variation would not be used. Use a LASSO to remove variables. Similarly to stepwise selection, this removes variables which do not decrease the test error (root mean squared error) based on the cross-validation results. Use clustering to group together customers based on the correlated variables and then use this cluster number as a new feature. Customers who had similar cluster characteristics would be grouped together. For instance, customers who were called during an economic downturn when interest rates, employment, and CPI were low may be in one cluster and customers who were called during a booming economy when CCI was high and prices were high may be in another. The cluster number would be a new categorical feature in the model. Note: only one answer is needed. Four are provided here for your reading benefit. 4.8 Example: Health Costs In our health insurance data, we can predict a person’s health costs based on their age, body mass index, and gender. Intuitively, we expect that these costs would increase as a person’s age increases, would be different for men than for women, and would be higher for those who have a less healthy BMI. We create a linear model using bmi, age, and sex as an inputs. The formula controls which variables are included. There are a few shortcuts for using R formulas. Formula Meaning charges ~ bmi + age Use age and bmi to predict charges charges ~ bmi + age + bmi*age Use age,bmi as well as an interaction to predict charges charges ~ (bmi &gt; 20) + age Use an indicator variable for bmi &gt; 20 age to predict charges log(charges) ~ log(bmi) + log(age) Use the logs of age and bmi to predict log(charges) charges ~ . Use all variables to predict charges While you can use formulas to create new variables, the exam questions tend to have you do this in the data itself. For example, if taking the log transform of a bmi, you would add a column log_bmi to the data and remove the original bmi column. Below we fit a simple linear model to predict charges. library(ExamPAData) library(tidyverse) model &lt;- lm(data = health_insurance, formula = charges ~ bmi + age + sex) The summary function gives details about the model. First, the Estimate, gives you the coefficients. The Std. Error is the error of the estimate for the coefficient. Higher standard error means greater uncertainty. This is relative to the average value of that variable. The p value tells you how “big” this error really is based on standard deviations. A small p-value (Pr (&gt;|t|))) means that we can safely reject the null hypothesis that says the coefficient is equal to zero. The little *, **, *** tell you the significance level. A variable with a *** means that the probability of getting a coefficient of that size given that the data was randomly generated is less than 0.001. The ** has a significance level of 0.01, and * of 0.05. summary(model) ## ## Call: ## lm(formula = charges ~ bmi + age + sex, data = health_insurance) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14974 -7073 -5072 6953 47348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6986.82 1761.04 -3.967 7.65e-05 *** ## bmi 327.54 51.37 6.377 2.49e-10 *** ## age 243.19 22.28 10.917 &lt; 2e-16 *** ## sexmale 1344.46 622.66 2.159 0.031 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11370 on 1334 degrees of freedom ## Multiple R-squared: 0.1203, Adjusted R-squared: 0.1183 ## F-statistic: 60.78 on 3 and 1334 DF, p-value: &lt; 2.2e-16 For this exam, variable selection tends to be based on the 0.05 significance level (single star *). When evaluating model performance, you should not rely on the summary alone as this is based on the training data. To look at performance, test the model on validation data. This can be done by either using a hold out set, or using cross-validation, which is even better. Let’s create an 80% training set and 20% testing set. You don’t need to worry about understanding this code as the exam will always give this to you. set.seed(1) library(caret) #create a train/test split index &lt;- createDataPartition(y = health_insurance$charges, p = 0.8, list = F) %&gt;% as.numeric() train &lt;- health_insurance %&gt;% slice(index) test &lt;- health_insurance %&gt;% slice(-index) Train the model on the train and test on test. model &lt;- lm(data = train, formula = charges ~ bmi + age) pred = predict(model, test) Let’s look at the Root Mean Squared Error (RMSE). get_rmse &lt;- function(y, y_hat){ sqrt(mean((y - y_hat)^2)) } get_rmse(pred, test$charges) ## [1] 11421.96 And the Mean Absolute Error as well. get_mae &lt;- function(y, y_hat){ sqrt(mean(abs(y - y_hat))) } get_mae(pred, test$charges) ## [1] 94.32336 The above metrics do not tell us if this is a good model or not by themselves. We need a comparison. The fastest check is to compare against a prediction of the mean. In other words, all values of the y_hat are the average of charges, which is about $13,000. get_rmse(mean(test$charges), test$charges) ## [1] 12574.97 get_mae(mean(test$charges), test$charges) ## [1] 96.63604 The RMSE and MAE are both higher (worse) when using just the mean, which is what we expect. If you ever fit a model and get an error which is worse than the average prediction, something must be wrong. The next test is to see if any assumptions have been violated. First, is there a pattern in the residuals? If there is, this means that the model is missing key information. For the model below, this is a yes, which means that this is a bad model. Because this is just for illustration, we are going to continue using it. plot(model, which = 1) Figure 4.1: Residuals vs. Fitted The normal QQ shows how well the quantiles of the predictions fit to a theoretical normal distribution. If this is true, then the graph is a straight 45-degree line. In this model, you can definitely see that this is not the case. If this were a good model, this distribution would be closer to normal. plot(model, which = 2) Figure 4.2: Normal Q-Q Once you have chosen your model, you should re-train over the entire data set. This is to make the coefficients more stable because n is larger. Below you can see that the standard error is lower after training over the entire data set. all_data &lt;- lm(data = health_insurance, formula = charges ~ bmi + age) testing &lt;- lm(data = test, formula = charges ~ bmi + age) term full_data_std_error test_data_std_error (Intercept) 1744.1 3824.2 bmi 51.4 111.1 age 22.3 47.8 All interpretations should be based on the model which was trained on the entire data set. Obviously, this only makes a difference if you are interpreting the precise values of the coefficients. If you are just looking at which variables are included, or at the size and sign of the coefficients, then this would probably not make a difference. coefficients(model) ## (Intercept) bmi age ## -4526.5284 286.8283 228.4372 Translating the above into an equation we have \\[\\hat{y_i} = -4,526 + 287 \\space\\text{bmi} + 228\\space \\text{age}\\] For example, if a patient has bmi = 27.9 and age = 19 then predicted value is \\[\\hat{y_1} = 4,526 + (287)(27.9) + (228)(19) = 16,865\\] This model structure implies that each of the variables \\(x_1, ..., x_p\\) each change the predicted \\(\\hat{y}\\). If \\(x_{ij}\\) increases by one unit, then \\(y_i\\) increases by \\(\\beta_j\\) units, regardless of what happens to all of the other variables. This is one of the main assumptions of linear models: variable independence. If the variables are correlated, say, then this assumption will be violated. Readings ISLR 2.1 What is statistical learning? ISLR 2.2 Assessing model accuracy "],
["generalized-linear-models-glms.html", " 5 Generalized linear Models (GLMs) 5.1 Advantages and disadvantages 5.2 GLMs for regression 5.3 Interpretation of coefficients 5.4 Other links", " 5 Generalized linear Models (GLMs) GLMs are a broad category of models. Ordinary Least Squares and Logistic Regression are both examples of GLMs. 5.0.1 Assumptions of OLS We assume that the target is Gaussian with mean equal to the linear predictor. This can be broken down into two parts: A random component: The target variable \\(Y|X\\) is normally distributed with mean \\(\\mu = \\mu(X) = E(Y|X)\\) A link between the target and the covariates (also known as the systemic component) \\(\\mu(X) = X\\beta\\) This says that each observation follows a normal distribution which has a mean that is equal to the linear predictor. Another way of saying this is that “after we adjust for the data, the error is normally distributed and the variance is constant.” If \\(I\\) is an n-by-in identity matrix, and \\(\\sigma^2 I\\) is the covariance matrix, then \\[ \\mathbf{Y|X} \\sim N( \\mathbf{X \\beta}, \\mathbf{\\sigma^2} I) \\] 5.0.2 Assumptions of GLMs Just as the name implies, GLMs are more general in that they are more flexible. We relax these two assumptions by saying that the model is defined by A random component: \\(Y|X \\sim \\text{some exponential family distribution}\\) A link: between the random component and covariates: \\[g(\\mu(X)) = X\\beta\\] where \\(g\\) is called the link function and \\(\\mu = E[Y|X]\\). Each observation follows some type of exponential distribution (Gamma, Inverse Gaussian, Poisson, Binomial, etc.) and that distribution has a mean which is related to the linear predictor through the link function. Additionally, there is a dispersion parameter, but that is more info that is needed here. For an explanation, see Ch. 2.2 of CAS Monograph 5. 5.1 Advantages and disadvantages There is usually at least one question on the PA exam which asks you to “list some of the advantages and disadvantages of using this particular model”, and so here is one such list. It is unlikely that the grader will take off points for including too many comments and so a good strategy is to include everything that comes to mind. GLM Advantages Easy to interpret Can easily be deployed in spreadsheet format Handles different response/target distributions Is commonly used in insurance ratemaking GLM Disadvantages Does not select features (without stepwise selection) Strict assumptions around distribution shape and randomness of error terms Predictor variables need to be uncorrelated Unable to detect non-linearity directly (although this can manually be addressed through feature engineering) Sensitive to outliers Low predictive power 5.2 GLMs for regression For regression problems, we try to match the actual distribution to the model’s distribution being used in the GLM. These are the most likely distributions. The choice of target distribution should be similar to the actual distribution of \\(Y\\). For instance, if \\(Y\\) is never less than zero, then using the Gaussian distribution is not ideal because this can allow for negative values. If the distribution is right-skewed, then the Gamma or Inverse Gaussian may be appropriate because they are also right-skewed. Notice that the top three distributions are continuous but the bottom two are discrete. There are five link functions for a continuous \\(Y\\), although the choice of distribution family will typically rule-out several of these immediately. The linear predictor (a.k.a., the systemic component) is \\(z\\) and the link function is how this connects to the expected value of the response. \\[z = X\\beta = g(\\mu)\\] If the target distribution must have a positive mean, such as in the case of the Inverse Gaussian or Gamma, then the Identity or Inverse links are poor choices because they allow for negative values; the range of the mean is \\((-\\infty, \\infty)\\). The other link functions force the mean to be positive. 5.3 Interpretation of coefficients The GLM’s interpretation depends on the choice of link function. 5.3.1 Identity link This is the easiest to interpret. For each one-unit increase in \\(X_j\\), the expected value of the target, \\(E[Y]\\), increases by \\(\\beta_j\\), assuming that all other variables are held constant. 5.3.2 Log link This is the most popular choice when the results need to be easy to understand. Simply take the exponent of the coefficients and the model turns into a product of numbers being multiplied together. \\[ log(\\hat{Y}) = X\\beta \\Rightarrow \\hat{Y} = e^{X \\beta} \\] For a single observation \\(Y_i\\), this is \\[ \\text{exp}(\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + ... + \\beta_p X_{ip}) = \\\\ e^{\\beta_0} e^{\\beta_1 X_{i1}}e^{\\beta_2 X_{i2}} ... e^{\\beta_p X_{ip}} = R_{i0} R_{i2} R_{i3} ... R_{ip} \\] \\(R_{ik}\\) is known as the relativity of the kth variable. This terminology is from insurance ratemaking where actuaries need to be able to explain the impact of each variable to insurance regulators. Another advantage to the log link is that the coefficients can be interpreted as having a percentage change on the target. Here is an example for a GLM with variables \\(X_1\\) and \\(X_2\\) and a log link function. This holds any continuous target distribution. Variable \\(\\beta_j\\) \\(e^{\\beta_j} - 1\\) Interpretation (intercept) 0.100 0.105 \\(X_1\\) 0.400 0.492 49% increase in \\(E[Y]\\) for each unit increase in \\(X_1\\)* \\(X_2\\) -0.500 -0.393 39% decrease in \\(E[Y]\\) for each unit increase in \\(X_2\\)* If categorical predictors are used, then the interpretation is very similar. Say that there is one predictor, COLOR, which takes on values of YELLO (reference level), RED, and BLUE. Variable \\(\\beta_j\\) \\(e^{\\beta_j} - 1\\) Interpretation (intercept) 0.100 0.105 Color=RED 0.400 0.492 49% increase in \\(E[Y]\\) for RED cars as opposed to YELLOW cars* Color=BLUE -0.500 -0.393 39% decrease in \\(E[Y]\\) for BLUE cars rather than YELLOW cars* * Assuming all other variables are held constant. Warning: Never take the log of Y with a GLM! This is a common mistake because that’s how we handled skewness for multiple linear regression models, but that was before we had the GLM in our toolbox. Don’t move on until you understand the difference between these two models: glm(y ~ x, family = gaussian(link = “log”), data = data) glm(log(y) ~ x, family = gaussian(link = “identity”), data = data) The first says that the target has a Gaussian distribution which has a mean equal to the log of the linear predictor. The second says that the log of the target has a Guassian distribution has a mean exactly equal to the linear predictor. You’ll remember from Exam P that when you apply a transform to a random variable, the distribution changes completely. Try running the above examples on real data and see if you can spot the differences in the results. 5.4 Other links The other link functions are not straight-forward to interpret using math. One solution is to use the model-demo-method. See the example at the end of this next chapter. "],
["glms-for-classification.html", " 6 GLMs for classification 6.1 Binary target 6.2 Count target 6.3 Link functions 6.4 Interpretation of coefficients 6.5 Demo the model for interpretation 6.6 Example - Auto Claims", " 6 GLMs for classification For classification, the predicted values need to be a category instead of a number. Using a discrete target distribution ensures that this will be the case. The probability of an event occurring is \\(E[Y] = p\\). Unlike the continuous case, all of the link functions have the same range between 0 and 1 because this is a probability. These StatQuest videos explain the most common type of GLM classification model: Logistic regression. 6.1 Binary target When \\(Y\\) is binary, then the Binomial distribution is the only choice. If there are multiple categories, then the Multinomial should be used. 6.2 Count target When \\(Y\\) is a count, the Poisson distribution is the only choice. Two examples are counting the number of claims which a policy has in a given year or counting the number of people visiting the ER in a given month. The key ingredients are 1) some event, and 2) some fixed period of time. Statistically, the name for this is a Poisson Process, which is a way of describing a serious of discrete events where the average time between events is known, called the “rate” \\(\\lambda\\), but the exact timing of events is unknown. For a time interval of length \\(m\\), the expected number of events is \\(\\lambda m\\). By using a GLM, we can fit a different rate for each observation. In the ER example, each patient would have a different rate. Those who are unhealthy or who work in risky environments would have a higher rate of ER visits than those are healthy and work in offices. \\[Y_i|X_i \\sim \\text{Poisson}(\\lambda_i m_i)\\] When all observations have the same exposure, \\(m = 1\\). When the mean of the data is far from the variance, an additional parameter known as the dispersion parameter is used. A classic example is when modeling insurance claim counts which have a lot of zero claims. Then the model is said to be an “over-dispersed Poisson” or “zero-inflated” model. 6.3 Link functions There are four link functions. The most common are the Logit and Probit, but the Cauchit and Cloglog did appear on the SOA’s Hospital Readmissions practice exam in 2019. The identity link does not make sense for classification because it would result in predictions being outside of \\((0,1)\\) The logit is also known as the standard logistic function or sigmoid and is also used in deep learning. Below we see how the linear predictor (x-axis) gets converted to a probability (y-axis). Logit: Most commonly used; default in R; canonical link for the binomial distribution. Probit: Sharper curves than the other links which may have best performance for certain data; Inverse CDF of a standard normal distribution makes it easy to explain. Cauchit: Very gradual curves may be best for certain data; CDF for the standard Cauchy distribution which is a t distribution with one degree of freedom. Complimentary Log-Log (cloglog) Asymmetric; Important in survival analysis (not on this exam). 6.4 Interpretation of coefficients Interpreting the coefficients in classification is trickier than in classification because the result must always be within \\((0,1)\\). 6.4.1 Logit The link function \\(log(\\frac{p}{1-p})\\) is known as the log-odds, where the odds are \\(\\frac{p}{1-p}\\). These come up in gambling, where bets are placed on the odds of some event occurring. For example: if the probability of a claim is \\(p = 0.8\\), then the probability of no claim is 0.2 and the odds of a claim occurring are 0.8/0.2 = 4. The transformation from probability to odds is monotonic. This is a fancy way of saying that if \\(p\\) increases, then the odds of \\(p\\) increases as well, and vice versa if \\(p\\) decreases. The log transform is monotonic as well. The net result is that when a variable increases the linear predictor, this increases the log odds, and this increases the log of the odds, and vice versa if the linear predictor decreases. In other words, the signs of the coefficients indicate whether the variable increases or decreases the probability of the event. 6.4.2 Probit, Cauchit, Cloglog These link functions are still monotonic and so the signs of the coefficients can be interpreted to mean that the variable has a positive or negative impact on the target. More extensive interpretation is not straight-forward. In the case of the Probit, instead of dealing with the log-odds function, we have the inverse CDF of a standard Normal distribution (a.k.a., a Gaussian distribution with mean 0 and variance 1). There is no way of taking this inverse directly. 6.5 Demo the model for interpretation For uglier link functions, we can rely on trial-and-error to interpret the result. We’ll call this the “model-demo method”, which as the name implies, involes running example cases through and seeing how the results change. This method works not only for categorical GLMs, but any other type of model such as a continuous GLM, GBM, or random forest. See the example from SOA PA 12/12/19 below to learn how this works. 6.6 Example - Auto Claims Using the auto_claim data, we predict whether or not a policy has a claim. This is also known as the claim frequency. auto_claim %&gt;% count(CLM_FLAG) ## # A tibble: 2 x 2 ## CLM_FLAG n ## &lt;chr&gt; &lt;int&gt; ## 1 No 7556 ## 2 Yes 2740 About 40% do not have a claim while 60% have at least one claim. set.seed(42) index &lt;- createDataPartition(y = auto_claim$CLM_FLAG, p = 0.8, list = F) %&gt;% as.numeric() auto_claim &lt;- auto_claim %&gt;% mutate(target = as.factor(ifelse(CLM_FLAG == &quot;Yes&quot;, 1,0))) train &lt;- auto_claim %&gt;% slice(index) test &lt;- auto_claim %&gt;% slice(-index) frequency &lt;- glm(target ~ AGE + GENDER + MARRIED + CAR_USE + BLUEBOOK + CAR_TYPE + AREA, data=train, family = binomial(link=&quot;logit&quot;)) All of the variables except for the CAR_TYPE and GENDERM are highly significant. The car types SPORTS CAR and SUV appear to be significant, and so if we wanted to make the model simpler we could create indicator variables for CAR_TYPE == SPORTS CAR and CAR_TYPE == SUV. frequency %&gt;% summary() ## ## Call: ## glm(formula = target ~ AGE + GENDER + MARRIED + CAR_USE + BLUEBOOK + ## CAR_TYPE + AREA, family = binomial(link = &quot;logit&quot;), data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8431 -0.8077 -0.5331 0.9575 3.0441 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.523e-01 2.517e-01 -1.400 0.16160 ## AGE -2.289e-02 3.223e-03 -7.102 1.23e-12 *** ## GENDERM -1.124e-02 9.304e-02 -0.121 0.90383 ## MARRIEDYes -6.028e-01 5.445e-02 -11.071 &lt; 2e-16 *** ## CAR_USEPrivate -1.008e+00 6.569e-02 -15.350 &lt; 2e-16 *** ## BLUEBOOK -4.025e-05 4.699e-06 -8.564 &lt; 2e-16 *** ## CAR_TYPEPickup -6.687e-02 1.390e-01 -0.481 0.63048 ## CAR_TYPESedan -3.689e-01 1.383e-01 -2.667 0.00765 ** ## CAR_TYPESports Car 6.159e-01 1.891e-01 3.256 0.00113 ** ## CAR_TYPESUV 2.982e-01 1.772e-01 1.683 0.09240 . ## CAR_TYPEVan -8.983e-03 1.319e-01 -0.068 0.94569 ## AREAUrban 2.128e+00 1.064e-01 19.993 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 9544.3 on 8236 degrees of freedom ## Residual deviance: 8309.6 on 8225 degrees of freedom ## AIC: 8333.6 ## ## Number of Fisher Scoring iterations: 5 The signs of the coefficients tell if the probability of having a claim is either increasing or decreasing by each variable. For example, the likelihood of an accident Decreases as the age of the car increases Is lower for men Is higher for sports cars and SUVs The p-values tell us if the variable is significant. Age, MarriedYes, CAR_USEPrivate, BLUEBOOK, and AreaUrban are significant. Certain values of CAR_TYPE are significant but others are not. The output is a predicted probability. We can see that this is centered around a probability of about 0.3. preds &lt;- predict(frequency, newdat=test,type=&quot;response&quot;) qplot(preds) Figure 6.1: Distribution of Predicted Probability In order to convert these values to predicted 0’s and 1’s, we assign a cutoff value so that if \\(\\hat{y}\\) is above this threshold we use a 1 and 0 otherwise. The default cutoff is 0.5. We change this to 0.3 and see that there are 763 policies predicted to have claims. test &lt;- test %&gt;% mutate(pred_zero_one = as.factor(1*(preds&gt;.3))) summary(test$pred_zero_one) ## 0 1 ## 1296 763 How do we decide on this cutoff value? We need to compare cutoff values based on some evaluation metric. For example, we can use accuracy. \\[\\text{Accuracy} = \\frac{\\text{Correct Guesses}}{\\text{Total Guesses}}\\] This results in an accuracy of 70%. But is this good? test %&gt;% summarise(accuracy = mean(pred_zero_one == target)) ## # A tibble: 1 x 1 ## accuracy ## &lt;dbl&gt; ## 1 0.699 Consider what would happen if we just predicted all 0’s. The accuracy is 74%. test %&gt;% summarise(accuracy = mean(0 == target)) ## # A tibble: 1 x 1 ## accuracy ## &lt;dbl&gt; ## 1 0.734 For policies which experience claims the accuracy is 63%. test %&gt;% filter(target == 1) %&gt;% summarise(accuracy = mean(pred_zero_one == target)) ## # A tibble: 1 x 1 ## accuracy ## &lt;dbl&gt; ## 1 0.631 But for policies that don’t actually experience claims this is 72%. test %&gt;% filter(target == 0) %&gt;% summarise(accuracy = mean(pred_zero_one == target)) ## # A tibble: 1 x 1 ## accuracy ## &lt;dbl&gt; ## 1 0.724 How do we know if this is a good model? We can repeat this process with a different cutoff value and get different accuracy metrics for these groups. Let’s use a cutoff of 0.6. 75% test &lt;- test %&gt;% mutate(pred_zero_one = as.factor(1*(preds&gt;.6))) test %&gt;% summarise(accuracy = mean(pred_zero_one == target)) ## # A tibble: 1 x 1 ## accuracy ## &lt;dbl&gt; ## 1 0.752 10% for policies with claims and 98% for policies without claims. test %&gt;% filter(target == 1) %&gt;% summarise(accuracy = mean(pred_zero_one == target)) ## # A tibble: 1 x 1 ## accuracy ## &lt;dbl&gt; ## 1 0.108 test %&gt;% filter(target == 0) %&gt;% summarise(accuracy = mean(pred_zero_one == target)) ## # A tibble: 1 x 1 ## accuracy ## &lt;dbl&gt; ## 1 0.985 The punchline is that the accuracy depends on the cutoff value, and changing the cutoff value changes whether the model is accuracy for the “true = 1” classes (policies with actual claims) vs. the “false = 0” classes (policies without claims). "],
["classification-metrics.html", " 7 Classification metrics 7.1 Area Under the ROC Curve (AUC) 7.2 Example - Auto Claims 7.3 Example: SOA HR, Task 5 7.4 Example: SOA PA 12/12/19, Task 11 7.5 Additional reading", " 7 Classification metrics For regression problems, when the output is a whole number, we can use the sum of squares \\(\\text{RSS}\\), the r-squared \\(R^2\\), the mean absolute error \\(\\text{MAE}\\), and the likelihood. For classification problems we need to a new set of metrics. A confusion matrix shows is a table that summarizes how the model classifies each group. No claims and predicted to not have claims - True Negatives (TN) = 1,489 Had claims and predicted to have claims - True Positives (TP) = 59 No claims but predicted to have claims - False Positives (FP) = 22 Had claims but predicted not to - False Negatives (FN) = 489 confusionMatrix(test$pred_zero_one,factor(test$target))$table ## Reference ## Prediction 0 1 ## 0 1489 489 ## 1 22 59 These definitions allow us to measure performance on the different groups. Precision answers the question “out of all of the positive predictions, what percentage were correct?” \\[\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\\] Recall answers the question “out of all of positive examples in the data set, what percentage were correct?” \\[\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\] The choice of using precision vs. recall depends on the relative cost of making a FP or a FN error. If FP errors are expensive, then use precision; if FN errors are expensive, then use recall. Example A: the model trying to detect a deadly disease, which only 1 out of every 1,000 patient’s survive without early detection. Then the goal should be to optimize recall, because we would want every patient that has the disease to get detected. Example B: the model is detecting which emails are spam or not. If an important email is flagged as spam incorrectly, the cost is 5 hours of lost productivity. In this case, precision is the main concern. In some cases we can compare this “cost” in actual values. For example, if a federal court is predicting if a criminal will recommit or not, they can agree that “1 out of every 20 guilty individuals going free” in exchange for “90% of those who are guilty being convicted”. When money is involved, a dollar amount can be used: flagging non-spam as spam may cost $100 whereas missing a spam email may cost $2. Then the cost-weighted accuracy is \\[\\text{Cost} = (100)(\\text{FN}) + (2)(\\text{FP})\\] The cutoff value can be tuned in order to find the minimum cost. Fortunately, all of this is handled in a single function called confusionMatrix. confusionMatrix(test$pred_zero_one,factor(test$target)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 1489 489 ## 1 22 59 ## ## Accuracy : 0.7518 ## 95% CI : (0.7326, 0.7704) ## No Information Rate : 0.7339 ## P-Value [Acc &gt; NIR] : 0.03366 ## ## Kappa : 0.1278 ## ## Mcnemar&#39;s Test P-Value : &lt; 2e-16 ## ## Sensitivity : 0.9854 ## Specificity : 0.1077 ## Pos Pred Value : 0.7528 ## Neg Pred Value : 0.7284 ## Prevalence : 0.7339 ## Detection Rate : 0.7232 ## Detection Prevalence : 0.9607 ## Balanced Accuracy : 0.5466 ## ## &#39;Positive&#39; Class : 0 ## 7.1 Area Under the ROC Curve (AUC) What if we look at both the true-positive rate (TPR) and false positive rate (FPR) simultaneously? That is, for each value of the cutoff, we can calculate the TPR and TNR. For example, say that we have 10 cutoff values, \\(\\{k_1, k_2, ..., k_{10}\\}\\). Then for each value of \\(k\\) we calculate both the true positive rates \\[\\text{TPR} = \\{\\text{TPR}(k_1), \\text{TPR}(k_2), .., \\text{TPR}(k_{10})\\} \\] and the true negative rates \\[\\{\\text{FNR} = \\{\\text{FNR}(k_1), \\text{FNR}(k_2), .., \\text{FNR}(k_{10})\\}\\] Then we set x = TPR and y = FNR and graph x against y. The resulting plot is called the Receiver Operator Curve (ROC) and the the Area Under the Curve is called the AUC. You can also think of AUC as being a probability. Unlike a conventional probability, this ranges between 0.5 and 1 instead of 0 and 1. In the Logit example, we were predicting whether or not an auto policy would file a claim. Then you can interpret the AUC as The expected proportion of positives ranked before a uniformly drawn random negative The probability that a model prediction for a policy which actually filed a claim is greater than the model prediction for a policy which did not file a claim The expected true positive rate if the ranking is split just before a uniformly drawn random negative. The expected proportion of negatives ranked after a uniformly drawn random positive. The expected false positive rate if the ranking is split just after a uniformly drawn random positive. You can save yourself time by memorizing these three scenarios: \\[ \\text{AUC} = 1.0 \\] This is a perfect model that predicts the correct class for new data each time. It will have a ROC plot showing the curve approaching the top left corner so that the area of the square is 1.0. \\[ \\text{AUC} = 0.5 \\]* When the ROC curve runs along the diagonal then the area is 0.5. This performance is no better than randomly selecting the class for new data such that the proportions of each class matches that of the data. \\[ \\text{AUC} &lt; 0.5 \\] Any model having an AUC less than 0.5 means it is providing predictions that are worse than random selection, with a near 0 AUC indicating that the model makes the wrong classification almost every time. This can occur in two ways The model is overfitting. For example, the AUC on the train data set may be higher than 0.8 but only 0.2 on the test data set. This indicates that you need to adjust your model’s parameters. See the chapter on the Bias-Variance Tradeoff. There is an error in the AUC calculation or model prediction. 7.2 Example - Auto Claims Let’s create an ROC curve and find the AUC for our logit. library(pROC) roc(test$target, preds, plot = T) Figure 7.1: AUC for auto_claim ## ## Call: ## roc.default(response = test$target, predictor = preds, plot = T) ## ## Data: preds in 1511 controls (test$target 0) &lt; 548 cases (test$target 1). ## Area under the curve: 0.7558 If we just randomly guess, the AUC would be 0.5, which is represented by the 45-degree line. A perfect model would maximize the curve to the upper-left corner. The AUC of 0.76 is decent. If we had multiple models we could compare them based on the AUC. In general, AUC is preferred over Accuracy when there are a lot more “true” classes than “false” classes, which is known as having class imbalance. An example is bank fraud detection: 99.99% of bank transactions are “false” or “0” classes, and so optimizing for accuracy alone will result in a low sensitivity for detecting actual fraud. 7.3 Example: SOA HR, Task 5 The following question is from the Hospital Readmissions sample project from 2018. Already enrolled? Watch the full video. \"With the target variable being only 0 or 1, the binomial distribution is the only reasonable choice. your assistant has done some research and learned that for the glm package in R there are five link functions that can be used with the binomial distribution. They are shown below (the inverse of the link function is presented here as it represents how the linear predictor is transformed into the actual response), where \\(\\nu\\) is the linear predictor and \\(\\p\\) is the response. I split the data up into a training and test set, where 75% of the patients where used for training the model and the other 25% were used for testing. The readmission rate was about 13% in both data sets. TRAIN: 0.1252346 TEST: 0.1280101 A GLM allows for multiple inputs to be mapped to predict a single output, called the response. In this case, the response is binary and so only the binomial response distribution makes sense. The model predictors are related to the predicted readmission through a link function. Because we want the result to be a probability between 0 and 1, we limit out choices of link functions to those which have a domain of (0,1). I ruled out the log link immediately because it allows for outputs outside of 0 and 1. I tested out the model on the test set for the other links. I compare the model’s performance based on the AUC score. The goal of this analysis was to beat the LACE index of 0.7. Logit The logit is the most common link function for binary classification is the canonical link for the binomial family. It is also the default in R, and is commonly used in Logistic Regression. Area under the curve: 0.7451 Probit The Probit is the inverse cumulative distribution of a standard normal random variable. The AUC the same as the logit. Area under the curve: 0.7451 Cauchit The Cauchit has a worse (lower) AUC than the Logit or Probit and so it is not considered further. Area under the curve: 0.7449 ClogLog Area under the curve: 0.745 I chose to use the Logit link. 7.4 Example: SOA PA 12/12/19, Task 11 Already enrolled? Watch the full video. Marketing has asked for a demonstration of how your model is to be used with examples of cases that predict high value and cases that predict low value. Your assistant has prepared some sample cases that can be run through your model. You may need to adjust some of them to obtain illustrative examples that would be of interest to marketing. Write, in language appropriate for marketing, the illustration and demonstration they are looking for. This demonstration should be more detailed than what will go into your executive summary (which could include an example). The sample cases are provided here and in your report template in case you wish to include them in your report. The answer asks you to run the example cases through your model to calculate the predicted probability of each case. You need to create the column Prob of high using your GLM. Then you assign each case as being “High” or “Low” depending on if this value is above the cutoff. The values that change are in bold. age education num marital status occupation cap_ga in hours_per week score Prob of high Value 39 10 Married-spouse Group 3 0 40 60 0.32 High 39 10 Never-married Group 3 0 40 60 0.24 Low 39 5 Married-spouse Group 3 0 40 60 0.39 High 39 10 Married-spouse Group 5 0 40 60 0.80 High 39 10 Married-spouse Group 3 0 30 60 0.18 Low You can interpret this as: The typical profitable customer is middle aged (39 years old), has 10 years of education, is married, and in group 3 Having never been married decreases profitability Being less educated does not decreases profitability. You can see this because the customer with education_num = 5 has the same characteristics as the first customer Being in Group 5 increases profitability. You can see this because Prob of high increases to 0.8. Working fewer than 40 hours per week decreases profitability This was a difficult question. Don’t be afraid to be assertive and think creatively or to change the values that they give you. The SOA’s solutions says Many candidates struggled with this task. Candidates needed to include sample cases that resulted in low and high value predictions and clearly describe the analysis for the marketing team. Candidates were encouraged to modify the supplied cases. Few elected to test changes in both directions from the base case. 7.5 Additional reading Title Source An Overview of Classification ISL 4.1 Understanding AUC - ROC Curv Sarang Narkhede, Towards Data Science Precision vs. Recall Shruti Saxena, Towards Data Science "],
["additional-glm-topics.html", " 8 Additional GLM topics 8.1 Residuals 8.2 Example 8.3 Log transforms of predictors 8.4 Reference levels 8.5 Interactions 8.6 Offsets 8.7 Tweedie regression 8.8 Combinations of Link Functions and Target Distributions", " 8 Additional GLM topics As you can tell, PA has a lot of small topics related to GLMs. This chapter completes some of the residual (no pun intended) topics. 8.1 Residuals Learning from mistakes is the path to improvement. For GLMs, residual analysis looks for patterns in the errors in order to find ways of improving the model. 8.1.1 Raw residuals The word “residual” by itself actually means the “raw residual” in GLM language. This is the difference in actual vs. predicted values. \\[\\text{Raw Residual} = y_i - \\hat{y_i}\\] 8.1.2 Deviance residuals This is not meaningful for GLMs with non-Gaussian distributions. To adjust for other distributions, we need the concept of deviance residuals. Deviance is a way of assessing the adequacy of a model by comparing it with a more general model with the maximum number of parameters that can be estimated. It is referred to as the saturated model. In the saturated model there is basically one parameter per observation. The deviance assesses the goodness of fit for the model by looking at the difference between the log-likelihood functions of the saturated model and the model under investigation, i.e. \\(l(b_{sat},y) - l(b,y)\\). Here sat \\(b_{sat}\\) denotes the maximum likelihood estimator of the parameter vector of the saturated model, \\(\\beta_{sat}\\) , and \\(b\\) is the maximum likelihood estimator of the parameters of the model under investigation, \\(\\beta\\). The maximum likelihood estimator is the estimator that maximizes the likelihood function. The deviance is defined as \\[D = 2[l(b_{sat},y) - l(b,y)]\\] The deviance residual uses the deviance of the ith observation \\(d_i\\) and then takes the square root and applies the same sign (aka, the + or - part) of the raw residual. \\[\\text{Deviance Residual} = \\text{sign}(y_i - \\hat{y_i})\\sqrt{d_i}\\] 8.2 Example Just as with OLS, there is a formula and data argument. In addition, we need to specify the target distribution and link function. model = glm(formula = charges ~ age + sex + smoker, family = Gamma(link = &quot;log&quot;), data = health_insurance) We see that age, sex, and smoker are all significant (p &lt;0.01). Reading off the coefficient signs, we see that claims Increase as age increases Are higher for women Are higher for smokers model %&gt;% tidy() ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 7.82 0.0600 130. 0. ## 2 age 0.0290 0.00134 21.6 3.40e- 89 ## 3 sexmale -0.0468 0.0377 -1.24 2.15e- 1 ## 4 smokeryes 1.50 0.0467 32.1 3.25e-168 Below you can see graph of deviance residuals vs. the predicted values. If this were a perfect model, all of these below assumptions would be met: Scattered around zero? Constant variance? No obvious pattern? plot(model, which = 3) The quantile-quantile (QQ) plot shows the quantiles of the deviance residuals (i.e., after adjusting for the Gamma distribution) against theoretical Gaussian quantiles. In a perfect model, all of these assumptions would be met: Points lie on a straight line? Tails are not significantly above or below line? Some tail deviation is ok. No sudden “jumps”? This indicates many \\(Y\\)’s which have the same value, such as insurance claims which all have the exact value of $100.00 or $0.00. plot(model, which = 2) 8.3 Log transforms of predictors When a log link is used, taking the natural logs of continuous variables allows for the scale of each predictor to match the scale of the thing that they are predicting, the log of the mean of the response. In addition, when the distribution of the continuous variable is skewed, taking the log helps to make it more symmetric. After taking the log of a predictor, the interpretation becomes a power transform of the original variable. For \\(\\mu\\) the mean response, \\[log(\\mu) = \\beta_0 + \\beta_1 log(X)\\] To solve for \\(\\mu\\), take the exponent of both sides \\[\\mu = e^{\\beta_1} e^{\\beta_1 log(X)} = e^{\\beta_0} X^{\\beta_1}\\] ### Example In the Hospital Readmission sample project, one of the predictor variables, “Length of stay”, is the number of days since a person has been readmitted to the hospital. You can tell that it’s right-skewd because the median is higher than the mean. summary(readmission$LOS) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 3.000 5.000 6.693 8.000 36.000 But it could also be thought of as a discrete variable because it only takes on 36 values. Should you still apply a log transform? readmission %&gt;% count(LOS) ## # A tibble: 36 x 2 ## LOS n ## &lt;dbl&gt; &lt;int&gt; ## 1 1 986 ## 2 2 7646 ## 3 3 9775 ## 4 4 11325 ## 5 5 8365 ## 6 6 6020 ## 7 7 4600 ## 8 8 3534 ## 9 9 2719 ## 10 10 1997 ## # ... with 26 more rows Here are the histograms Yes, the SOA’s solution applys the log transform. 8.4 Reference levels When a categorical variable is used in a GLM, the model actually uses indicator variables for each level. The default reference level is the order of the R factors. For the sex variable, the order is female and then male. This means that the base level is female by default. health_insurance$sex %&gt;% as.factor() %&gt;% levels() ## [1] &quot;female&quot; &quot;male&quot; Why does this matter? Statistically, the coefficients are most stable when there are more observations. health_insurance$sex %&gt;% as.factor() %&gt;% summary() ## female male ## 662 676 There is already a function to do this in the tidyverse called fct_infreq. Let’s quickly fix the sex column so that these factor levels are in order of frequency. health_insurance &lt;- health_insurance %&gt;% mutate(sex = fct_infreq(sex)) Now male is the base level. health_insurance$sex %&gt;% as.factor() %&gt;% levels() ## [1] &quot;male&quot; &quot;female&quot; 8.5 Interactions An interaction occurs when the effect of a variable on the response is different depending on the level of other variables in the model. Consider this model: Let \\(x_2\\) be an indicator variable, which is 1 for some observations and 0 otherwise. \\[\\hat{y_i} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2\\] There are now two different linear models depending on whether x_1 is 0 or 1. When \\(x_1 = 0\\), \\[\\hat{y_i} = \\beta_0 + \\beta_2 x_2\\] and when \\(x_1 = 1\\) \\[\\hat{y_i} = \\beta_0 + \\beta_1 + \\beta_2 x_2 + \\beta_3 x_2\\] By rewriting this we can see that the intercept changes from \\(\\beta_0\\) to \\(\\beta_0^*\\) and the slope changes from \\(\\beta_1\\) to \\(\\beta_1^*\\) \\[ (\\beta_0 + \\beta_1) + (\\beta_2 + \\beta_3 ) x_2 \\\\ = \\beta_0^* + \\beta_1^* x_2 \\] The SOA’s modules give an example with the using age and gender as below. This is not a very strong interaction, as the slopes are almost identical across gender. interactions %&gt;% ggplot(aes(age, actual, color = gender)) + geom_line() + labs(title = &quot;Age vs. Actual by Gender&quot;, subtitle = &quot;Interactions imply different slopes&quot;, caption= &quot;data: interactions&quot;) Figure 8.1: Example of weak interaction Here is a clearer example from the auto_claim data. The lines show the slope of a linear model, assuming that only BLUEBOOK and CAR_TYPE were predictors in the model. You can see that the slope for Sedans and Sports Cars is higher than for Vans and Panel Trucks. auto_claim %&gt;% sample_frac(0.2) %&gt;% ggplot(aes(log(CLM_AMT), log(BLUEBOOK), color = CAR_TYPE)) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, se = F) + labs(title = &quot;Kelly Bluebook Value vs Claim Amount&quot;) Figure 8.2: Example of strong interaction Any time that the effect that one variable has on the response is different depending on the value of other variables we say that there is an interaction. We can also use an hypothesis test with a GLM to check this. Simply include an interaction term and see if the coefficient is zero at the desired significance level. 8.6 Offsets In certain situations, it is convenient to include a constant term in the linear predictor. This is the same as including a variable that has a coefficient equal to 1. We call this an offset. \\[g(\\mu) = \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p + \\text{offset}\\] On Exam PA, offsets will only be used for one special case: With Poisson regression With a log link function As a measure of exposure (usually length of policy period) While it’s technically possible to use offsets in other ways, this is not likely to appear on PA. If modeling the spread of COVID, the exposure would be the number of people who were exposed to the virus and the response would be the number of people who were infected. In auto insurance, the exposure might be the number of months of coverage and the response would be the claims incurred. Consider a very simple model which only uses the year that the car was manufactured as a predictor. This expected value of the claims, the target variable, would be \\[log(E[\\frac{\\text{Claims}}{\\text{Months}}]) = \\beta_0 + \\beta_1 \\text{Year}\\] Then you can use the property of the log where \\(log(\\frac{A}{B}) = log(A) - log(B)\\) to move things around. Because \\(\\text{Months}\\) is known, you can remove the expected value. This is the offset term. \\[log(E[\\text{Claims}]) = \\beta_0 + \\beta_1 \\text{Year} + \\text{Months}\\] 8.7 Tweedie regression While this topic is briefly mentioned on the modules, the only R libraries which support Tweedie Regression (statmod and tweedie) are not on the syllabus, and so there is no way that the SOA could ask you to build a tweedie model. This means that you can be safely skip this section. 8.8 Combinations of Link Functions and Target Distributions What is an example of when to use a log link with a Gaussian response? What about a Gamma family with an inverse link? What about an inverse Gaussian response and an inverse square link? As these questions illustrate, there are many combinations of link and response family. In the real world, a model never fits perfectly, and so often these choices come down to the judgement of the modeler - which model is the best fit and meets the business objectives? However, there is one way that we can know for certain which link and response family is the best, and that is if we generate the data ourselves. Recall that a GLM has two parts: A random component: \\(Y|X \\sim \\text{some exponential family distribution}\\) A link function: between the random component and the covariates: \\(g(\\mu(X)) = X\\beta\\) where \\(\\mu = E[Y|X]\\) Following this recipe, we can simulate data from any combination of link function and response family. This helps us to understand the GLM framework very clearly. 8.8.1 Gaussian Response with Log Link We create a function that takes in data \\(x\\) and returns a Gaussian random variable that has mean equal to the inverse link, which in the case of a log link is the exponent. We add 10 to \\(x\\) so that the values will always be positive, as will be described later on. sim_norm &lt;- function(x) { rnorm(1, mean = exp(10 + x), sd = 1) } The values of \\(X\\) do not need to be normal. The above assumption is merely that the mean of the response \\(Y\\) is related to \\(X\\) through the link function, mean = exp(10 + x), and that the distribution is normal. This has been accomplished with rnorm already. For illustration, here we use \\(X\\)’s from a uniform distribution. data &lt;- tibble(x = runif(500)) %&gt;% mutate(y = x %&gt;% map_dbl(sim_norm)) We already know what the answer is: a Gaussian response with a log link. We fit a GLM and see a perfect fit. glm &lt;- glm(y ~ x, family = gaussian(link = &quot;log&quot;), data = data) summary(glm) ## ## Call: ## glm(formula = y ~ x, family = gaussian(link = &quot;log&quot;), data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.05488 -0.73818 -0.01268 0.71014 2.93377 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.000e+01 2.981e-06 3354536 &lt;2e-16 *** ## x 1.000e+00 4.383e-06 228152 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.061249) ## ## Null deviance: 5.817e+10 on 499 degrees of freedom ## Residual deviance: 5.285e+02 on 498 degrees of freedom ## AIC: 1452.7 ## ## Number of Fisher Scoring iterations: 2 par(mfrow = c(2,2)) plot(glm, cex = 0.4) 8.8.2 Gaussian Response with Inverse Link The same steps are repeated except the link function is now the inverse, mean = 1/x. We see that some values of \\(Y\\) are negative, which is ok. sim_norm &lt;- function(x) { rnorm(1, mean = 1/x, 1) } data &lt;- tibble(x = runif(500)) %&gt;% mutate(y = x %&gt;% map_dbl(sim_norm)) summary(data) ## x y ## Min. :0.002351 Min. : -1.392 ## 1st Qu.:0.278258 1st Qu.: 1.149 ## Median :0.528553 Median : 2.287 ## Mean :0.509957 Mean : 6.875 ## 3rd Qu.:0.760526 3rd Qu.: 4.014 ## Max. :0.999992 Max. :425.760 glm &lt;- glm(y ~ x, family = gaussian(link = &quot;inverse&quot;), data = data) summary(glm) ## ## Call: ## glm(formula = y ~ x, family = gaussian(link = &quot;inverse&quot;), data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.11464 -0.77276 -0.02954 0.70110 2.50767 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.605e-06 1.238e-05 0.13 0.897 ## x 9.983e-01 4.032e-03 247.58 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.080454) ## ## Null deviance: 383534.20 on 499 degrees of freedom ## Residual deviance: 538.07 on 498 degrees of freedom ## AIC: 1461.6 ## ## Number of Fisher Scoring iterations: 4 par(mfrow = c(2,2)) plot(glm, cex = 0.4) 8.8.3 Gaussian Response with Identity Link And now the link is the identity, mean = x. sim_norm &lt;- function(x) { rnorm(1, mean = x, 1) } data &lt;- tibble(x = rnorm(500)) %&gt;% mutate(y = x %&gt;% map_dbl(sim_norm)) glm &lt;- glm(y ~ x, family = gaussian(link = &quot;identity&quot;), data = data) summary(glm) ## ## Call: ## glm(formula = y ~ x, family = gaussian(link = &quot;identity&quot;), data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.8731 -0.6460 0.0225 0.6519 3.3242 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.02411 0.04503 0.535 0.593 ## x 1.01347 0.04599 22.035 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.0125) ## ## Null deviance: 995.82 on 499 degrees of freedom ## Residual deviance: 504.23 on 498 degrees of freedom ## AIC: 1429.1 ## ## Number of Fisher Scoring iterations: 2 par(mfrow = c(2,2)) plot(glm, cex = 0.4) 8.8.4 Gaussian Response with Log Link and Negative Values By Gaussian response we say that the mean of the response is Gaussian. The range of a normal random variable is \\((-\\infty, +\\infty)\\), which means that negative values are always possible. Now, if the mean is a large positive number, than negative values are much less likely but still possible: about 95% of the observations will be within 2 standard deviations of the mean. We see below that there are some \\(Y\\) values which are negative. sim_norm &lt;- function(x) { rnorm(1, mean = exp(x), sd = 1) } data &lt;- tibble(x = runif(500)) %&gt;% mutate(y = x %&gt;% map_dbl(sim_norm)) summary(data) ## x y ## Min. :0.0002768 Min. :-1.505 ## 1st Qu.:0.2282455 1st Qu.: 0.947 ## Median :0.5205783 Median : 1.660 ## Mean :0.5081372 Mean : 1.680 ## 3rd Qu.:0.7499811 3rd Qu.: 2.424 ## Max. :0.9984118 Max. : 4.707 We can also see this from the histogram. data %&gt;% ggplot(aes(y)) + geom_density( fill = 1, alpha = 0.3) If we try to fit a GLM with a log link, there is an error. glm &lt;- glm(y ~ x, family = gaussian(link = &quot;log&quot;), data = data) Error in eval(family$initialize) : cannot find valid starting values: please specify some This is because the domain of the natural logarithm only includes positive numbers, and we just tried to take the log of negative numbers. Our initial reaction might be to add some constant to each \\(Y\\), say 10 for instance, so that they are all positive. This does produce a model which is a good fit. glm &lt;- glm(y + 10 ~ x, family = gaussian(link = &quot;log&quot;), data = data) summary(glm) ## ## Call: ## glm(formula = y + 10 ~ x, family = gaussian(link = &quot;log&quot;), data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.14633 -0.63601 -0.01264 0.70930 2.87307 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.386801 0.007862 303.58 &lt;2e-16 *** ## x 0.138190 0.012846 10.76 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.017812) ## ## Null deviance: 624.76 on 499 degrees of freedom ## Residual deviance: 506.87 on 498 degrees of freedom ## AIC: 1431.8 ## ## Number of Fisher Scoring iterations: 4 par(mfrow = c(2,2)) plot(glm, cex = 0.4) We see that on average, the predictions are 10 higher than the target. This is no surprise since \\(E[Y + 10] = E[Y] + 10\\). y &lt;- data$y y_hat &lt;- predict(glm, type = &quot;response&quot;) mean(y_hat) - mean(y) ## [1] 9.999967 But we see that the actual predictions are bad. If we were to loot at the R-squared, MAE, RMSE, or any other metric it would tell us the same story. This is because our GLM assumption is not that \\(Y\\) is related to the link function of \\(X\\), but that the mean of \\(Y\\) is. tibble(y = y, y_hat = y_hat - 10) %&gt;% ggplot(aes(y, y_hat)) + geom_point() One solution is to adjust the \\(X\\) which the model is based on. Add a constant term to \\(X\\) so that the mean of \\(Y\\) is larger, and hence \\(Y\\) is non zero. While is a viable approach in the case of only one predictor variable, with more predictors this would not be easy to do. data &lt;- tibble(x = runif(500) + 10) %&gt;% mutate(y = x %&gt;% map_dbl(sim_norm)) summary(data) ## x y ## Min. :10.00 Min. :22040 ## 1st Qu.:10.27 1st Qu.:28835 ## Median :10.50 Median :36402 ## Mean :10.50 Mean :37838 ## 3rd Qu.:10.75 3rd Qu.:46688 ## Max. :11.00 Max. :59756 glm &lt;- glm(y ~ x, family = gaussian(link = &quot;log&quot;), data = data) par(mfrow = c(2,2)) plot(glm, cex = 0.4) A better approach may be to use an inverse link even though the data was generated from a log link. This is a good illustration of the saying “all models are wrong, but some are useful” in that the statistical assumption of the model is not correct but the model still works. data &lt;- tibble(x = runif(500)) %&gt;% mutate(y = x %&gt;% map_dbl(sim_norm)) glm &lt;- glm(y ~ x, family = gaussian(link = &quot;inverse&quot;), data = data) par(mfrow = c(2,2)) plot(glm, cex = 0.4) summary(glm) ## ## Call: ## glm(formula = y ~ x, family = gaussian(link = &quot;inverse&quot;), data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.63660 -0.71949 -0.02573 0.70123 2.68053 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.87559 0.04503 19.44 &lt;2e-16 *** ## x -0.53090 0.05703 -9.31 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.104314) ## ## Null deviance: 662.67 on 499 degrees of freedom ## Residual deviance: 549.95 on 498 degrees of freedom ## AIC: 1472.5 ## ## Number of Fisher Scoring iterations: 6 8.8.5 Gamma Response with Log Link The gamma distribution with rate parameter \\(\\alpha\\) and scale parameter \\(\\theta\\) is density. \\[f(y) = \\frac{(y/\\theta)^\\alpha}{x \\Gamma(\\alpha)}e^{-x/\\theta}\\] The mean is \\(\\alpha\\theta\\). Let’s use a gamma with shape 2 and scale 0.5, which has mean 1. gammas &lt;- rgamma(500, shape=2, scale = 0.5) mean(gammas) ## [1] 1.018104 We then generate random gamma values. Because the mean now depends on two parameters instead of one, which was just \\(\\mu\\) in the Gaussian case, we need to use a slightly different approach to simulate the random values. The link function here is seen in exp(x). #random component x &lt;- runif(1000, min=0, max=100) #relate Y to X with a log link function y &lt;- gammas*exp(x) data &lt;- tibble(x = x, y = y) summary(data) ## x y ## Min. : 0.1447 Min. :0.000e+00 ## 1st Qu.:25.0189 1st Qu.:4.231e+10 ## Median :49.2899 Median :1.476e+21 ## Mean :50.0386 Mean :2.544e+41 ## 3rd Qu.:76.4823 3rd Qu.:8.476e+32 ## Max. :99.8365 Max. :2.595e+43 As expected, the residual plots are all perfect because the model is perfect. glm &lt;- glm(y ~ x, family = Gamma(link = &quot;log&quot;), data = data) par(mfrow = c(2,2)) plot(glm, cex = 0.4) If we had tried using an inverse instead of the log, the residual plots would look much worse. glm &lt;- glm(y ~ x, family = Gamma(link = &quot;inverse&quot;), data = data) par(mfrow = c(2,2)) plot(glm, cex = 0.4) ## Warning in sqrt(crit * p * (1 - hh)/hh): NaNs produced ## Warning in sqrt(crit * p * (1 - hh)/hh): NaNs produced 8.8.6 Gamma with Inverse Link With the inverse link, the mean has a factor 1/(x + 1). Note that we need to add 1 to x to avoid dividing by zero. #relate Y to X with a log link function y &lt;- gammas*1/(x + 1) data &lt;- tibble(x = x, y = y) summary(data) ## x y ## Min. : 0.1447 Min. :0.0005573 ## 1st Qu.:25.0189 1st Qu.:0.0095364 ## Median :49.2899 Median :0.0177203 ## Mean :50.0386 Mean :0.0454184 ## 3rd Qu.:76.4823 3rd Qu.:0.0383153 ## Max. :99.8365 Max. :1.5248073 glm &lt;- glm(y ~ x, family = Gamma(link = &quot;inverse&quot;), data = data) par(mfrow = c(2,2)) plot(glm, cex = 0.4) "],
["glm-variable-selection.html", " 9 GLM variable selection 9.1 Stepwise subset selection 9.2 Example: SOA PA 6/12/19, Task 6 9.3 Penalized Linear Models 9.4 Ridge Regression 9.5 Lasso 9.6 Elastic Net 9.7 Advantages and disadvantages 9.8 Example: Ridge Regression 9.9 Example: The Lasso", " 9 GLM variable selection Predictive Analytics is about using results to solve business problems. Complex models are almost useless if they cannot be explained. In this chapter, we will learn how to make GLMs easier to explain by either removing variables entirely or lessening their impact. 9.1 Stepwise subset selection In theory, we could test all possible combinations of variables and interaction terms. This includes all \\(p\\) models with one predictor, all p-choose-2 models with two predictors, all p-choose-3 models with three predictors, and so forth. Then we take whichever model has the best performance as the final model. This “brute force” approach is statistically ineffective: the more variables which are searched, the higher the chance of finding models that over fit. A subtler method, known as stepwise selection, reduces the chances of over-fitting by only looking at the most promising models. Forward Stepwise Selection: Start with no predictors in the model; Evaluate all \\(p\\) models which use only one predictor and choose the one with the best performance (highest \\(R^2\\) or lowest \\(\\text{RSS}\\)); Repeat the process when adding one additional predictor, and continue until there is a model with one predictor, a model with two predictors, a model with three predictors, and so forth until there are \\(p\\) models; Select the single best model which has the best \\(\\text{AIC}\\),\\(\\text{BIC}\\), or adjusted \\(R^2\\). Backward Stepwise Selection: Start with a model that contains all predictors; Create a model which removes all predictors; Choose the best model which removes all-but-one predictor; Choose the best model which removes all-but-two predictors; Continue until there are \\(p\\) models; Select the single best model which has the best \\(\\text{AIC}\\),\\(\\text{BIC}\\), or adjusted \\(R^2\\). Both Forward &amp; Backward Selection: A hybrid approach is to consider use both forward and backward selection. This is done by creating two lists of variables at each step, one from forward and one from backward selection. Then variables from both lists are tested to see if adding or subtracting from the current model would improve the fit or not. ISLR does not mention this directly, however, by default the stepAIC function uses a default of both. Tip: Always load the MASS library before dplyr or tidyverse. Otherwise there will be conflicts as there are functions named select() and filter() in both. Alternatively, specify the library in the function call with dplyr::select(). Readings CAS Monograph 5 Chapter 2 9.2 Example: SOA PA 6/12/19, Task 6 Already enrolled? Watch the full video. AIC and BIC are among the available techniques for feature selection. Briefly describe them and outline the differences in the two criteria. Make a recommendation as to which one should be used for this problem. Use only your recommended criterion when completing this task. Some of the features may lack predictive power and lead to overfitting. Determine which features should be retained. Use the stepAIC function (from the MASS package) to make this determination. When using this function, there are two decisions to make. Make each decision based on the business problem. Use ?stepAIC to learn more about these parameters (note that the MASS package must be loaded before help on this function can be accessed). Use direction = “backward” or direction = “forward” Use AIC (k = 2) or BIC (k=log(nrow(train))) 9.3 Penalized Linear Models One of the main weaknesses of the GLM, including all linear models in this chapter, is that the features need to be selected by hand. Stepwise selection helps to improve this process, but fails when the inputs are correlated and often has a strong dependence on seemingly arbitrary choices of evaluation metrics such as using AIC or BIC and forward or backward directions. The Bias Variance Trade-off is about finding the lowest error by changing the flexibility of the model. Penalization methods use a parameter to control for this flexibility directly. Earlier on we said that the linear model minimizes the sum of square terms, known as the residual sum of squares (RSS) \\[ \\text{RSS} = \\sum_i(y_i - \\hat{y})^2 = \\sum_i(y_i - \\beta_0 - \\sum_{j = 1}^p\\beta_j x_{ij})^2 \\] This loss function can be modified so that models which include more (and larger) coefficients are considered as worse. In other words, when there are more \\(\\beta\\)’s, or \\(\\beta\\)’s which are larger, the RSS is higher. 9.4 Ridge Regression Ridge regression adds a penalty term which is proportional to the square of the sum of the coefficients. This is known as the “L2” norm. \\[ \\sum_i(y_i - \\beta_0 - \\sum_{j = 1}^p\\beta_j x_{ij})^2 + \\lambda \\sum_{j = 1}^p\\beta_j^2 \\] This \\(\\lambda\\) controls how much of a penalty is imposed on the size of the coefficients. When \\(\\lambda\\) is high, simpler models are treated more favorably because the \\(\\sum_{j = 1}^p\\beta_j^2\\) carries more weight. Conversely, then \\(\\lambda\\) is low, complex models are more favored. When \\(\\lambda = 0\\), we have an ordinary GLM. 9.5 Lasso The official name is the Least Absolute Shrinkage and Selection Operator, but the common name is just “the lasso”. Just as with Ridge regression, we want to favor simpler models; however, we also want to select variables. This is the same as forcing some coefficients to be equal to 0. Instead of taking the square of the coefficients (L2 norm), we take the absolute value (L1 norm). \\[ \\sum_i(y_i - \\beta_0 - \\sum_{j = 1}^p\\beta_j x_{ij})^2 + \\lambda \\sum_{j = 1}^p|\\beta_j| \\] In ISLR, Hastie et al show that this results in coefficients being forced to be exactly 0. This is extremely useful because it means that by changing \\(\\lambda\\), we can select how many variables to use in the model. Note: While any response family is possible with penalized regression, in R, only the Gaussian family is possible in the library glmnet, and so this is the only type of question that the SOA can ask. 9.6 Elastic Net The Elastic Net uses a penalty term which is between the L1 and L2 norms. The penalty term is a weighted average using the mixing parameter \\(0 \\leq \\alpha \\leq 1\\). The loss function is then \\[\\text{RSS} + (1 - \\alpha) \\sum_{j = 1}^{p}\\beta_j^2 + \\alpha \\sum_{j = 1}^p |\\beta_j|\\] When \\(\\alpha = 1\\), the model is known as the Lasso, and when \\(\\alpha = 0\\), the model is known as Ridge Regression. Luckily, none of this needs to be memorized. On the exam, read the documentation in R to refresh your memory. For the Elastic Net, the function is glmnet, and so running ?glmnet will give you this info. Shortcut: When using complicated functions on the exam, use ?function_name to get the documentation. 9.7 Advantages and disadvantages Elastic Net/Lasso/Ridge Advantages All benefits from GLMS Automatic variable selection for Lasso; smaller coefficients for Ridge Better predictive power than GLM Elastic Net/Lasso/Ridge Disadvantages All cons of GLMs Readings ISLR 6.1 Subset Selection ISLR 6.2 Shrinkage Methods 9.8 Example: Ridge Regression We will use the glmnet package in order to perform ridge regression and the lasso. The main function in this package is glmnet(), which can be used to fit ridge regression models, lasso models, and more. This function has slightly different syntax from other model-fitting functions that we have encountered thus far in this book. In particular, we must pass in an \\(x\\) matrix as well as a \\(y\\) vector, and we do not use the \\(y \\sim x\\) syntax. Before proceeding, let’s first ensure that the missing values have been removed from the data, as described in the previous lab. Hitters = na.omit(Hitters) We will now perform ridge regression and the lasso in order to predict Salary on the Hitters data. Let’s set up our data: x = model.matrix(Salary~., Hitters)[,-1] # trim off the first column # leaving only the predictors y = Hitters %&gt;% select(Salary) %&gt;% unlist() %&gt;% as.numeric() The model.matrix() function is particularly useful for creating \\(x\\); not only does it produce a matrix corresponding to the 19 predictors but it also automatically transforms any qualitative variables into dummy variables. The latter property is important because glmnet() can only take numerical, quantitative inputs. The glmnet() function has an alpha argument that determines what type of model is fit. If alpha = 0 then a ridge regression model is fit, and if alpha = 1 then a lasso model is fit. We first fit a ridge regression model: grid = 10^seq(10, -2, length = 100) ridge_mod = glmnet(x, y, alpha = 0, lambda = grid) By default the glmnet() function performs ridge regression for an automatically selected range of \\(\\lambda\\) values. However, here we have chosen to implement the function over a grid of values ranging from \\(\\lambda = 10^10\\) to \\(\\lambda = 10^{-2}\\), essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit. As we will see, we can also compute model fits for a particular value of \\(\\lambda\\) that is not one of the original grid values. Note that by default, the glmnet() function standardizes the variables so that they are on the same scale. To turn off this default setting, use the argument standardize = FALSE. Associated with each value of \\(\\lambda\\) is a vector of ridge regression coefficients, stored in a matrix that can be accessed by coef(). In this case, it is a \\(20 \\times 100\\) matrix, with 20 rows (one for each predictor, plus an intercept) and 100 columns (one for each value of \\(\\lambda\\)). dim(coef(ridge_mod)) ## [1] 20 100 We expect the coefficient estimates to be much smaller, in terms of \\(l_2\\) norm, when a large value of \\(\\lambda\\) is used, as compared to when a small value of \\(\\lambda\\) is used. These are the coefficients when \\(\\lambda = 11498\\), along with their \\(l_2\\) norm: ridge_mod$lambda[50] #Display 50th lambda value ## [1] 11497.57 coef(ridge_mod)[,50] # Display coefficients associated with 50th lambda value ## (Intercept) AtBat Hits HmRun Runs ## 407.356050200 0.036957182 0.138180344 0.524629976 0.230701523 ## RBI Walks Years CAtBat CHits ## 0.239841459 0.289618741 1.107702929 0.003131815 0.011653637 ## CHmRun CRuns CRBI CWalks LeagueN ## 0.087545670 0.023379882 0.024138320 0.025015421 0.085028114 ## DivisionW PutOuts Assists Errors NewLeagueN ## -6.215440973 0.016482577 0.002612988 -0.020502690 0.301433531 sqrt(sum(coef(ridge_mod)[-1,50]^2)) # Calculate l2 norm ## [1] 6.360612 In contrast, here are the coefficients when \\(\\lambda = 705\\), along with their \\(l_2\\) norm. Note the much larger \\(l_2\\) norm of the coefficients associated with this smaller value of \\(\\lambda\\). ridge_mod$lambda[60] #Display 60th lambda value ## [1] 705.4802 coef(ridge_mod)[,60] # Display coefficients associated with 60th lambda value ## (Intercept) AtBat Hits HmRun Runs RBI ## 54.32519950 0.11211115 0.65622409 1.17980910 0.93769713 0.84718546 ## Walks Years CAtBat CHits CHmRun CRuns ## 1.31987948 2.59640425 0.01083413 0.04674557 0.33777318 0.09355528 ## CRBI CWalks LeagueN DivisionW PutOuts Assists ## 0.09780402 0.07189612 13.68370191 -54.65877750 0.11852289 0.01606037 ## Errors NewLeagueN ## -0.70358655 8.61181213 sqrt(sum(coef(ridge_mod)[-1,60]^2)) # Calculate l2 norm ## [1] 57.11001 We can use the predict() function for a number of purposes. For instance, we can obtain the ridge regression coefficients for a new value of \\(\\lambda\\), say 50: predict(ridge_mod, s=50, type=&quot;coefficients&quot;)[1:20,] ## (Intercept) AtBat Hits HmRun Runs ## 4.876610e+01 -3.580999e-01 1.969359e+00 -1.278248e+00 1.145892e+00 ## RBI Walks Years CAtBat CHits ## 8.038292e-01 2.716186e+00 -6.218319e+00 5.447837e-03 1.064895e-01 ## CHmRun CRuns CRBI CWalks LeagueN ## 6.244860e-01 2.214985e-01 2.186914e-01 -1.500245e-01 4.592589e+01 ## DivisionW PutOuts Assists Errors NewLeagueN ## -1.182011e+02 2.502322e-01 1.215665e-01 -3.278600e+00 -9.496680e+00 We now split the samples into a training set and a test set in order to estimate the test error of ridge regression and the lasso. set.seed(1) train = Hitters %&gt;% sample_frac(0.5) test = Hitters %&gt;% setdiff(train) x_train = model.matrix(Salary~., train)[,-1] x_test = model.matrix(Salary~., test)[,-1] y_train = train %&gt;% select(Salary) %&gt;% unlist() %&gt;% as.numeric() y_test = test %&gt;% select(Salary) %&gt;% unlist() %&gt;% as.numeric() Next we fit a ridge regression model on the training set, and evaluate its MSE on the test set, using \\(\\lambda = 4\\). Note the use of the predict() function again: this time we get predictions for a test set, by replacing type=\"coefficients\" with the newx argument. ridge_mod = glmnet(x_train, y_train, alpha=0, lambda = grid, thresh = 1e-12) ridge_pred = predict(ridge_mod, s = 4, newx = x_test) mean((ridge_pred - y_test)^2) ## [1] 139858.6 The test MSE is 101242.7. Note that if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations. In that case, we could compute the test set MSE like this: mean((mean(y_train) - y_test)^2) ## [1] 224692.1 We could also get the same result by fitting a ridge regression model with a very large value of \\(\\lambda\\). Note that 1e10 means \\(10^{10}\\). ridge_pred = predict(ridge_mod, s = 1e10, newx = x_test) mean((ridge_pred - y_test)^2) ## [1] 224692.1 So fitting a ridge regression model with \\(\\lambda = 4\\) leads to a much lower test MSE than fitting a model with just an intercept. We now check whether there is any benefit to performing ridge regression with \\(\\lambda = 4\\) instead of just performing least squares regression. Recall that least squares is simply ridge regression with \\(\\lambda = 0\\). * Note: In order for glmnet() to yield the exact least squares coefficients when \\(\\lambda = 0\\), we use the argument exact=T when calling the predict() function. Otherwise, the predict() function will interpolate over the grid of \\(\\lambda\\) values used in fitting the glmnet() model, yielding approximate results. Even when we use exact = T, there remains a slight discrepancy in the third decimal place between the output of glmnet() when \\(\\lambda = 0\\) and the output of lm(); this is due to numerical approximation on the part of glmnet(). ridge_pred = predict(ridge_mod, s = 0, x = x_train, y = y_train, newx = x_test, exact = T) mean((ridge_pred - y_test)^2) ## [1] 175051.7 lm(Salary~., data = train) ## ## Call: ## lm(formula = Salary ~ ., data = train) ## ## Coefficients: ## (Intercept) AtBat Hits HmRun Runs RBI ## 2.398e+02 -1.639e-03 -2.179e+00 6.337e+00 7.139e-01 8.735e-01 ## Walks Years CAtBat CHits CHmRun CRuns ## 3.594e+00 -1.309e+01 -7.136e-01 3.316e+00 3.407e+00 -5.671e-01 ## CRBI CWalks LeagueN DivisionW PutOuts Assists ## -7.525e-01 2.347e-01 1.322e+02 -1.346e+02 2.099e-01 6.229e-01 ## Errors NewLeagueN ## -4.616e+00 -8.330e+01 predict(ridge_mod, s = 0, x = x_train, y = y_train, exact = T, type=&quot;coefficients&quot;)[1:20,] ## (Intercept) AtBat Hits HmRun Runs ## 239.83274953 -0.00175359 -2.17853087 6.33694957 0.71369687 ## RBI Walks Years CAtBat CHits ## 0.87329878 3.59421378 -13.09231408 -0.71351092 3.31523605 ## CHmRun CRuns CRBI CWalks LeagueN ## 3.40701392 -0.56709530 -0.75240961 0.23467433 132.15949536 ## DivisionW PutOuts Assists Errors NewLeagueN ## -134.58503816 0.20992473 0.62288126 -4.61583857 -83.29432536 It looks like we are indeed improving over regular least-squares! Side note: in general, if we want to fit a (unpenalized) least squares model, then we should use the lm() function, since that function provides more useful outputs, such as standard errors and \\(p\\)-values for the coefficients. Instead of arbitrarily choosing \\(\\lambda = 4\\), it would be better to use cross-validation to choose the tuning parameter \\(\\lambda\\). We can do this using the built-in cross-validation function, cv.glmnet(). By default, the function performs 10-fold cross-validation, though this can be changed using the argument folds. Note that we set a random seed first so our results will be reproducible, since the choice of the cross-validation folds is random. set.seed(1) cv.out = cv.glmnet(x_train, y_train, alpha = 0) # Fit ridge regression model on training data plot(cv.out) # Draw plot of training MSE as a function of lambda bestlam = cv.out$lambda.min # Select lamda that minimizes training MSE bestlam ## [1] 326.1406 Therefore, we see that the value of \\(\\lambda\\) that results in the smallest cross-validation error is 339.1845 What is the test MSE associated with this value of \\(\\lambda\\)? ridge_pred = predict(ridge_mod, s = bestlam, newx = x_test) # Use best lambda to predict test data mean((ridge_pred - y_test)^2) # Calculate test MSE ## [1] 140056.2 This represents a further improvement over the test MSE that we got using \\(\\lambda = 4\\). Finally, we refit our ridge regression model on the full data set, using the value of \\(\\lambda\\) chosen by cross-validation, and examine the coefficient estimates. out = glmnet(x, y, alpha = 0) # Fit ridge regression model on full dataset predict(out, type = &quot;coefficients&quot;, s = bestlam)[1:20,] # Display coefficients using lambda chosen by CV ## (Intercept) AtBat Hits HmRun Runs RBI ## 15.44835008 0.07716945 0.85906253 0.60120339 1.06366687 0.87936073 ## Walks Years CAtBat CHits CHmRun CRuns ## 1.62437580 1.35296287 0.01134998 0.05746377 0.40678422 0.11455696 ## CRBI CWalks LeagueN DivisionW PutOuts Assists ## 0.12115916 0.05299953 22.08942749 -79.03490973 0.16618830 0.02941513 ## Errors NewLeagueN ## -1.36075644 9.12528398 As expected, none of the coefficients are exactly zero - ridge regression does not perform variable selection! 9.9 Example: The Lasso We saw that ridge regression with a wise choice of \\(\\lambda\\) can outperform least squares as well as the null model on the Hitters data set. We now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. In order to fit a lasso model, we once again use the glmnet() function; however, this time we use the argument alpha=1. Other than that change, we proceed just as we did in fitting a ridge model: lasso_mod = glmnet(x_train, y_train, alpha = 1, lambda = grid) # Fit lasso model on training data plot(lasso_mod) # Draw plot of coefficients Notice that in the coefficient plot that depending on the choice of tuning parameter, some of the coefficients are exactly equal to zero. We now perform cross-validation and compute the associated test error: set.seed(1) cv.out = cv.glmnet(x_train, y_train, alpha = 1) # Fit lasso model on training data plot(cv.out) # Draw plot of training MSE as a function of lambda bestlam = cv.out$lambda.min # Select lamda that minimizes training MSE lasso_pred = predict(lasso_mod, s = bestlam, newx = x_test) # Use best lambda to predict test data mean((lasso_pred - y_test)^2) # Calculate test MSE ## [1] 143273 This is substantially lower than the test set MSE of the null model and of least squares, and very similar to the test MSE of ridge regression with \\(\\lambda\\) chosen by cross-validation. However, the lasso has a substantial advantage over ridge regression in that the resulting coefficient estimates are sparse. Here we see that 12 of the 19 coefficient estimates are exactly zero: out = glmnet(x, y, alpha = 1, lambda = grid) # Fit lasso model on full dataset lasso_coef = predict(out, type = &quot;coefficients&quot;, s = bestlam)[1:20,] # Display coefficients using lambda chosen by CV lasso_coef ## (Intercept) AtBat Hits HmRun Runs ## 1.27429897 -0.05490834 2.18012455 0.00000000 0.00000000 ## RBI Walks Years CAtBat CHits ## 0.00000000 2.29189433 -0.33767315 0.00000000 0.00000000 ## CHmRun CRuns CRBI CWalks LeagueN ## 0.02822467 0.21627609 0.41713051 0.00000000 20.28190194 ## DivisionW PutOuts Assists Errors NewLeagueN ## -116.16524424 0.23751978 0.00000000 -0.85604181 0.00000000 Selecting only the predictors with non-zero coefficients, we see that the lasso model with \\(\\lambda\\) chosen by cross-validation contains only seven variables: lasso_coef[lasso_coef!=0] # Display only non-zero coefficients ## (Intercept) AtBat Hits Walks Years ## 1.27429897 -0.05490834 2.18012455 2.29189433 -0.33767315 ## CHmRun CRuns CRBI LeagueN DivisionW ## 0.02822467 0.21627609 0.41713051 20.28190194 -116.16524424 ## PutOuts Errors ## 0.23751978 -0.85604181 Practice questions: How do ridge regression and the lasso improve on simple least squares? In what cases would you expect ridge regression outperform the lasso, and vice versa? "],
["bias-variance-trade-off.html", " 10 Bias-variance trade-off", " 10 Bias-variance trade-off This is a big topic in machine learning in general but only has had a handful of questions on PA. Without stating this explicitly as “the bias-variance tradeoff”, you have already been using this concept. We first need some definitions: Mean Squared Error (MSE): The sum of the squared difference between the predictions and target. Variance of model: The variance of the parameters, \\(var(f(X))\\). When variance is high, the model is often overfitting. Bias: The difference between the expected value of the estimate and the actual expected value. When bias is high, the model is underfitting and is not complex enough to capture signal in the data. \\[\\text{Model Bias} = E(Y) - f(X)\\] Irreducible Error: Random noise in the data that can never be understood. This is “irreducible” meaning that the model is unable reduce it, but you can reduce it by cleaning the data, transforming variables, and engineering additional features. The Bias-variance trade-off says that when the bias of the parameter estimates increases, the variance decreases and vice versa as the bias decreases. \\[\\text{MSE} = \\text{Variance of model} + \\text{Bias}^2 + \\text{Irreducible Error}\\] Your goal is to make the MSE as small as possible. When you test different models, tune parameters, and perform shrinkage or variable selection, you are changing the bias and the variance. A helpful way to remember this relationship is with the following picture. Imagine that you are at a shooting range and am firing a pistol at a target. Your goal is to get as close to the center of the bullseye as possible. Ideally, your bullets would have low bias and low variance (upper left). This would mean that you consistently hit the center of the target. In the worst case, your bullets would have high bias and high variance (lower right). You would be changing your aim between shots and would not be centered at the bullseye. The other diagnoals (lower left and upper right) are the more common outcomes. Either you keep your arm steady and don’t change your aim between shots but miss the center, or you move around too much and have high variance. You can decrease the variance by using more data. From Exam P, you may remember that the variance of the sample mean decreases as as the square root of \\(N\\), the sample size, increases. To decrease the bias, you can change the type of model being used. Model flexibility is the amount that the model can change. The easiest way to understand flexiblity is in the case of the linear model. A GLM with 1 predictor has low flexibility. A GLM with 100 predictors has high flexibility. This is a general definition because you technically need to consider the size of the coefficients as well. It’s easy to confuse flexibility with the model’s variance, but the two concepts are different. In this GLM example, the variance would be determined by the standard errors on the coefficients. A model with high variance would have large p-values, but this could still be inflexible if only a few predictors are included. Conversely, a model could have high flexibility by having many predictor variables and interaction terms but have low variance if all of the p-values were small. Your goal of PA is to solve a business problem. There is a constant balance between making a model that is interpretable and one that has good performance. Highly flexible models, which are often called black boxes, are only useful for making predictions. The parameters that you change also have an impact. In the case of the lasso or ridge regression, by increasing \\(\\lambda\\) you can decrease the flexibility. For stepwise selection, the value of k controls the amount by which the log likelihood is adjusted based on the number of parameters. As you will see in the next chapter on trees, decision trees have flexibility adjusted by CP and random forests (RFs) and gradient boosted machines (GBMs) have their own parameters. "]
]
