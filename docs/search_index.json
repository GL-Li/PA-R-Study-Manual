[
["index.html", "Exam PA Study Manual 1 What’s in this book 1.1 Get full access 1.2 About the author 1.3 Help to make this book even better", " Exam PA Study Manual Sam Castillo 2019-11-14 1 What’s in this book Explanations of the statistical concepts (Clustering &amp; PCA Sections Pending) All data sets needed packaged in an R library R code examples Tips on taking the exam 1.1 Get full access While the ebook is available online, sign up at ExamPA.net for Two original practice exams Solutions to prior exams Hospital Readmissions (Completed) Student Success (Later this week) June 2019 Exam (Coming next week) An online discussion forum where I will answer your questions Video Example: 1.2 About the author Sam Castillo is a predictive modeler at Milliman in Chicago, maintains a blog about the future of risk, and won the 2019 SOA Predictive Analytics and Fururism’s Jupyter contest. Contact: Support: ExamPATutor@gmail.com 1.3 Help to make this book even better Your suggested improvements can be added to this book immediatelly. A special thanks to the following people who have helped to improve this book and or study package: Erlan Wheeler, David Hill, and Caden Collier. To submit suggestions, see the 20:00 mark of https://www.youtube.com/watch?v=dVqVscgwSpw Clicking the “edit” button at the top of this page Signing in (or signing up) for github Forking the repository Submitting a pull request with the improvements. "],
["the-exam.html", " 2 The exam", " 2 The exam The main challenge of this exam is in communication: both understanding what they want you to do as well as telling the grader what it is that you did. You will have 5 hours and 15 minutes to use RStudio and Excel to fill out a report in Word on a Prometric toaster-oven computer. The syllabus uses fancy language to describe the topics covered on the exam, making it sound more difficult than it should be. A good analogy is a job description that has many complex-sounding tasks, when in reality the day-to-day operations of the employee are far simpler. A non-technical translation is as follows: Writing in Microsoft Word (30-40%) Write in professional language Type more than 50 words-per-minute Manipulating Data in R (15-25%) Quickly clean data sets Find data errors planted by the SOA Perform queries (aggregations, summaries, transformations) Machine learning and statistics (40-50%) Interpret results within a business context Change model parameters "],
["you-already-know-what-learning-is.html", " 3 You already know what learning is", " 3 You already know what learning is All of use are already familiar with how to learn - by improving from our mistakes. By repeating what is successful and avoiding what results in failure, we learn by doing, by experience, or trial-and-error. Machines learn in a similar way. Take for example the process of studying for an exam. Some study methods work well, but other methods do not. The “data” are the practice problems, and the “label” is the answer (A,B,C,D,E). We want to build a mental “model” that reads the question and predicts the answer. We all know that memorizing answers without understanding concepts is ineffective, and statistics calls this “overfitting”. Conversely, not learning enough of the details and only learning the high-level concepts is “underfitting”. The more practice problems that we do, the larger the training data set, and the better the prediction. When we see new problems, ones which have not appeared in the practice exams, we often have a difficult time. Quizing ourselves on realistic questions estimates our preparedness, and this is identical to a process known as “holdout testing” or “cross-validation”. We can clearly state our objective: get as many correct answers as possible! We want to correctly predict the solution to every problem. Said another way, we are trying to minimize the error, known as the “loss function”. Different study methods work well for different people. Some cover material quickly and others slowly absorb every detail. A model has many “parameters” such as the “learning rate”. The only way to know which parameters are best is to test them on real data, known as “training”. "],
["getting-started.html", " 4 Getting started 4.1 Download the data 4.2 Download ISLR 4.3 New users", " 4 Getting started 4.1 Download the data For your convenience, all data in this book, including data from prior exams and sample solutions, has been put into a library called ExamPAData by the author. To access, simplly run the below lines of code to download this data. #check if devtools is installed #then install ExamPAData from github if(&quot;devtools&quot; %in% installed.packages()){ library(devtools) install_github(&quot;sdcastillo/ExamPAData&quot;) } else{ install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;sdcastillo/ExamPAData&quot;) } Once this has run, you can access the data using library(ExamPAData). To check that this is installed correctly see if the insurance data set has loaded. If this returns “object not found”, then the library was not installed. library(ExamPAData) summary(insurance) ## district group age holders ## Min. :1.00 Length:64 Length:64 Min. : 3.00 ## 1st Qu.:1.75 Class :character Class :character 1st Qu.: 46.75 ## Median :2.50 Mode :character Mode :character Median : 136.00 ## Mean :2.50 Mean : 364.98 ## 3rd Qu.:3.25 3rd Qu.: 327.50 ## Max. :4.00 Max. :3582.00 ## claims ## Min. : 0.00 ## 1st Qu.: 9.50 ## Median : 22.00 ## Mean : 49.23 ## 3rd Qu.: 55.50 ## Max. :400.00 4.2 Download ISLR This book references the publically-avialable textbook “An Introduction to Statistical Learning”, which can be downloaded for free http://faculty.marshall.usc.edu/gareth-james/ISL/ If you already have R and Rstudio installed then skip to “Download the data”. 4.3 New users Install R: This is the engine that runs the code. https://cran.r-project.org/mirrors.html Install RStudio This is the tool that helps you to write the code. Just as MS Word creates documents, RStudio creates R scripts and other documents. Download RStudio Desktop (the free edition) and choose a place on your computer to install it. https://rstudio.com/products/rstudio/download/ Set the R library R code is organized into libraries. You want to use the exact same code that will be on the Prometric Computers. This requires installing older versions of libraries. Change your R library to the one which was included within the SOA’s modules. .libPaths(&quot;PATH_TO_SOAS_LIBRARY/PAlibrary&quot;) "],
["r-programming.html", " 5 R programming 5.1 Notebook chunks 5.2 Basic operations 5.3 Lists 5.4 Functions 5.5 Data frames 5.6 Pipes", " 5 R programming This book covers the bare minimum of R programming needed for Exam PA. The book “R for Data Science” provides more detail. https://r4ds.had.co.nz/ 5.1 Notebook chunks On the Exam, you will start with an .Rmd (R Markdown) template, which organize code into R Notebooks. Within each notebook, code is organized into chunks. #this is a chunk Your time is valuable. Throughout this book, I will include useful keyboard shortcuts. Shortcut: To run everything in a chunk quickly, press CTRL + SHIFT + ENTER. To create a new chunk, use CTRL + ALT + I. 5.2 Basic operations The usual math operations apply. #addition 1 + 2 ## [1] 3 3 - 2 ## [1] 1 #multiplication 2*2 ## [1] 4 #division 4/2 ## [1] 2 #exponentiation 2^3 ## [1] 8 There are two assignment operators: = and &lt;-. The latter is preferred because it is specific to assigning a variable to a value. The “=” operator is also used for assigning values in functions (see the functions section). Shortcut: ALT + - creates a &lt;-.. #variable assignment x = 2 y &lt;- 2 #equality 4 == 2 #False ## [1] FALSE 5 == 5 #true ## [1] TRUE 3.14 &gt; 3 #true ## [1] TRUE 3.14 &gt;= 3 #true ## [1] TRUE Vectors can be added just like numbers. The c stands for “concatenate”, which creates vectors. x &lt;- c(1,2) y &lt;- c(3,4) x + y ## [1] 4 6 x*y ## [1] 3 8 z &lt;- x + y z^2 ## [1] 16 36 z/2 ## [1] 2 3 z + 3 ## [1] 7 9 Lists are like vectors but can take any type of object type. I already mentioned numeric types. There are also character (string) types, factor types, and boolean types. character &lt;- &quot;The&quot; character_vector &lt;- c(&quot;The&quot;, &quot;Quick&quot;) Characters are combined with the paste function. a = &quot;The&quot; b = &quot;Quick&quot; c = &quot;Brown&quot; d = &quot;Fox&quot; paste(a,b,c,d) ## [1] &quot;The Quick Brown Fox&quot; Factors are characters that expect only specific values. A character can take on any value. A factor is only allowed a finite number of values. This reduces the memory size. The below factor has only one “level”, which is the list of assigned values. factor = as.factor(character) levels(factor) ## [1] &quot;The&quot; The levels of a factor are by default in R in alphabetical order (Q comes alphabetically before T). factor_vector &lt;- as.factor(character_vector) levels(factor_vector) ## [1] &quot;Quick&quot; &quot;The&quot; In building linear models, the order of the factors matters. In GLMs, the “reference level” or “base level” should always be the level which has the most observations. This will be covered in the section on linear models. Booleans are just True and False values. R understands T or TRUE in the same way. When doing math, bools are converted to 0/1 values where 1 is equivalent to TRUE and 0 FALSE. bool_true &lt;- T bool_false &lt;- F bool_true*bool_false ## [1] 0 Booleans are automatically converted into 0/1 values when there is a math operation. bool_true + 1 ## [1] 2 Vectors work in the same way. bool_vect &lt;- c(T,T, F) sum(bool_vect) ## [1] 2 Vectors are indexed using []. abc &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) abc[1] ## [1] &quot;a&quot; abc[2] ## [1] &quot;b&quot; abc[c(1,3)] ## [1] &quot;a&quot; &quot;c&quot; abc[c(1,2)] ## [1] &quot;a&quot; &quot;b&quot; abc[-2] ## [1] &quot;a&quot; &quot;c&quot; abc[-c(2,3)] ## [1] &quot;a&quot; 5.3 Lists Lists are vectors that can hold mixed object types. Vectors need to be all of the same type. ls &lt;- list(T, &quot;Character&quot;, 3.14) ls ## [[1]] ## [1] TRUE ## ## [[2]] ## [1] &quot;Character&quot; ## ## [[3]] ## [1] 3.14 Lists can be named. ls &lt;- list(bool = T, character = &quot;character&quot;, numeric = 3.14) ls ## $bool ## [1] TRUE ## ## $character ## [1] &quot;character&quot; ## ## $numeric ## [1] 3.14 The $ operator indexes lists. ls$numeric ## [1] 3.14 ls$numeric + 5 ## [1] 8.14 Lists can also be indexed using []. ls[1] ## $bool ## [1] TRUE ls[2] ## $character ## [1] &quot;character&quot; Lists can contain vectors, other lists, and any other object. everything &lt;- list(vector = c(1,2,3), character = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), list = ls) everything ## $vector ## [1] 1 2 3 ## ## $character ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; ## ## $list ## $list$bool ## [1] TRUE ## ## $list$character ## [1] &quot;character&quot; ## ## $list$numeric ## [1] 3.14 To find out the type of an object, use class or str or summary. class(x) ## [1] &quot;numeric&quot; class(everything) ## [1] &quot;list&quot; str(everything) ## List of 3 ## $ vector : num [1:3] 1 2 3 ## $ character: chr [1:3] &quot;a&quot; &quot;b&quot; &quot;c&quot; ## $ list :List of 3 ## ..$ bool : logi TRUE ## ..$ character: chr &quot;character&quot; ## ..$ numeric : num 3.14 summary(everything) ## Length Class Mode ## vector 3 -none- numeric ## character 3 -none- character ## list 3 -none- list 5.4 Functions You only need to understand the very basics of functions for this exam. Still, understanding functions helps you to understand everything in R, since R is a functional programming language, unlike Python, C, VBA, Java which are all object-oriented, or SQL which isn’t really a language but a series of set-operations. Functions do things. The convention is to name a function as a verb. The function make_rainbows() would create a rainbow. The function summarise_vectors would summarise vectors. Functions may or may not have an input and output. If you need to do something in R, there is a high probability that someone has already written a function to do it. That being said, creating simple functions is quite useful. A function that does not return anything greet_me &lt;- function(my_name){ print(paste0(&quot;Hello, &quot;, my_name)) } greet_me(&quot;Future Actuary&quot;) ## [1] &quot;Hello, Future Actuary&quot; A function that returns something When returning something, the return statement is optional. add_together &lt;- function(x, y){ x + y } add_together(2,5) ## [1] 7 add_together &lt;- function(x, y){ return(x + y) } add_together(2,5) ## [1] 7 Functions can work with vectors. x_vector &lt;- c(1,2,3) y_vector &lt;- c(4,5,6) add_together(x_vector, y_vector) ## [1] 5 7 9 Many functions in R actually return lists! This is why R objects can be indexed with dollar sign. library(ExamPAData) model &lt;- lm(charges ~ age, data = health_insurance) model$coefficients ## (Intercept) age ## 3165.8850 257.7226 Here’s a function that returns a list. sum_multiply &lt;- function(x,y){ sum &lt;- x + y product &lt;- x*y list(&quot;Sum&quot; = sum, &quot;Product&quot; = product) } result &lt;- sum_multiply(2,3) result$Sum ## [1] 5 result$Product ## [1] 6 5.5 Data frames R is an old programming language. The original data.frame object has been updated with the newer and better tibble (like the word “table”). Tibbles are really lists of vectors, where each column is a vector. #the tibble library has functions for making tibbles library(tibble) data &lt;- tibble(age = c(25, 35), has_fsa = c(F, T)) data ## # A tibble: 2 x 2 ## age has_fsa ## &lt;dbl&gt; &lt;lgl&gt; ## 1 25 FALSE ## 2 35 TRUE To index columns in a tibble, the same “$” is used as indexing a list. data$age ## [1] 25 35 To find the number of rows and columns, use dim. dim(data) ## [1] 2 2 To fine a summary, use summary summary(data) ## age has_fsa ## Min. :25.0 Mode :logical ## 1st Qu.:27.5 FALSE:1 ## Median :30.0 TRUE :1 ## Mean :30.0 ## 3rd Qu.:32.5 ## Max. :35.0 5.6 Pipes The pipe operator %&gt;% is a way of making code modular, meaning that it can be written and executed in incremental steps. Those familiar with Python’s Pandas will be see that %&gt;% is quite similar to “.”. This also makes code easier to read. In five seconds, tell me what the below code is doing. log(sqrt(exp(log2(sqrt((max(c(3, 4, 16)))))))) ## [1] 1 Getting to the answer of 1 requires starting from the inner-most nested brackets and moving outwards from right to left. The math notation would be slightly easier to read, but still painful. \\[log(\\sqrt{e^{log_2(\\sqrt{max(3,4,16)})}})\\] Here is the same algebra using the pipe. To read this, replace the %&gt;% with the word THEN. #the pipe is from the dplyr library library(dplyr) max(c(3, 4, 16)) %&gt;% sqrt() %&gt;% log2() %&gt;% exp() %&gt;% sqrt() %&gt;% log() ## [1] 1 #max(c(3, 4, 16) THEN #The max of 3, 4, and 16 is 16 # sqrt() THEN #The square root of 16 is 4 # log2() THEN #The log in base 2 of 4 is 2 # exp() THEN #the exponent of 2 is e^2 # sqrt() THEN #the square root of e^2 is e # log() #the natural logarithm of e is 1 Pipes are exceptionally useful for data manipulations, which is covered in the next chapter. Tip: To quickly produce pipes, use CTRL + SHIFT + M. By highlighting only certain sections, we can run the code in steps as if we were using a debugger. This makes testing out code much faster. max(c(3, 4, 16)) ## [1] 16 max(c(3, 4, 16)) %&gt;% sqrt() ## [1] 4 max(c(3, 4, 16)) %&gt;% sqrt() %&gt;% log2() ## [1] 2 max(c(3, 4, 16)) %&gt;% sqrt() %&gt;% log2() %&gt;% exp() ## [1] 7.389056 max(c(3, 4, 16)) %&gt;% sqrt() %&gt;% log2() %&gt;% exp() %&gt;% sqrt() ## [1] 2.718282 max(c(3, 4, 16)) %&gt;% sqrt() %&gt;% log2() %&gt;% exp() %&gt;% sqrt() %&gt;% log() ## [1] 1 "],
["data-manipulation.html", " 6 Data manipulation 6.1 Look at the data 6.2 Transform the data 6.3 Exercises 6.4 Answers to exercises", " 6 Data manipulation About two hours in this exam will be spent just on data manipulation. Putting in extra practice in this area is garanteed to give you a better score because it will free up time that you can use elsewhere. In addition, a common saying when building models is “garbage in means garbage out”, on this exam, mistakes on the data manipulation can lead to lost points on the modeling sections. Suggested reading of R for Data Science (https://r4ds.had.co.nz/index.html): Chapter Topic 9 Introduction 10 Tibbles 12 Tidy data 15 Factors 16 Dates and times 17 Introduction 18 Pipes 19 Functions 20 Vectors All data for this book can be accessed from the package ExamPAData. In the real exam, you will read the file from the Prometric computer. To read files into R, the readr package has several tools, one for each data format. For instance, the most common format, comma separated values (csv) are read with the read_csv() function. Because the data is already loaded, simply use the below code to access the data. library(ExamPAData) 6.1 Look at the data The data that we are using is health_insurance, which has information on patients and their health care costs. The descriptions of the columns are below. age: Age of the individual sex: Sex bmi: Body Mass Index children: Number of children smoker: Is this person a smoker? region: Region charges: Annual health care costs. head() shows the top n rows. head(20) shows the top 20 rows. library(tidyverse) head(health_insurance) ## # A tibble: 6 x 7 ## age sex bmi children smoker region charges ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 19 female 27.9 0 yes southwest 16885. ## 2 18 male 33.8 1 no southeast 1726. ## 3 28 male 33 3 no southeast 4449. ## 4 33 male 22.7 0 no northwest 21984. ## 5 32 male 28.9 0 no northwest 3867. ## 6 31 female 25.7 0 no southeast 3757. Using a pipe is an alternative way of doing this. health_insurance %&gt;% head() Shortcut: Use CTRL + SHFT + M to create pipes %&gt;% The glimpse function is a transpose of the head() function, which can be more spatially efficient. This also gives you the dimension (1,338 rows, 7 columns). health_insurance %&gt;% glimpse() ## Observations: 1,338 ## Variables: 7 ## $ age &lt;dbl&gt; 19, 18, 28, 33, 32, 31, 46, 37, 37, 60, 25, 62, 23, 5... ## $ sex &lt;chr&gt; &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;... ## $ bmi &lt;dbl&gt; 27.900, 33.770, 33.000, 22.705, 28.880, 25.740, 33.44... ## $ children &lt;dbl&gt; 0, 1, 3, 0, 0, 0, 1, 3, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0,... ## $ smoker &lt;chr&gt; &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;... ## $ region &lt;chr&gt; &quot;southwest&quot;, &quot;southeast&quot;, &quot;southeast&quot;, &quot;northwest&quot;, &quot;... ## $ charges &lt;dbl&gt; 16884.924, 1725.552, 4449.462, 21984.471, 3866.855, 3... One of the most useful data science tools is counting things. The function count() gives the number of records by a categorical feature. health_insurance %&gt;% count(children) ## # A tibble: 6 x 2 ## children n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 574 ## 2 1 324 ## 3 2 240 ## 4 3 157 ## 5 4 25 ## 6 5 18 Two categories can be counted at once. This creates a table with all combinations of region and sex and shows the number of records in each category. health_insurance %&gt;% count(region, sex) ## # A tibble: 8 x 3 ## region sex n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 northeast female 161 ## 2 northeast male 163 ## 3 northwest female 164 ## 4 northwest male 161 ## 5 southeast female 175 ## 6 southeast male 189 ## 7 southwest female 162 ## 8 southwest male 163 The summary() function is shows a statistical summary. One caveat is that each column needs to be in it’s appropriate type. For example, smoker, region, and sex are all listed as characters when if they were factors, summary would give you count info. With incorrect data types health_insurance %&gt;% summary() ## age sex bmi children ## Min. :18.00 Length:1338 Min. :15.96 Min. :0.000 ## 1st Qu.:27.00 Class :character 1st Qu.:26.30 1st Qu.:0.000 ## Median :39.00 Mode :character Median :30.40 Median :1.000 ## Mean :39.21 Mean :30.66 Mean :1.095 ## 3rd Qu.:51.00 3rd Qu.:34.69 3rd Qu.:2.000 ## Max. :64.00 Max. :53.13 Max. :5.000 ## smoker region charges ## Length:1338 Length:1338 Min. : 1122 ## Class :character Class :character 1st Qu.: 4740 ## Mode :character Mode :character Median : 9382 ## Mean :13270 ## 3rd Qu.:16640 ## Max. :63770 With correct data types This tells you that there are 324 patients in the northeast, 325 in the northwest, 364 in the southeast, and so fourth. health_insurance &lt;- health_insurance %&gt;% modify_if(is.character, as.factor) health_insurance %&gt;% summary() ## age sex bmi children smoker ## Min. :18.00 female:662 Min. :15.96 Min. :0.000 no :1064 ## 1st Qu.:27.00 male :676 1st Qu.:26.30 1st Qu.:0.000 yes: 274 ## Median :39.00 Median :30.40 Median :1.000 ## Mean :39.21 Mean :30.66 Mean :1.095 ## 3rd Qu.:51.00 3rd Qu.:34.69 3rd Qu.:2.000 ## Max. :64.00 Max. :53.13 Max. :5.000 ## region charges ## northeast:324 Min. : 1122 ## northwest:325 1st Qu.: 4740 ## southeast:364 Median : 9382 ## southwest:325 Mean :13270 ## 3rd Qu.:16640 ## Max. :63770 6.2 Transform the data Transforming, manipulating, querying, and wrangling are synonyms in data terminology. R syntax is designed to be similar to SQL. They begin with a SELECT, use GROUP BY to aggregate, and have a WHERE to remove records. Unlike SQL, the ordering of these does not matter. SELECT can come after a WHERE. R to SQL translation select() -&gt; SELECT mutate() -&gt; user-defined columns summarize() -&gt; aggregated columns left_join() -&gt; LEFT JOIN filter() -&gt; WHERE group_by() -&gt; GROUP BY filter() -&gt; HAVING arrange() -&gt; ORDER BY health_insurance %&gt;% select(age, region) %&gt;% head() ## # A tibble: 6 x 2 ## age region ## &lt;dbl&gt; &lt;fct&gt; ## 1 19 southwest ## 2 18 southeast ## 3 28 southeast ## 4 33 northwest ## 5 32 northwest ## 6 31 southeast Tip: use CTRL + SHIFT + M to create pipes %&gt;%. Let’s look at only those in the southeast region. Instead of WHERE, use filter. health_insurance %&gt;% filter(region == &quot;southeast&quot;) %&gt;% select(age, region) %&gt;% head() ## # A tibble: 6 x 2 ## age region ## &lt;dbl&gt; &lt;fct&gt; ## 1 18 southeast ## 2 28 southeast ## 3 31 southeast ## 4 46 southeast ## 5 62 southeast ## 6 56 southeast The SQL translation is SELECT age, region FROM health_insurance WHERE region = &#39;southeast&#39; Instead of ORDER BY, use arrange. Unlike SQL, the order does not matter and ORDER BY doesn’t need to be last. health_insurance %&gt;% arrange(age) %&gt;% select(age, region) %&gt;% head() ## # A tibble: 6 x 2 ## age region ## &lt;dbl&gt; &lt;fct&gt; ## 1 18 southeast ## 2 18 southeast ## 3 18 northeast ## 4 18 northeast ## 5 18 northeast ## 6 18 southeast The group_by comes before the aggregation, unlike in SQL where the GROUP BY comes last. health_insurance %&gt;% group_by(region) %&gt;% summarise(avg_age = mean(age)) ## # A tibble: 4 x 2 ## region avg_age ## &lt;fct&gt; &lt;dbl&gt; ## 1 northeast 39.3 ## 2 northwest 39.2 ## 3 southeast 38.9 ## 4 southwest 39.5 In SQL, this would be SELECT region, AVG(age) as avg_age FROM health_insurance GROUP BY region Just like in SQL, many different aggregate functions can be used such as SUM, MEAN, MIN, MAX, and so forth. health_insurance %&gt;% group_by(region) %&gt;% summarise(avg_age = mean(age), max_age = max(age), median_charges = median(charges), bmi_std_dev = sd(bmi)) ## # A tibble: 4 x 5 ## region avg_age max_age median_charges bmi_std_dev ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 northeast 39.3 64 10058. 5.94 ## 2 northwest 39.2 64 8966. 5.14 ## 3 southeast 38.9 64 9294. 6.48 ## 4 southwest 39.5 64 8799. 5.69 To create new columns, the mutate function is used. For example, if we wanted a column of the person’s annual charges divided by their age health_insurance %&gt;% mutate(charges_over_age = charges/age) %&gt;% select(age, charges, charges_over_age) %&gt;% head(5) ## # A tibble: 5 x 3 ## age charges charges_over_age ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 19 16885. 889. ## 2 18 1726. 95.9 ## 3 28 4449. 159. ## 4 33 21984. 666. ## 5 32 3867. 121. We can create as many new columns as we want. health_insurance %&gt;% mutate(age_squared = age^2, age_cubed = age^3, age_fourth = age^4) %&gt;% head(5) ## # A tibble: 5 x 10 ## age sex bmi children smoker region charges age_squared age_cubed ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 19 fema~ 27.9 0 yes south~ 16885. 361 6859 ## 2 18 male 33.8 1 no south~ 1726. 324 5832 ## 3 28 male 33 3 no south~ 4449. 784 21952 ## 4 33 male 22.7 0 no north~ 21984. 1089 35937 ## 5 32 male 28.9 0 no north~ 3867. 1024 32768 ## # ... with 1 more variable: age_fourth &lt;dbl&gt; The CASE WHEN function is quite similar to SQL. For example, we can create a column which is 0 when age &lt; 50, 1 when 50 &lt;= age &lt;= 70, and 2 when age &gt; 70. health_insurance %&gt;% mutate(age_bucket = case_when(age &lt; 50 ~ 0, age &lt;= 70 ~ 1, age &gt; 70 ~ 2)) %&gt;% select(age, age_bucket) ## # A tibble: 1,338 x 2 ## age age_bucket ## &lt;dbl&gt; &lt;dbl&gt; ## 1 19 0 ## 2 18 0 ## 3 28 0 ## 4 33 0 ## 5 32 0 ## 6 31 0 ## 7 46 0 ## 8 37 0 ## 9 37 0 ## 10 60 1 ## # ... with 1,328 more rows SQL translation: SELECT CASE WHEN AGE &lt; 50 THEN 0 ELSE WHEN AGE &lt;= 70 THEN 1 ELSE 2 FROM health_insurance 6.3 Exercises Run this code on your computer to answer these exercises. The data actuary_salaries contains the salaries of actuaries collected from the DW Simpson survey. Use this data to answer the exercises below. actuary_salaries %&gt;% glimpse() ## Observations: 138 ## Variables: 6 ## $ industry &lt;chr&gt; &quot;Casualty&quot;, &quot;Casualty&quot;, &quot;Casualty&quot;, &quot;Casualty&quot;, &quot;C... ## $ exams &lt;chr&gt; &quot;1 Exam&quot;, &quot;2 Exams&quot;, &quot;3 Exams&quot;, &quot;4 Exams&quot;, &quot;1 Exam... ## $ experience &lt;dbl&gt; 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5,... ## $ salary &lt;chr&gt; &quot;48 - 65&quot;, &quot;50 - 71&quot;, &quot;54 - 77&quot;, &quot;58 - 82&quot;, &quot;54 - ... ## $ salary_low &lt;dbl&gt; 48, 50, 54, 58, 54, 57, 62, 63, 65, 70, 72, 85, 55... ## $ salary_high &lt;chr&gt; &quot;65&quot;, &quot;71&quot;, &quot;77&quot;, &quot;82&quot;, &quot;72&quot;, &quot;81&quot;, &quot;87&quot;, &quot;91&quot;, &quot;9... How many industries are represented? The salary_high column is a character type when it should be numeric. Change this column to numeric. What are the highest and lowest salaries for an actuary in Health with 5 exams passed? Create a new column called salary_mid which has the middle of the salary_low and salary_high columns. When grouping by industry, what is the highest salary_mid? What about salary_high? What is the lowest salary_low? There is a mistake when salary_low == 11. Find and fix this mistake, and then rerun the code from the previous task. Create a new column, called n_exams, which is an integer. Use 7 for ASA/ACAS and 10 for FSA/FCAS. Use the code below as a starting point and fill in the _ spaces Create a column called social_life, which is equal to n_exams/experience. What is the average (mean) social_life by industry? Bonus question: what is wrong with using this as a statistical measure? actuary_salaries &lt;- actuary_salaries %&gt;% mutate(n_exams = case_when(exams == &quot;FSA&quot; ~ _, exams == &quot;ASA&quot; ~ _, exams == &quot;FCAS&quot; ~ _, exams == &quot;ACAS&quot; ~ _, TRUE ~ as.numeric(substr(exams,_,_)))) Create a column called social_life, which is equal to n_exams/experience. What is the average (mean) social_life by industry? Bonus question: what is wrong with using this as a statistical measure? 6.4 Answers to exercises Answers to these exercises, along with a video tutorial, are available at ExamPA.net. "],
["visualization.html", " 7 Visualization 7.1 Create a plot object (ggplot) 7.2 Add a plot 7.3 Data manipulation chaining", " 7 Visualization This sections shows how to create and interpret simple graphs. In past exams, the SOA has provided code for any technical visualizations which are needed. 7.1 Create a plot object (ggplot) The first step is to create a blank canvas that holds the columns that are needed. Let’s say that the goal is to graph income and count. We put these into a ggplot object called p. The aesthetic argument, aes, means that the x-axis will have income and the y-axis will have count. library(Cairo) library(tidyverse) library(ExamPAData) theme_set(theme_bw()) p &lt;- insurance %&gt;% ggplot(aes(claims)) If we look at p, we see that it is nothing but white space with axis for count and income. p 7.2 Add a plot We add a histogram p + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Different plots are called “geoms” for “geometric objects”. Geometry = Geo (space) + metre (measure), and graphs measure data. For instance, instead of creating a histogram, we can draw a gamma distribution with stat_density. p + stat_density() Create an xy plot by adding and x and a y argument to aesthetic. insurance %&gt;% ggplot(aes(x = holders, y = claims)) + geom_point() 7.3 Data manipulation chaining Pipes allow for data manipulations to be chained with visualizations. termlife %&gt;% filter(FACE &gt; 0) %&gt;% mutate(INCOME_AGE_RATIO = INCOME/AGE) %&gt;% ggplot(aes(INCOME_AGE_RATIO, FACE)) + geom_point() + theme_bw() library(ggplot2) theme_set(theme_bw()) "],
["introduction-to-modeling.html", " 8 Introduction to Modeling 8.1 Model Notation 8.2 Ordinary least squares (OLS) 8.3 Example", " 8 Introduction to Modeling About 40-50% of the exam grade is based on modeling. 8.1 Model Notation The number of observations will be denoted by \\(n\\). When we refer to the size of a data set, we are referring to \\(n\\). We use \\(p\\) to refer the number of input variables used. The word “variables” is synonymous with “features”. For example, in the health_insurance data, the variables are age, sex, bmi, children, smoker and region. These 7 variables mean that \\(p = 7\\). The data is collected from 1,338 patients, which means that \\(n = 1,338\\). Scalar numbers are denoted by ordinary variables (i.e., \\(x = 2\\), \\(z = 4\\)), and vectors are denoted by bold-faced letters \\[\\mathbf{a} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{pmatrix}\\] We use \\(\\mathbf{y}\\) to denote the target variable. This is the variable which we are trying to predict. This can be either a whole number, in which case we are performing regression, or a category, in which case we are performing classification. In the health insurance example, y = charges, which are the annual health care costs for a patient. Both \\(n\\) and \\(p\\) are important because they tell us what types of models are likely to work well, and which methods are likely to fail. For the PA exam, we will be dealing with small \\(n\\) (&lt;100,000) due to the limitations of the Prometric computers. We will use a small \\(p\\) (&lt; 20) in order to make the data sets easier to interpret. We organize these variables into matrices. Take an example with \\(p\\) = 2 columns and 3 observations. The matrix is said to be \\(3 \\times 2\\) (read as “2-by-3”) matrix. \\[ \\mathbf{X} = \\begin{pmatrix}x_{11} &amp; x_{21}\\\\ x_{21} &amp; x_{22}\\\\ x_{31} &amp; x_{32} \\end{pmatrix} \\] The target is \\[\\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix}\\] This represents the unknown quantity that we want to be able to predict. In the health care costs example, \\(y_1\\) would be the costs of the first patient, \\(y_2\\) the costs of the second patient, and so forth. The variables \\(x_{11}\\) and \\(x_{12}\\) might represent the first patient’s age and sex respectively, where \\(x_{i1}\\) is the patient’s age, and \\(x_{i2} = 1\\) if the ith patient is male and 0 if female. Machine learning is about using \\(\\mathbf{X}\\) to predict \\(\\mathbf{y}\\). We call this “y-hat”, or simply the prediction. This is based on a function of the data \\(X\\). \\[\\mathbf{\\hat{y}} = f(\\mathbf{X}) = \\begin{pmatrix} \\hat{y_1} \\\\ \\hat{y_2} \\\\ \\hat{y_3} \\end{pmatrix}\\] This is almost never going to happen perfectly, and so there is always an error term, \\(\\mathbf{\\epsilon}\\). This can be made smaller, but is never exactly zero. \\[ \\mathbf{\\hat{y}} + \\mathbf{\\epsilon} = f(\\mathbf{X}) + \\mathbf{\\epsilon} \\] In other words, \\(\\epsilon = y - \\hat{y}\\). We call this the residual. When we predict a person’s health care costs, this is the difference between the predicted costs (which we had created the year before) and the actual costs that the patient experienced (of that current year). 8.2 Ordinary least squares (OLS) The type of model used refers to the class of function of \\(f\\). If \\(f\\) is linear, then we are using a linear model. If \\(f\\) is non-parametric (does not have input parameters), then it is non-parametric modeling. Linear models are linear in the parameters, \\(\\beta\\). We have the data \\(\\mathbf{X}\\) and the target \\(\\mathbf{y}\\), where all of the y’s are real numbers, or \\(y_i \\in \\mathbb{R}\\). We want to find a \\(\\mathbf{\\beta}\\) so that \\[ \\mathbf{\\hat{y}} = \\mathbf{X} \\mathbf{\\beta} \\] Which means that each \\(y_i\\) is a linear combination of the variables \\(x_1, ..., x_p\\), plus a constant \\(\\beta_0\\) which is called the intercept term. \\[ y_i = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p \\] In the one-dimensional case, this creates a line connecting the points. In higher dimensions, this creates a hyperplane. The question then is how can we choose the best values of \\(\\beta?\\) First of all, we need to define what we mean by “best”. Ideally, we will choose these values which will create close predictions of \\(\\mathbf{y}\\) on new, unseen data. To solve for \\(\\mathbf{\\beta}\\), we first need to define a loss function. This allows us to compare how well a model is fitting the data. The most commonly used loss function is the residual sum of squares (RSS), also called the squared error loss or the L2 norm. When RSS is small, then the predictions are close to the actual values and the model is a good fit. When RSS is large, the model is a poor fit. \\[ \\text{RSS} = \\sum_i(y_i - \\hat{y})^2 \\] When you replace \\(\\hat{y_i}\\) in the above equation with \\(\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p\\), take the derivative with respect to \\(\\beta\\), set equal to zero, and solve, we can find the optimal values. This turns the problem of statistics into a problem of numeric optimization, which computers can do quickly. You might be asking: why does this need to be the squared error? Why not the absolute error, or the cubed error? Technically, these could be used as well. In fact, the absolute error (L1 norm) is useful in other models. Taking the square has a number of advantages. It provides the same solution if we assume that the distribution of \\(\\mathbf{Y}|\\mathbf{X}\\) is guassian and maximize the likelihood function. This method is used for GLMs, in the next chapter. Empirically it has been shown to be less likely to overfit as compared to other loss functions 8.3 Example In our health, we can create a linear model using bmi, age, and sex as an inputs. The formula controls which variables are included. There are a few shortcuts for using R formulas. Formula Meaning charges ~ bmi + age Use age and bmi to predict charges charges ~ bmi + age + bmi*age Use age,bmi as well as an interaction to predict charges charges ~ (bmi &gt; 20) + age Use an indicator variable for bmi &gt; 20 age to predict charges log(charges) ~ log(bmi) + log(age) Use the logs of age and bmi to predict log(charges) charges ~ . Use all variables to predict charges You can use formulas to create new variables (aka feature engineering). This can save you from needing to re-run code to create data. Below we fit a simple linear model to predict charges. library(ExamPAData) library(tidyverse) model &lt;- lm(data = health_insurance, formula = charges ~ bmi + age) The summary function gives details about the model. First, the Estimate, gives you the coefficients. The Std. Error is the error of the estimate for the coefficient. Higher standard error means greater uncertainty. This is relative to the average value of that variable. The t value tells you how “big” this error really is based on standard deviations. A larger t value implies a low probability of the null hypothesis being rejected saying that the coefficient is zero. This is the same as having a p-value (Pr (&gt;|t|))) being close to zero. The little *, **, *** indicate that the variable is either somewhat significant, significant, or highly significant. “significance” here means that there is a low probability of the coefficient being that size if there were no actual casual relationship, or if the data was random noise. summary(model) ## ## Call: ## lm(formula = charges ~ bmi + age, data = health_insurance) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14457 -7045 -5136 7211 48022 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6424.80 1744.09 -3.684 0.000239 *** ## bmi 332.97 51.37 6.481 1.28e-10 *** ## age 241.93 22.30 10.850 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11390 on 1335 degrees of freedom ## Multiple R-squared: 0.1172, Adjusted R-squared: 0.1159 ## F-statistic: 88.6 on 2 and 1335 DF, p-value: &lt; 2.2e-16 When evaluating model performance, you should not rely on the summary alone as this is based on the training data. To look at performance, test the model on validation data. This can be done by a) using a hold out set, or b) using cross-validation, which is even better. Let’s create an 80% training set and 20% testing set. You don’t need to worry about understanding this code as the exam will always give this to you. library(caret) #create a train/test split index &lt;- createDataPartition(y = health_insurance$charges, p = 0.8, list = F) %&gt;% as.numeric() train &lt;- health_insurance %&gt;% slice(index) test &lt;- health_insurance %&gt;% slice(-index) Train the model on the train and test on test. model &lt;- lm(data = train, formula = charges ~ bmi + age) pred = predict(model, test) Let’s look at the Root Mean Squared Error (RMSE). get_rmse &lt;- function(y, y_hat){ sqrt(mean((y - y_hat)^2)) } get_rmse(pred, test$charges) ## [1] 10642.78 The above number does not tell us if this is a good model or not by itself. We need a comparison. The fastest check is to compare against a prediction of the mean. In other words, all values of the y_hat are the average of charges get_rmse(mean(test$charges), test$charges) ## [1] 11136.87 The RMSE is higher (worse) when using just the mean, which is what we expect. If you ever fit a model and get an error which is worse than the average prediction, something must be wrong. The next test is to see if any assumptions have been violated. First, is there a pattern in the residuals? If there is, this means that the model is missing key information. For the model below, this is a yes, which means that this is a bad model. Because this is just for illustration, I’m going to continue using it, however. plot(model, which = 1) Figure 8.1: Residuals vs. Fitted The normal QQ shows how well the quantiles of the predictions fit to a theoretical normal distribution. If this is true, then the graph is a straight 45-degree line. In this model, you can definitely see that this is not the case. If this were a good model, this distribution would be closer to normal. plot(model, which = 2) Figure 8.2: Normal Q-Q Caution: The normal-QQ plot is useless when the response family is not normal. This includes Gamma, Inverse Gaussian, etc, as well as count data (Poisson Regression) or binary (Logistic Regression or other model with a binomial response family. There was a mistake in the June PA Exam which said that the QQ-plot always applies. See https://www.reddit.com/r/actuary/comments/dvtb85/mistake_in_the_june_exam_pa/ The below is from an excellent post of Stack Exchange. R does not have a distinct plot.glm() method. When you fit a model with glm() and run plot(), it calls ?plot.lm, which is appropriate for linear models (i.e., with a normally distributed error term). More specifically, the plots will often ‘look funny’ and lead people to believe that there is something wrong with the model when it is perfectly fine. We can see this by looking at those plots with a couple of simple simulations where we know the model is correct: Once you have chosen your model, you should re-train over the entire data set. This is to make the coefficients more stable because n is larger. Below you can see that the standard error is lower after training over the entire data set. all_data &lt;- lm(data = health_insurance, formula = charges ~ bmi + age) testing &lt;- lm(data = test, formula = charges ~ bmi + age) term full_data_std_error test_data_std_error (Intercept) 1744.1 3845.1 bmi 51.4 112.2 age 22.3 46.5 All interpretations should be based on the model which was trained on the entire data set. Obviously, this only makes a difference if you are interpreting the precise values of the coefficients. If you are just looking at which variables are included, or at the size and sign of the coefficients, then this would not change. coefficients(model) ## (Intercept) bmi age ## -6471.5710 318.0462 257.3905 Translating the above into an equation we have \\[\\hat{y_i} = -6,424.80 + 332.97 \\space\\text{bmi} + 241.93\\space \\text{age}\\] For example, if a patient has bmi = 27.9 and age = 19 then predicted value is \\[\\hat{y_1} = -6,424.80 + (332.97)(27.9) + (241.93)(19) = 7,461.73\\] This model structure implies that each of the variables \\(\\mathbf{x_1}, ..., \\mathbf{x_p}\\) each change the predicted \\(\\mathbf{\\hat{y}}\\). If \\(x_{ij}\\) increases by one unit, then \\(y_i\\) increases by \\(\\beta_j\\) units, regardless of what happens to all of the other variables. This is one of the main assumptions of linear models: variable indepdendence. If the variables are correlated, say, then this assumption will be violated. Readings ISLR 2.1 What is statistical learning? ISLR 2.2 Assessing model accuracy "],
["generalized-linear-models-glms.html", " 9 Generalized linear models (GLMs) 9.1 Model form 9.2 Example 9.3 Reference levels 9.4 Interactions", " 9 Generalized linear models (GLMs) 9.1 Model form Instead of the model being a direct linear combination of the variables, there is an intermediate step called a link function \\(g\\). \\[ g(\\mathbf{\\hat{y}}) = \\mathbf{X} \\mathbf{\\beta} \\] This implies that the response \\(\\mathbf{y}\\) is related to the linear predictor \\(\\mathbf{X} \\mathbf{\\beta}\\) through the inverse link function. \\[ \\mathbf{\\hat{y}} = g^-1(\\mathbf{X} \\mathbf{\\beta}) \\] This means that \\(g(.)\\) must be an invertable. For example, if \\(g\\) is the natural logarithm (aka, the “log-link”), then \\[ log(\\mathbf{\\hat{y}}) = \\mathbf{X} \\mathbf{\\beta} \\Rightarrow \\mathbf{\\hat{y}} = e^{\\mathbf{X} \\mathbf{\\beta}} \\] This is useful when the distribution of \\(Y\\) is skewed, as taking the log corrects skewness. Figure 9.1: Taking the log corrects for skewness You might be asking, what if the distribution of \\(Y\\) is not normal, no matter what choice we have for \\(g\\)? The short answer is that we can change our assumption of the distribution of \\(Y\\), and use this to change the parameters. If you have taken exam STAM then you are familiar with maximum likelihood estimation. We have a response \\(\\mathbf{Y}\\), and we fit a distribution to \\(\\mathbf{Y} | \\mathbf{X}\\). This is the target variable conditioned on the data. For each \\(y_i\\), each observation, we assign a probability \\(f_Y(y_i)\\) \\[ f_y(y_i | X_1 = x_1, X_2 = x_2, ..., X_p = x_p) = Pr(Y = y_i | \\mathbf{X}) \\] Now, when we choose the response family, we are simply changing \\(f_Y\\). If we say that the response family is Gaussian, then \\(f\\) has a Gaussian PDF. If we are modeling counts, then \\(f\\) is a Poisson PDF. This only works if \\(f\\) is in the exponential family of distributions, which consists of the common names such as Gaussian, Binomial, Gamma, Inverse Gamma, and so forth. Reading the CAS Monograph 5 will provide more detail into this. The possible combinations of link functions and distribution families are summarized nicely on Wikipedia. Figure 9.2: Distribution-Link Function Combinations For this exam, a common question is to ask candiates to choose the best distribution and link function. There is no all-encompasing answer, but a few suggestions are If \\(Y\\) is counting something, such as the number of claims, number of accidents, or some other discrete and positive counting sequence, use the Poisson; If \\(Y\\) contains negative values, then do not use the Exponential, Gamma, or Inverse Gaussian as these are strictly positive. Conversely, if \\(Y\\) is only positive, such as the price of a policy (price is always &gt; 0), or the claim costs, then these are good choices; If \\(Y\\) is binary, the the binomial response with either a Probit or Logit link. The Logit is more common. If \\(Y\\) has more than two categories, the multinomial distribution with either the Probit or Logic link (See Logistic Regression) The exam will always ask you to interpret the GLM. These questions can usually be answered by inverting the link function and interpreting the coefficients. In the case of the log link, simply take the exponent of the coefficients and each of these represents a “relativity” factor. \\[ log(\\mathbf{\\hat{y}}) = \\mathbf{X} \\mathbf{\\beta} \\Rightarrow \\mathbf{\\hat{y}} = e^{\\mathbf{X} \\mathbf{\\beta}} \\] For a single observation \\(y_i\\), this is \\[ \\text{exp}(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip}) = \\\\ e^{\\beta_0} e^{\\beta_1 x_{i1}}e^{\\beta_2 x_{i2}} ... e^{\\beta_p x_{ip}} = R_0 R_2 R_3 ... R_{p} \\] Where \\(R_k\\) is the relativity of the kth variable. This terminology is from insurance ratemaking, where actuaries need to be able to explain the impact of each variable in pricing insurance. The data science community does not use this language. For binary outcomes with logit or probit link, there is no easy interpretation. This has come up in at least one past sample exam, and the solution was to create “psuedo” observations and observe how changing each \\(x_k\\) would change the predicted value. Due to the time requirements, this is unlikely to come up on an exam. So if you are asked to use a logit or probit link, saying that the result is not easy to interpret should suffice. 9.2 Example Just as with OLS, there is a formula and data argument. In addition, we need to specify the response distribution and link function. model = glm(formula = charges ~ age + sex + children, family = gaussian(link = &quot;log&quot;), data = health_insurance) We see that age, sex, and children are all significant (p &lt;0.01). Reading off the coefficient signs, we see that claims Increase as age increases Are higher for men Are slightly higher for patients wich children model %&gt;% tidy() ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 8.55 0.0953 89.7 0. ## 2 age 0.0201 0.00179 11.3 3.12e-28 ## 3 sexmale 0.112 0.0459 2.44 1.49e- 2 ## 4 children 0.0489 0.0182 2.69 7.29e- 3 9.3 Reference levels When a categorical variable is used in a GLM, the model actually uses indicator variables for each level. The default reference level is the order of the R factors. For the sex variable, the order is female and then male. This means that the base level is female by default. health_insurance$sex %&gt;% as.factor() %&gt;% levels() ## [1] &quot;female&quot; &quot;male&quot; Why does this matter? Statistically, the coefficients are most stable when there are more observations. health_insurance$sex %&gt;% as.factor() %&gt;% summary() ## female male ## 662 676 There is already a function to do this in the tidyverse called fct_infreq. Let’s quickly fix the sex column so that these factor levels are in order of frequency. health_insurance &lt;- health_insurance %&gt;% mutate(sex = fct_infreq(sex)) Now male is the base level. health_insurance$sex %&gt;% as.factor() %&gt;% levels() ## [1] &quot;male&quot; &quot;female&quot; 9.4 Interactions An interaction occurs when the effect of a variable on the response is different depending on the level of other variables in the model. Consider this model: Let \\(x_2\\) be an indicator variable, which is 1 for some records and 0 otherwise. \\[\\hat{y_i} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2\\] There are now two different linear models dependong on whether x_1 is 0 or 1. When \\(x_1 = 0\\), \\[\\hat{y_i} = \\beta_0 + \\beta_2 x_2\\] and when \\(x_1 = 1\\) \\[\\hat{y_i} = \\beta_0 + \\beta_1 + \\beta_2 x_2 + \\beta_3 x_2\\] By rewriting this we can see that the intercept changes from \\(\\beta_0\\) to \\(\\beta_0^*\\) and the slope changes from \\(\\beta_1\\) to \\(\\beta_1^*\\) \\[ (\\beta_0 + \\beta_1) + (\\beta_2 + \\beta_3 ) x_2 \\\\ = \\beta_0^* + \\beta_1^* x_2 \\] The SOA’s modules give an example with the using age and gender as below. This is not a very strong interaction, as the slopes are almost identical across gender. interactions %&gt;% ggplot(aes(age, actual, color = gender)) + geom_line() + labs(title = &quot;Age vs. Actual by Gender&quot;, subtitle = &quot;Interactions imply different slopes&quot;, caption= &quot;data: interactions&quot;) Figure 9.3: Example of weak interaction Here is a clearer example from the auto_claim data. The lines show the slope of a linear model, assuming that only BLUEBOOK and CAR_TYPE were predictors in the model. You can see that the slope for Sedans and Sports Cars is higher than for Vans and Panel Trucks. auto_claim %&gt;% ggplot(aes(log(CLM_AMT), log(BLUEBOOK), color = CAR_TYPE)) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, se = F) + labs(title = &quot;Kelly Bluebook Value vs Claim Amount&quot;) Figure 9.4: Example of strong interaction Any time that the effect that one variable has on the response is different depending on the value of other variables we say that there is an interaction. We can also use an hypothesis test with a GLM to check this. Simply include an interaction term and see if the coefficient is zero at the desired significance level. 9.4.1 Poisson Regression When the dependent variable is a count, such as the number of claims per month, Poisson regression is appropriate. This requires that each claim is independent in that one claim will not make another claim more or less likely. This means that the target variable is actually a rate, \\(\\frac{\\text{claims}}{\\text{months}}\\). More generally, we call the months the exposure. Let \\(m_i\\) by the units of exposure and \\(y_i\\) the target. We use a log-link function to correct for skewness. \\[ log(\\frac{\\hat{y_i}}{m_i}) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p\\] By using the fact that \\(log(\\frac{a}{b}) = log(a) - log(b)\\) this turns into \\[log(\\hat{y_i}) = log(m_i) + \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p\\] We call the \\(log(m_i)\\) the offset term. Notice that there is no coefficient (beta) on this value, because we already know what the impact will be. In R, the code for this equation would be glm(y ~ offset(log(m)) + x, family=poisson(link=log) ) 9.4.2 Tweedie regression While this topic is briefly mentioned on the modules, the only R libraries which support Tweedie Regression (statmod and tweedie) are not on the syllabus, and so there is no way that the SOA could ask you to build a tweedie model. This means that you can be safely skip this section. 9.4.3 Stepwise subset selection In theory, we could test all possible combinations of variables and interaction terms. This includes all \\(p\\) models with one predictor, all p-choose-2 models with two predictors, all p-choose-3 models with three predictors, and so forth. Then we take whichever model has the best performance as the final model. This “brute force” approach is statistically ineffective: the more variables which are searched, the higher the chance of finding models that overfit. A subtler method, known as stepwise selection, reduces the chances of overfitting by only looking at the most promising models. Forward Stepwise Selection: Start with no predictors in the model; Evaluate all \\(p\\) models which use only one predictor and choose the one with the best performance (highest \\(R^2\\) or lowest \\(\\text{RSS}\\)); Repeat the process when adding one additional predictor, and continue until there is a model with one predictor, a model with two predictors, a model with three predictors, and so forth until there are \\(p\\) models; Select the single best model which has the best \\(\\text{AIC}\\),\\(\\text{BIC}\\), or adjusted \\(R^2\\). Backward Stepwise Selection: Start with a model that contains all predictors; Create a model which removes all predictors; Choose the best model which removes all-but-one predictor; Choose the best model which removes all-but-two predictors; Continue until there are \\(p\\) models; Select the single best model which has the best \\(\\text{AIC}\\),\\(\\text{BIC}\\), or adjusted \\(R^2\\). Both Forward &amp; Backward Selection: A hybrid approach is to consider use both forward and backward selection. This is done by creating two lists of variables at each step, one from forward and one from backward selection. Then variables from both lists are tested to see if adding or subtracting from the current model would improve the fit or not. ISLR does not mention this directly, however, by default the stepAIC function uses a default of both. Tip: Always load the MASS library before dplyr or tidyverse. Otherwise there will be conflicts as there are functions named select() and filter() in both. Alternatively, specify the library in the function call with dplyr::select(). Readings CAS Monograph 5 Chapter 2 9.4.4 Advantages and disadvantages There is usually at least one question on the PA exam which asks you to “list some of the advantages and disadvantages of using this particular model”, and so here is one such list. It is unlikely that the grader will take off points for including too many comments and so a good strategy is to include everything that comes to mind. GLM Advantages Easy to interpret Can easily be deployed in spreadsheet format Handles skewed data through different response distributions Models the average response which leads to stable predictions on new data Handles continuous and categorical data Works well on small data sets GLM Disadvantages Does not select features (without stepwise selection) Strict assumptions around distribution shape, randomness of error terms, and variable correlations Unable to detect non-linearity directly (although this can manually be addressed through feature engineering) Sensitive to outliers Low predictive power "],
["logistic-regression.html", " 10 Logistic Regression 10.1 Model form 10.2 Example 10.3 Classification metrics", " 10 Logistic Regression 10.1 Model form Logistic regression is a special type of GLM. The name is confusing because the objective is classification and not regression. While most examples focus on binary classification, logistic regression also works for multiclass classification. The model form is as before \\[g(\\mathbf{\\hat{y}}) = \\mathbf{X} \\mathbf{\\beta}\\] However, now the target \\(y_i\\) is a category. Our objective is to predict a probability of being in each category. For regression, \\(\\hat{y_i}\\) can be any number, but now we need \\(0 \\leq \\hat{y_i} \\leq 1\\). We can use a special link function, known as the standard logistic function, sigmoid, or logit, to force the output to be in this range of \\(\\{0,1\\}\\). \\[\\mathbf{\\hat{y}} = g^{-1}(\\mathbf{X} \\mathbf{\\beta}) = \\frac{1}{1 + e^{-\\mathbf{X} \\mathbf{\\beta}}}\\] Figure 10.1: Standard Logistic Function Other link functions for classification problems are possible as well, although the logistic function is the most common. If a problem asks for an alternative link, such as the probit, fit both models and compare the performance. 10.2 Example Using the auto_claim data, we predict whether or not a policy has a claim. This is also known as the claim frequency. auto_claim %&gt;% count(CLM_FLAG) ## # A tibble: 2 x 2 ## CLM_FLAG n ## &lt;chr&gt; &lt;int&gt; ## 1 No 7556 ## 2 Yes 2740 About 40% do not have a claim while 60% have at least one claim. set.seed(42) index &lt;- createDataPartition(y = auto_claim$CLM_FLAG, p = 0.8, list = F) %&gt;% as.numeric() auto_claim &lt;- auto_claim %&gt;% mutate(target = as.factor(ifelse(CLM_FLAG == &quot;Yes&quot;, 1,0))) train &lt;- auto_claim %&gt;% slice(index) test &lt;- auto_claim %&gt;% slice(-index) frequency &lt;- glm(target ~ AGE + GENDER + MARRIED + CAR_USE + BLUEBOOK + CAR_TYPE + AREA, data=train, family = binomial(link=&quot;logit&quot;)) All of the variables except for the CAR_TYPE are highly significant. The car types SPORTS CAR and SUV appear to be significant, and so if we wanted to make the model simpler we could create indicator variables for CAR_TYPE == SPORTS CAR and CAR_TYPE == SUV. frequency %&gt;% summary() ## ## Call: ## glm(formula = target ~ AGE + GENDER + MARRIED + CAR_USE + BLUEBOOK + ## CAR_TYPE + AREA, family = binomial(link = &quot;logit&quot;), data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8431 -0.8077 -0.5331 0.9575 3.0441 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.523e-01 2.517e-01 -1.400 0.16160 ## AGE -2.289e-02 3.223e-03 -7.102 1.23e-12 *** ## GENDERM -1.124e-02 9.304e-02 -0.121 0.90383 ## MARRIEDYes -6.028e-01 5.445e-02 -11.071 &lt; 2e-16 *** ## CAR_USEPrivate -1.008e+00 6.569e-02 -15.350 &lt; 2e-16 *** ## BLUEBOOK -4.025e-05 4.699e-06 -8.564 &lt; 2e-16 *** ## CAR_TYPEPickup -6.687e-02 1.390e-01 -0.481 0.63048 ## CAR_TYPESedan -3.689e-01 1.383e-01 -2.667 0.00765 ** ## CAR_TYPESports Car 6.159e-01 1.891e-01 3.256 0.00113 ** ## CAR_TYPESUV 2.982e-01 1.772e-01 1.683 0.09240 . ## CAR_TYPEVan -8.983e-03 1.319e-01 -0.068 0.94569 ## AREAUrban 2.128e+00 1.064e-01 19.993 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 9544.3 on 8236 degrees of freedom ## Residual deviance: 8309.6 on 8225 degrees of freedom ## AIC: 8333.6 ## ## Number of Fisher Scoring iterations: 5 There is no easy way of interpreting the coefficients when using a logit link function. The most inference that we can make is to note which variables are significant. CAR_USE, MARRIED, BLUEBOOK are highly significant Certain values of CAR_TYPE are significant but others are not. The output is a predicted probability. We can see that this is centered around a probability of about 0.5. preds &lt;- predict(frequency, newdat=test,type=&quot;response&quot;) qplot(preds) Figure 10.2: Distribution of Predicted Probability In order to convert these values to predicted 0’s and 1’s, we assign a cutoff value so that if \\(\\hat{y}\\) is above this threshold we use a 1 and 0 othersise. The default cutoff is 0.5. We change this to 0.3 and see that there are 763 policies predicted to have claims. test &lt;- test %&gt;% mutate(pred_zero_one = as.factor(1*(preds&gt;.3))) summary(test$pred_zero_one) ## 0 1 ## 1296 763 How do we decide on this cutoff value? We need to compare cutoff values based on some evaluation metric. For example, we can use accuracy. \\[\\text{Accuracy} = \\frac{\\text{Correct Guesses}}{\\text{Total Guesses}}\\] This results in an accuracy of 70%. But is this good? test %&gt;% summarise(accuracy = mean(pred_zero_one == target)) ## # A tibble: 1 x 1 ## accuracy ## &lt;dbl&gt; ## 1 0.699 Consider what would happen if we just predicted all 0’s. The accuracy is 74%. test %&gt;% summarise(accuracy = mean(0 == target)) ## # A tibble: 1 x 1 ## accuracy ## &lt;dbl&gt; ## 1 0.734 For policies which experience claims the accuracy is 63%. test %&gt;% filter(target == 1) %&gt;% summarise(accuracy = mean(pred_zero_one == target)) ## # A tibble: 1 x 1 ## accuracy ## &lt;dbl&gt; ## 1 0.631 But for policies that don’t actually experience claims this is 72%. test %&gt;% filter(target == 0) %&gt;% summarise(accuracy = mean(pred_zero_one == target)) ## # A tibble: 1 x 1 ## accuracy ## &lt;dbl&gt; ## 1 0.724 How do we know if this is a good model? We can repeat this process with a different cutoff value and get different accuracy metrics for these groups. Let’s use a cutoff of 0.6. 75% test &lt;- test %&gt;% mutate(pred_zero_one = as.factor(1*(preds&gt;.6))) test %&gt;% summarise(accuracy = mean(pred_zero_one == target)) ## # A tibble: 1 x 1 ## accuracy ## &lt;dbl&gt; ## 1 0.752 10% for policies with claims and 98% for policies without claims. test %&gt;% filter(target == 1) %&gt;% summarise(accuracy = mean(pred_zero_one == target)) ## # A tibble: 1 x 1 ## accuracy ## &lt;dbl&gt; ## 1 0.108 test %&gt;% filter(target == 0) %&gt;% summarise(accuracy = mean(pred_zero_one == target)) ## # A tibble: 1 x 1 ## accuracy ## &lt;dbl&gt; ## 1 0.985 The punchline is that the accuracy depends on the cutoff value, and changing the cutoff value changes whether the model is accuracy for the positive classes (policies with actual claims) vs. the negative classes (policies without claims). 10.3 Classification metrics For regression problems, when the output is a whole number, we can use the sum of squares \\(\\text{RSS}\\), the r-squared \\(R^2\\), the mean absolute error \\(\\text{MAE}\\), and the likelihood. For classification problems where the output is in \\(\\{0,1\\}\\), we need to a new set of metrics. A confusion matrix shows is a table that summarises how the model classifies each group. No claims and predicted to not have claims - True Negatives (TN) = 1,489 Had claims and predicted to have claims - True Positives (TP) = 59 No claims but predited to have claims - False Negatives (FN) = 22 Had claims but predicted not to - False Positives (FP) = 489 confusionMatrix(test$pred_zero_one,factor(test$target))$table ## Reference ## Prediction 0 1 ## 0 1489 489 ## 1 22 59 These definitions allow us to measure performance on the different groups. Precision answers the question “out of all of the positive predictions, what percentage were correct?” \\[\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\\] Recall answers the question “out of all of positive examples in the data set, what percentage were correct?” \\[\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\] The choice of using precision or recall depends on the relative cost of making a FP or a FN error. If FP errors are expensive, then use precision; if FN errors are expensive, then use recall. Example A: the model trying to detect a deadly disease, which only 1 out of every 1000 patient’s survive without early detection. Then the goal should be to optimize recall, because we would want every patient that has the disease to get detected. Example B: the model is detecting which emails are spam or not. If an important email is flagged as spam incorrectly, the cost is 5 hours of lost productivity. In this case, precision is the main concern. In some cases we can compare this “cost” in actual values. For example, if a federal court is predicting if a criminal will recommit or not, they can agree that “1 out of every 20 guilty individuals going free” in exchange for “90% of those who are guilty being convicted”. When money is involed, this a dollar amount can be used: flagging non-spam as spam may cost $100 whereas missing a spam email may cost $2. Then the cost-weighted accuracy is \\[\\text{Cost} = (100)(\\text{FN}) + (2)(\\text{FP})\\] Then the cutoff value can be tuned in order to find the minimum cost. Fortunately, all of this is handled in a single function called confusionMatrix. confusionMatrix(test$pred_zero_one,factor(test$target)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 1489 489 ## 1 22 59 ## ## Accuracy : 0.7518 ## 95% CI : (0.7326, 0.7704) ## No Information Rate : 0.7339 ## P-Value [Acc &gt; NIR] : 0.03366 ## ## Kappa : 0.1278 ## ## Mcnemar&#39;s Test P-Value : &lt; 2e-16 ## ## Sensitivity : 0.9854 ## Specificity : 0.1077 ## Pos Pred Value : 0.7528 ## Neg Pred Value : 0.7284 ## Prevalence : 0.7339 ## Detection Rate : 0.7232 ## Detection Prevalence : 0.9607 ## Balanced Accuracy : 0.5466 ## ## &#39;Positive&#39; Class : 0 ## 10.3.1 Area Under the ROC Curv (AUC) What if we look at both the true-positive rate (TPR) and false positive rate (FPR) simultaneously? That is, for each value of the cutoff, we can calculate the TPR and TNR. For example, say that we have 10 cutoff values, \\(\\{k_1, k_2, ..., k_{10}\\}\\). Then for each value of \\(k\\) we calculate both the true positive rates \\[\\text{TPR} = \\{\\text{TPR}(k_1), \\text{TPR}(k_2), .., \\text{TPR}(k_{10})\\} \\] and the true negative rates \\[\\{\\text{FNR} = \\{\\text{FNR}(k_1), \\text{FNR}(k_2), .., \\text{FNR}(k_{10})\\}\\] Then we set x = TPR and y = FNR and graph x against y. The plot below shows the ROC for the auto_claims data. The Area Under the Curv of 0.6795 is what we would get if we integrated under the curve. library(pROC) roc(test$target, preds, plot = T) Figure 10.3: AUC for auto_claim ## ## Call: ## roc.default(response = test$target, predictor = preds, plot = T) ## ## Data: preds in 1511 controls (test$target 0) &lt; 548 cases (test$target 1). ## Area under the curve: 0.7558 If we just randomly guess, the AUC would be 0.5, which is represented by the 45-degree line. A perfect model would maximize the curve to the upper-left corner. "],
["penalized-linear-models.html", " 11 Penalized Linear Models 11.1 Ridge Regression 11.2 Lasso 11.3 Elastic Net 11.4 Advantages and disadvantages", " 11 Penalized Linear Models One of the main weaknesses of the GLM, including all linear models in this chapter, is that the features need to be selected by hand. Stepwise selection helps to improve this process, but fails when the inputs are correlated and often has a strong dependence on seemingly arbitrary choices of evaluation metrics such as using AIC or BIC and forward or backwise directions. The Bias Variance Tradoff is about finding the lowest error by changing the flexibility of the model. Penalization methods use a parameter to control for this flexibility directly. Earlier on we said that the linear model minimizes the sum of square terms, known as the residual sum of squares (RSS) \\[ \\text{RSS} = \\sum_i(y_i - \\hat{y})^2 = \\sum_i(y_i - \\beta_0 - \\sum_{j = 1}^p\\beta_j x_{ij})^2 \\] This loss function can be modified so that models which include more (and larger) coefficients are considered as worse. In other words, when there are more \\(\\beta\\)’s, or \\(\\beta\\)’s which are larger, the RSS is higher. 11.1 Ridge Regression Ridge regression adds a penalty term which is proportional to the square of the sum of the coefficients. This is known as the “L2” norm. \\[ \\sum_i(y_i - \\beta_0 - \\sum_{j = 1}^p\\beta_j x_{ij})^2 + \\lambda \\sum_{j = 1}^p\\beta_j^2 \\] This \\(\\lambda\\) controls how much of a penalty is imposed on the size of the coefficients. When \\(\\lambda\\) is high, simpler models are treated more favorably because the \\(\\sum_{j = 1}^p\\beta_j^2\\) carries more weight. Conversely, then \\(\\lambda\\) is low, complex models are more favored. When \\(\\lambda = 0\\), we have an ordinary GLM. 11.2 Lasso The official name is the Least Absolute Shrinkage and Selection Operator, but the common name is just “the lasso”. Just as with Ridge regression, we want to favor simpler models; however, we also want to select variables. This is the same as forcing some coefficients to be equal to 0. Instead of taking the square of the coefficients (L2 norm), we take the absolute value (L1 norm). \\[ \\sum_i(y_i - \\beta_0 - \\sum_{j = 1}^p\\beta_j x_{ij})^2 + \\lambda \\sum_{j = 1}^p|\\beta_j| \\] In ISLR, Hastie et al show that this results in coefficients being forced to be exactly 0. This is extremely useful because it means that by changing \\(\\lambda\\), we can select how many variables to use in the model. Note: While any response family is possible with penalized regression, in R, only the Gaussian family is possible in the library glmnet, and so this is the only type of question that the SOA can ask. 11.3 Elastic Net The Elastic Net uses a penalty term which is between the L1 and L2 norms. The penalty term is a weighted average using the mixing parameter \\(0 \\leq \\alpha \\leq 1\\). The loss fucntion is then \\[\\text{RSS} + (1 - \\alpha)/2 \\sum_{j = 1}^{p}\\beta_j^2 + \\alpha \\sum_{j = 1}^p |\\beta_j|\\] When \\(\\alpha = 1\\) is turns into a Lasso; when \\(\\alpha = 1\\) this is the Ridge model. Luckily, none of this needs to be memorized. On the exam, read the documentation in R to refresh your memory. For the Elastic Net, the function is glmnet, and so running ?glmnet will give you this info. Shortcut: When using complicated functions on the exam, use ?function_name to get the documentation. 11.4 Advantages and disadvantages Elastic Net/Lasso/Ridge Advantages All benefits from GLMS Automatic variable selection for Lasso; smaller coefficients for Ridge Better predictive power than GLM Elastic Net/Lasso/Ridge Disadvantages All cons of GLMs Readings ISLR 6.1 Subset Selection ISLR 6.2 Shrinkage Methods "],
["tree-based-models.html", " 12 Tree-based models 12.1 Decision Trees 12.2 Ensemble learning 12.3 Random Forests 12.4 Gradient Boosted Trees 12.5 Exercises 12.6 Answers to Exercises", " 12 Tree-based models 12.1 Decision Trees 12.1.1 Model form Decision trees can be used for either classification or regression problems. The model structure is a series of yes/no questions. Depending on how each observation answers these questions, a prediction is made. The below example shows how a single tree can predict health claims. For non-smokers, the predicted annual claims are 8,434. This represents 80% of the observations For smokers with a bmi of less than 30, the predicted annual claims are 21,000. 10% of patients fall into this bucket. For smokers with a bmi of more than 30, the prediction is 42,000. This bucket accounts for 11% of patients. We can cut the data set up into these groups and look at the claim costs. From this grouping, we can see that smoker is the most important variable as the difference in average claims is about 20,000. smoker bmi_30 mean_claims percent no bmi &lt; 30 $7,977.03 0.38 no bmi &gt;= 30 $8,842.69 0.42 yes bmi &lt; 30 $21,363.22 0.10 yes bmi &gt;= 30 $41,557.99 0.11 This was a very simple example because there were only two variables. If we have more variables, the tree will get large very quickly. This will result in overfitting; there will be good performance on the training data but poor performance on the test data. The step-by-step process of building a tree is Step 1: Choose a variable at random. This could be any variable in age, children, charges, sex, smoker, age_bucket, bmi, or region. Step 2: Find the split point which best seperates observations out based on the value of \\(y\\). A good split is one where the \\(y\\)’s are very different. * In this case, smoker was chosen. Then we can only split this in one way: smoker = 1 or smoker = 0. Then for each of these groups, smokers and non-smokers, choose another variable at random. In this case, for no-smokers, age was chosen. To find the best cut point of age, look at all possible age cut points from 18, 19, 20, 21, …, 64 and choose the one which best separates the data. There are three ways of deciding where to split Entropy (aka, information gain) Gini Classification error Of these, only the first two are commonly used. The exam is not going to ask you to calculate either of these. Just know that neither method will work better on all data sets, and so the best practice is to test both and compare the performance. Step 3: Continue doing this until a stopping criteria is reached. For example, the minimum number of observations is 5 or less. As you can see, this results in a very deep tree. tree &lt;- rpart(formula = charges ~ ., data = health_insurance, control = rpart.control(cp = 0.003)) rpart.plot(tree, type = 3) Step 4: Apply cost comlexity pruning to simplify the tree Intuitively, we know that the above model would perform poorly due to overfitting. We want to make it simpler by removing nodes. This is very similar to how in linear models we reduce complexity by reducing the number of coefficients. A measure of the depth of the tree is the complexity. A simple way of measuring this from the number of terminal nodes, called \\(|T|\\). This is similar to the “degrees of freedom” in a linear model. In the above example, \\(|T| = 8\\). The amount of penalization is controlled by \\(\\alpha\\). This is very similar to \\(\\lambda\\) in the Lasso. Intuitively, merely only looking at the number of nodes by itself is too simple because not all data sets will have the same characteristics such as \\(n\\), \\(p\\), the number of categorical variables, correlations between variables, and so fourth. In addition, if we just looked at the error (squared error in this case) we would overfit very easily. To address this issue, we use a cost function which takes into account the error as well as \\(|T|\\). To calculate the cost of a tree, number the terminal nodes from \\(1\\) to \\(|T|\\), and let the set of observations that fall into the \\(mth\\) bucket be \\(R_m\\). Then add up the squared error over all terminal nodes to the penalty term. \\[ \\text{Cost}_\\alpha(T) = \\sum_{m=1}^{|T|} \\sum_{R_m}(y_i - \\hat{y}_{Rm})^2 + \\alpha |T| \\] Step 5: Use cross-validation to select the best alpha The cost is controlled by the CP parameter. In the above example, did you notice the line rpart.control(cp = 0.003)? This is telling rpart to continue growing the tree until the CP reaches 0.003. At each subtree, we can measure the cost CP as well as the cross-validation error xerror. This is stored in the cptable tree &lt;- rpart(formula = charges ~ ., data = health_insurance, control = rpart.control(cp = 0.0001)) cost &lt;- tree$cptable %&gt;% as_tibble() %&gt;% select(nsplit, CP, xerror) cost %&gt;% head() ## # A tibble: 6 x 3 ## nsplit CP xerror ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.620 1.00 ## 2 1 0.144 0.382 ## 3 2 0.0637 0.239 ## 4 3 0.00967 0.180 ## 5 4 0.00784 0.173 ## 6 5 0.00712 0.167 As more splits are added, the cost continues to decrease, reaches a minimum, and then begins to increase. To optimize performance, choose the number of splits which has the lowest error. Often, though, the goal of using a decision tree is to create a simple model. In this case, we can err or the side of a lower nsplit so that the tree is shorter and more interpretable. All of the questions on so far have only used decision trees for interpretability, and a different model method has been used when predictive power is needed. Once we have selected \\(\\alpha\\), the tree is pruned. Sometimes the CP with the lowest error has a large number of splits, such as the case is here. tree$cptable %&gt;% as_tibble() %&gt;% select(nsplit, CP, xerror) %&gt;% arrange(xerror) %&gt;% head() ## # A tibble: 6 x 3 ## nsplit CP xerror ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 16 0.00105 0.149 ## 2 15 0.00116 0.149 ## 3 18 0.000910 0.149 ## 4 22 0.000759 0.149 ## 5 17 0.000913 0.150 ## 6 19 0.000837 0.150 The SOA will give you code to find the lowest CP value such as below. This may or may not be useful depending on if they are asking for predictive performance or interpretability. pruned_tree &lt;- prune(tree, cp = tree$cptable[which.min(tree$cptable[, &quot;xerror&quot;]), &quot;CP&quot;]) To make a simple tree, there are a few options Set the maximum depth of a tree with maxdepth Manually set cp to be higher Use fewer input variables and avoid categories with many levels Force a high number of minimum observations per terminal node with minbucket For instance, using these suggestions allows for a simpler tree to be fit. library(caret) set.seed(42) index &lt;- createDataPartition(y = health_insurance$charges, p = 0.8, list = F) train &lt;- health_insurance %&gt;% slice(index) test &lt;- health_insurance %&gt;% slice(-index) simple_tree &lt;- rpart(formula = charges ~ ., data = train, control = rpart.control(cp = 0.0001, minbucket = 200, maxdepth = 10)) rpart.plot(simple_tree, type = 3) We evaluate the performance on the test set. Because the target variable charges is highly skewed, we use the Root Mean Squared Log Error (RMSLE). We see that the complex tree has the best (lowest) error, but also has 8 terminal nodes. The simple tree with only three terminal nodes has worse (higher) error, but this is still an improvement over the mean prediction. tree_pred &lt;- predict(tree, test) simple_tree_pred &lt;- predict(simple_tree, test) get_rmsle &lt;- function(y, y_hat){ sqrt(mean((log(y) - log(y_hat))^2)) } get_rmsle(test$charges, tree_pred) ## [1] 0.3920546 get_rmsle(test$charges, simple_tree_pred) ## [1] 0.5678457 get_rmsle(test$charges, mean(train$charges)) ## [1] 0.9996513 12.1.2 Advantages and disadvantages Advantages Easy to interpret Captures interaction effects Captures non-linearities Handles continuous and categorical data Handles missing values Disadvantages Is a “weak learner” because of low predictive power Does not work on small data sets Is often a simplification of the underlying process because all observations at terminal nodes have equal predicted values Is biased towards selecting high-cardinality features because more possible split points for these features tend to lead to overfitting High variance (which can be alleviated with stricter parameters) leads the “easy to interpret results” to change upon retraining Unable to predict beyond the range of the training data for regression (because each predicted value is an average of training samples) Readings ISLR 8.1.1 Basics of Decision Trees ISLR 8.1.2 Classification Trees rpart Documentation (Optional) 12.2 Ensemble learning The “wisdom of crowds” says that often many are smater than the few. In the context of modeling, the models which we have looked at so far have been single guesses; however, often the underlying process is more complex than any single model can explain. If we build separate models and then combine them, known as ensembling, performance can be improved. Instead of trying to create a single perfect model, many simple models, known as weak learners are combined into a meta-model. The two main ways that models are combined are through bagging and boosting. 12.2.1 Bagging To start, we create many “copies” of the training data by sampling with replacement. Then we fit a simple model, typically a decision tree or linear model, to each of the data sets. Because each model is looking at different areas of the data, the predictions are different. The final model is a weighted average of each of the individual models. 12.2.2 Boosting Boosting always uses the original training data and iteratively fits models to the error of the prior models. These weak learners are ineffective by themselves but powerful when added together. Unlike with bagging, the computer must train these weak learners sequentially instead of in parallel. 12.3 Random Forests 12.3.1 Model form A random forest is the most common example of bagging. As the name implies, a forest is made up of trees. Seperate trees are fit to sampled datasets. For random forests, there is one minor modification: in order to make each model even more different, each tree selects a random subset of variables. Assume that the underlying process, \\(Y\\), has some signal within the data \\(\\mathbf{X}\\). Introduce randomness (variance) to capture the signal. Remove the variance by taking an average. When using only a single tree, there can only be as many predictions as there are terminal nodes. In a random forest, predictions can be more granular due to the contribution of each of the trees. The below graph illustrates this. A single tree (left) has stair-like, step-wise predictions whereas a random forest is free to predict any value. The color represents the predicted value (yellow = highest, black = lowest). Unlike decision trees, random forest trees do not need to be pruned. This is because overfitting is less of a problem: if one tree overfits, there are other trees which overfit in other areas to compensate. In most applications, only the mtry parameter, which controls how many variables to consider at each split, needs to be tuned. Tuning the ntrees parameter is not required; however, the soa may still ask you to. 12.3.2 Example Using the basic randomForest package we fit a model with 500 trees. This expects only numeric values. We create dummy (indicator) columns. rf_data &lt;- health_insurance %&gt;% mutate(sex = ifelse(sex == &quot;male&quot;, 1, 0), smoker = ifelse(smoker == &quot;yes&quot;, 1, 0), region_ne = ifelse(region == &quot;northeast&quot;, 1,0), region_nw = ifelse(region == &quot;northwest&quot;, 1,0), region_se = ifelse(region == &quot;southeast&quot;, 1,0), region_sw = ifelse(region == &quot;southwest&quot;, 1,0)) %&gt;% select(-region) rf_data %&gt;% glimpse(50) ## Observations: 1,338 ## Variables: 10 ## $ age &lt;dbl&gt; 19, 18, 28, 33, 32, 31, 46,... ## $ sex &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 0, 0, 1, ... ## $ bmi &lt;dbl&gt; 27.900, 33.770, 33.000, 22.... ## $ children &lt;dbl&gt; 0, 1, 3, 0, 0, 0, 1, 3, 2, ... ## $ smoker &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, ... ## $ charges &lt;dbl&gt; 16884.924, 1725.552, 4449.4... ## $ region_ne &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, ... ## $ region_nw &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 1, 0, ... ## $ region_se &lt;dbl&gt; 0, 1, 1, 0, 0, 1, 1, 0, 0, ... ## $ region_sw &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, ... library(caret) set.seed(42) index &lt;- createDataPartition(y = rf_data$charges, p = 0.8, list = F) train &lt;- rf_data %&gt;% slice(index) test &lt;- rf_data %&gt;% slice(-index) rf &lt;- randomForest(charges ~ ., data = train, ntree = 500) plot(rf) We again use RMSLE. This is lower (better) than a model that uses the average as a baseline. pred &lt;- predict(rf, test) get_rmsle &lt;- function(y, y_hat){ sqrt(mean((log(y) - log(y_hat))^2)) } get_rmsle(test$charges, pred) ## [1] 0.4772576 get_rmsle(test$charges, mean(train$charges)) ## [1] 0.9996513 12.3.3 Variable Importance Variable importance is a way of measuring how each variable contributes to overall model. For single decision trees, if a variable was “higher” up in the tree, then this variable would have greater influence. Statistically, there are two ways of measuring this: Look at the reduction in error when a the variable is randomly permuted verses using the actual values. This is done with type = 1. Use the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index; for regression, it is measured by the residual sum of squares \\(\\text{RSS}\\). This is type = 2. smoker, bmi, and age are the most importance predictors of charges. As you can imagine, variable importance is a highly useful tool for building models. We could use this to test out newly engineered features, or perform feature selection by taking the top-n features and use them in a different model. Random forests can handle very high dimensional data which allows for many tests to be run at once. varImpPlot(x = rf) 12.3.4 Partial dependence We know which variables are important, but what about the direction of the change? In a linear model we would be able to just look at the sign of the coefficient. In tree-based models, we have a tool called partial dependence. This attempts to measure the change in the predicted value by taking the average \\(\\hat{\\mathbf{y}}\\) after removing the effects of all other predictors. Although this is commonly used for trees, this approach is model-agnostic in that any model could be used. Take a model of two predictors, \\(\\hat{\\mathbf{y}} = f(\\mathbf{X}_1, \\mathbf{X_2})\\). For simplicity, say that \\(f(x_1, x_2) = 2x_1 + 3x_2\\). The data looks like this df &lt;- tibble(x1 = c(1,1,2,2), x2 = c(3,4,5,6)) %&gt;% mutate(f = 2*x1 + 3*x2) df ## # A tibble: 4 x 3 ## x1 x2 f ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3 11 ## 2 1 4 14 ## 3 2 5 19 ## 4 2 6 22 Here is the partial dependence of x1 on to f. df %&gt;% group_by(x1) %&gt;% summarise(f = mean(f)) ## # A tibble: 2 x 2 ## x1 f ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 12.5 ## 2 2 20.5 This method of using the mean is know as the Monte Carlo method. There are other methods for partial dependence that are not on the syllabus. For the RandomForest, this is done with pdp::partial(). library(pdp) bmi &lt;- pdp::partial(rf, pred.var = &quot;bmi&quot;, grid.resolution = 20) %&gt;% autoplot() + theme_bw() age &lt;- pdp::partial(rf, pred.var = &quot;age&quot;, grid.resolution = 20) %&gt;% autoplot() + theme_bw() ggarrange(bmi, age) Figure 12.1: Partial Dependence 12.3.5 Advantages and disadvantages Advantages Resilient to overfitting due to bagging Only one parameter to tune (mtry, the number of features considered at each split) Very good a multi-class prediction Nonlinearities Interaction effects Handles missing data Deals with unbalanced after over/undersampling Disadvantages Does not work on small data sets Weaker performance than other methods (GBM, NN) Unable to predict beyond training data for regression Readings ISLR 8.2.1 Bagging ISLR 8.1.2 Random Forests 12.4 Gradient Boosted Trees Another ensemble learning method is gradient boosting, also known as the Gradient Boosted Machine (GBM). Although this is unlikely to get significant attention on the PA exam due to the complexity, this is the most widely-used and powerful machine learning algorithms that are in use today. We start with an initial model, which is just a constant prediction of the mean. \\[f = f_0(\\mathbf{x_i}) = \\frac{1}{n}\\sum_{i=1}^ny_i\\] Then we update the target (what the model is predicting) by subtracting off the previously predicted value. \\[ \\hat{y_i} \\leftarrow y_i - f_0(\\mathbf{x_i})\\] This \\(\\hat{y_i}\\) is called the residual. In our example, instead of predicting charges, this would be predicting the residual of \\(\\text{charges}_i - \\text{Mean}(\\text{charges})\\). We now use this model for the residuals to update the prediction. If we updated each prediction with the prior residual directly, the algorithm would be unstable. To make this process more gradual, we use a learning rate parameter. At step 2, we have \\[f = f_0 + \\alpha f_1\\] Then we go back and fit another weak learner to this residual and repeat. \\[f = f_0 + \\alpha f_1 + \\alpha f_2\\] We then iterate through this process hundreds or thousands of times, slowly improving the prediction. Because each new tree is fit to residuals instead of the response itself, the process continuously improves the prediction. As the prediction improves, the residuals get smaller and smaller. In random forests, or other bagging algorithms, the model performance is more limited by the individual trees because each only contributes to the overall average. The name is gradient boosting because the residuals are an approximation of the gradient, and gradient descent is how the loss functions are optimized. Similarly to how GLMs can be used for classification problems through a logit transform (aka logistic regression), GBMs can also be used for classification. 12.4.1 Parameters For random forests, the individual tree parameters do not get tuned. For GBMs, however, these parameters can make a significant difference in model performance. Boosting parameters: n.trees: Integer specifying the total number of trees to fit. This is equivalent to the number of iterations and the number of basis functions in the additive expansion. Default is 100. shrinkage: a shrinkage parameter applied to each tree in the expansion. Also known as the learning rate or step-size reduction; 0.001 to 0.1 usually work, but a smaller learning rate typically requires more trees. Default is 0.1. Tree parameters: interaction.depth: Integer specifying the maximum depth of each tree (i.e., the highest level of variable interactions allowed). A value of 1 implies an additive model, a value of 2 implies a model with up to 2-way interactions, etc. Default is 1. n.minobsinnode: Integer specifying the minimum number of observations in the terminal nodes of the trees. Note that this is the actual number of observations, not the total weight. GBMs are easy to overfit, and the parameters need to be carefully tuned using cross-validation. In the Examples section we go through how to do this. Tip: Whenever fitting a model, use ?model_name to get the documentation. The parameters below are from ?gbm. 12.4.2 Example We fit a gbm below without tuning the parameters for the sake of example. library(gbm) gbm &lt;- gbm(charges ~ ., data = train, n.trees = 100, interaction.depth = 2, n.minobsinnode = 50, shrinkage = 0.1) ## Distribution not specified, assuming gaussian ... pred &lt;- predict(gbm, test, n.trees = 100) get_rmsle(test$charges, pred) ## [1] 0.4411494 get_rmsle(test$charges, mean(train$charges)) ## [1] 0.9996513 12.4.3 Advantages and disadvantages This exam covers the basics of GBMs. There are many variations of GBMs not covered in detail such as xgboost. Advantages High prediction accuracy Shown to work empirically well on many types of problems Nonlinearities, interaction effects, resilient to outliers, corrects for missing values Deals with class imbalance directly by weighting observations Disadvantages Requires large sample size Longer training time Does not detect linear combinations of features. These must be engineered Can overfit if not tuned correctly Readings ISLR 8.2.3 Boosting 12.5 Exercises Run this code on your computer to answer these exercises. 1. RF with randomForest (Part 1 of 2) The below code is set up to fit a random forest to the soa_mortality data set to predict actual_cnt. There is a problem: all of the predictions are coming out to be 1. Find out why this is happening and fix it. set.seed(42) #For the sake of this example, only take 20% of the records df &lt;- soa_mortality %&gt;% sample_frac(0.2) %&gt;% mutate(target = as.factor(ifelse(actual_cnt == 0, 1, 0))) %&gt;% select(target, prodcat, distchan, smoker, sex, issage, uwkey) %&gt;% mutate_if(is.character, ~as.factor(.x)) #check that the target has 0&#39;s and 1&#39;s df %&gt;% count(target) ## # A tibble: 2 x 2 ## target n ## &lt;fct&gt; &lt;int&gt; ## 1 0 1811 ## 2 1 98189 library(caret) index &lt;- createDataPartition(y = df$target, p = 0.8, list = F) train &lt;- df %&gt;% slice(index) test &lt;- df %&gt;% slice(-index) k = 0.1 cutoff=c(k,1-k) model &lt;- randomForest( formula = target ~ ., data = train, ntree = 100, cutoff = cutoff ) pred &lt;- predict(model, test) confusionMatrix(pred, test$target) (Part 2 of 2) Downsample the majority class and refit the model, and then choose between the original data and the downsampled data based on the model performance. Use your own judgement when choosing how to evaluate the model based on accuracy, sensitivity, specificity, and Kappa. down_train &lt;- downSample(x = train %&gt;% select(-target), y = train$target) down_test &lt;- downSample(x = test %&gt;% select(-target), y = test$target) down_train %&gt;% count(Class) model &lt;- randomForest( formula = Class ~ ., data = down_train, ntree = 100, cutoff = cutoff ) down_pred &lt;- predict(model, down_test) confusionMatrix(down_pred, down_test$Class) Now up-sample the minority class and repeat the same procedure. up_train &lt;- upSample(x = train %&gt;% select(-target), y = train$target) up_test &lt;- upSample(x = test %&gt;% select(-target), y = test$target) up_train %&gt;% count(Class) model &lt;- randomForest( formula = Class ~ ., data = up_train, ntree = 100, cutoff = cutoff ) up_pred &lt;- predict(model, up_test) confusionMatrix(up_pred, up_test$Class) 2. RF tuning with caret The best practice of tuning a model is with cross-validation. This can only be done in the caret library. If the SOA asks you to use caret, they will likely ask you a question related to cross validation as below. An actuary has trained a predictive model and chosen the best hyperparameters, cleaned the data, and performed feature engineering. They have one problem, however: the error on the training data is far lower than on new, unseen test data. Read the code below and determine their problem. Find a way to lower the error on the test data without changing the model or the data. Explain the rational behind your method. library(caret) set.seed(42) #Take only 1000 records #Uncomment this when completing this exercise data &lt;- health_insurance %&gt;% sample_n(1000) index &lt;- createDataPartition( y = data$charges, p = 0.8, list = F) %&gt;% as.numeric() train &lt;- health_insurance %&gt;% slice(index) test &lt;- health_insurance %&gt;% slice(-index) control &lt;- trainControl( method=&#39;boot&#39;, number=2, p = 0.2) tunegrid &lt;- expand.grid(.mtry=c(1,3,5)) rf &lt;- train(charges ~ ., data = train, method=&#39;rf&#39;, tuneGrid=tunegrid, trControl=control) pred_train &lt;- predict(rf, train) pred_test &lt;- predict(rf, test) get_rmse &lt;- function(y, y_hat){ sqrt(mean((y - y_hat)^2)) } get_rmse(pred_train, train$charges) get_rmse(pred_test, test$charges) 12.5.0.1 Tuning a GBM with caret If the SOA asks you to tune a GBM, they will need to give you starting hyperparameters which are close to the “best” values due to how slow the Prometric computers are. Another possibility is that they pre-train a GBM model object and ask that you use it. This example looks at 27 combinations of hyper parameters. tunegrid &lt;- expand.grid( interaction.depth = c(1,3,5), n.trees = c(20, 40, 100), shrinkage = c(0.01, 0.001, 0.0001), n.minobsinnode = 20) nrow(tunegrid) gbm &lt;- train(charges ~ bmi + age + sex + region, data = health_insurance_train, method=&#39;gbm&#39;, tuneGrid=tunegrid, trControl=control, #Show detailed output verbose = FALSE ) The output shows the RMSE for each of the 27 models tested. trellis.par.set(caretTheme()) plot(gbm) The summary.gbm function, which R understands from the summary function, shows the variable importance. The most predictive feature is age, followed by bmi. summary(gbm, plotit = F) %&gt;% as_tibble() 12.6 Answers to Exercises Answers to these exercises are available at ExamPA.net. "],
["practice-exams.html", " 13 Practice Exams", " 13 Practice Exams Practice exams are available at ExamPA.net. Current practice exams and video solutions: One introductory practice exam with a custom data set from Kaggle An adaptation of the Miner’s Union December 2018 Exam to the current format A video solution to Hospital Readmissions A tutorial on data manipulation (Coming next week) - Student Success Video Solution - June 2019 Exam Video Solution "]
]
