<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 18 Unsupervised Learning | Predictive Analytics for Actuaries</title>
  <meta name="description" content=" 18 Unsupervised Learning | Predictive Analytics for Actuaries" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content=" 18 Unsupervised Learning | Predictive Analytics for Actuaries" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 18 Unsupervised Learning | Predictive Analytics for Actuaries" />
  
  
  

<meta name="author" content="Sam Castillo" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="images/artificial_actuary_logo_favicon.png" type="image/x-icon" />
<link rel="prev" href="tree-based-models.html"/>
<link rel="next" href="references-2.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Exam PA Study Manual</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="join-the-online-course.html"><a href="join-the-online-course.html"><i class="fa fa-check"></i><b>1</b> Join the online course</a></li>
<li class="chapter" data-level="2" data-path="how-to-use-this-book.html"><a href="how-to-use-this-book.html"><i class="fa fa-check"></i><b>2</b> How to use this book</a></li>
<li class="chapter" data-level="3" data-path="how-to-contribute-to-this-book.html"><a href="how-to-contribute-to-this-book.html"><i class="fa fa-check"></i><b>3</b> How to Contribute to this book</a></li>
<li class="chapter" data-level="4" data-path="the-exam.html"><a href="the-exam.html"><i class="fa fa-check"></i><b>4</b> The exam</a></li>
<li class="chapter" data-level="5" data-path="prometric-demo.html"><a href="prometric-demo.html"><i class="fa fa-check"></i><b>5</b> Prometric Demo</a></li>
<li class="chapter" data-level="6" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>6</b> Introduction</a></li>
<li class="chapter" data-level="7" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>7</b> Getting started</a><ul>
<li class="chapter" data-level="7.1" data-path="getting-started.html"><a href="getting-started.html#download-the-data"><i class="fa fa-check"></i><b>7.1</b> Download the data</a></li>
<li class="chapter" data-level="7.2" data-path="getting-started.html"><a href="getting-started.html#download-islr"><i class="fa fa-check"></i><b>7.2</b> Download ISLR</a></li>
<li class="chapter" data-level="7.3" data-path="getting-started.html"><a href="getting-started.html#new-users"><i class="fa fa-check"></i><b>7.3</b> New users</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="r-programming.html"><a href="r-programming.html"><i class="fa fa-check"></i><b>8</b> R programming</a><ul>
<li class="chapter" data-level="8.1" data-path="r-programming.html"><a href="r-programming.html#notebook-chunks"><i class="fa fa-check"></i><b>8.1</b> Notebook chunks</a></li>
<li class="chapter" data-level="8.2" data-path="r-programming.html"><a href="r-programming.html#basic-operations"><i class="fa fa-check"></i><b>8.2</b> Basic operations</a></li>
<li class="chapter" data-level="8.3" data-path="r-programming.html"><a href="r-programming.html#lists"><i class="fa fa-check"></i><b>8.3</b> Lists</a></li>
<li class="chapter" data-level="8.4" data-path="r-programming.html"><a href="r-programming.html#functions"><i class="fa fa-check"></i><b>8.4</b> Functions</a></li>
<li class="chapter" data-level="8.5" data-path="r-programming.html"><a href="r-programming.html#data-frames"><i class="fa fa-check"></i><b>8.5</b> Data frames</a></li>
<li class="chapter" data-level="8.6" data-path="r-programming.html"><a href="r-programming.html#pipes"><i class="fa fa-check"></i><b>8.6</b> Pipes</a></li>
<li class="chapter" data-level="8.7" data-path="r-programming.html"><a href="r-programming.html#the-soas-code-doesnt-use-pipes-or-dplyr-so-can-i-skip-learning-this"><i class="fa fa-check"></i><b>8.7</b> The SOA‚Äôs code doesn‚Äôt use pipes or dplyr, so can I skip learning this?</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="data-manipulation.html"><a href="data-manipulation.html"><i class="fa fa-check"></i><b>9</b> Data manipulation</a><ul>
<li class="chapter" data-level="9.1" data-path="data-manipulation.html"><a href="data-manipulation.html#garbage-in-garbage-out"><i class="fa fa-check"></i><b>9.1</b> Garbage in; garbage out üóë</a></li>
<li class="chapter" data-level="9.2" data-path="data-manipulation.html"><a href="data-manipulation.html#be-a-detective"><i class="fa fa-check"></i><b>9.2</b> Be a detective üîé</a></li>
<li class="chapter" data-level="9.3" data-path="data-manipulation.html"><a href="data-manipulation.html#a-picture-is-worth-a-thousand-words"><i class="fa fa-check"></i><b>9.3</b> A picture is worth a thousand words üì∑</a></li>
<li class="chapter" data-level="9.4" data-path="data-manipulation.html"><a href="data-manipulation.html#factor-or-numeric"><i class="fa fa-check"></i><b>9.4</b> Factor or numeric ‚ùì</a></li>
<li class="chapter" data-level="9.5" data-path="data-manipulation.html"><a href="data-manipulation.html#of-statistics-are-false"><i class="fa fa-check"></i><b>9.5</b> 73.6% of statistics are false üò®</a></li>
<li class="chapter" data-level="9.6" data-path="data-manipulation.html"><a href="data-manipulation.html#how-to-save-time-with-dplyr"><i class="fa fa-check"></i><b>9.6</b> How to save time with dplyr</a></li>
<li class="chapter" data-level="9.7" data-path="data-manipulation.html"><a href="data-manipulation.html#look-at-the-data"><i class="fa fa-check"></i><b>9.7</b> Look at the data</a></li>
<li class="chapter" data-level="9.8" data-path="data-manipulation.html"><a href="data-manipulation.html#transform-the-data"><i class="fa fa-check"></i><b>9.8</b> Transform the data</a></li>
<li class="chapter" data-level="9.9" data-path="data-manipulation.html"><a href="data-manipulation.html#exercises"><i class="fa fa-check"></i><b>9.9</b> Exercises</a></li>
<li class="chapter" data-level="9.10" data-path="data-manipulation.html"><a href="data-manipulation.html#answers-to-exercises"><i class="fa fa-check"></i><b>9.10</b> Answers to exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i><b>10</b> Visualization</a><ul>
<li class="chapter" data-level="10.1" data-path="visualization.html"><a href="visualization.html#create-a-plot-object-ggplot"><i class="fa fa-check"></i><b>10.1</b> Create a plot object (ggplot)</a></li>
<li class="chapter" data-level="10.2" data-path="visualization.html"><a href="visualization.html#add-a-plot"><i class="fa fa-check"></i><b>10.2</b> Add a plot</a></li>
<li class="chapter" data-level="10.3" data-path="visualization.html"><a href="visualization.html#data-manipulation-chaining"><i class="fa fa-check"></i><b>10.3</b> Data manipulation chaining</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html"><i class="fa fa-check"></i><b>11</b> Introduction to modeling</a><ul>
<li class="chapter" data-level="11.1" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#modeling-vocabulary"><i class="fa fa-check"></i><b>11.1</b> Modeling vocabulary</a></li>
<li class="chapter" data-level="11.2" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#modeling-notation"><i class="fa fa-check"></i><b>11.2</b> Modeling notation</a></li>
<li class="chapter" data-level="11.3" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>11.3</b> Ordinary Least Squares (OLS)</a></li>
<li class="chapter" data-level="11.4" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#regression-vs.-classification"><i class="fa fa-check"></i><b>11.4</b> Regression vs.¬†classification</a></li>
<li class="chapter" data-level="11.5" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#regression-metrics"><i class="fa fa-check"></i><b>11.5</b> Regression metrics</a></li>
<li class="chapter" data-level="11.6" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#example"><i class="fa fa-check"></i><b>11.6</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html"><i class="fa fa-check"></i><b>12</b> Generalized linear Models (GLMs)</a><ul>
<li class="chapter" data-level="12.0.1" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#assumptions-of-ols"><i class="fa fa-check"></i><b>12.0.1</b> Assumptions of OLS</a></li>
<li class="chapter" data-level="12.0.2" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#assumptions-of-glms"><i class="fa fa-check"></i><b>12.0.2</b> Assumptions of GLMs</a></li>
<li class="chapter" data-level="12.1" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#advantages-and-disadvantages"><i class="fa fa-check"></i><b>12.1</b> Advantages and disadvantages</a></li>
<li class="chapter" data-level="12.2" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#glms-for-regression"><i class="fa fa-check"></i><b>12.2</b> GLMs for regression</a></li>
<li class="chapter" data-level="12.3" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#interpretation-of-coefficients"><i class="fa fa-check"></i><b>12.3</b> Interpretation of coefficients</a><ul>
<li class="chapter" data-level="12.3.1" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#identity-link"><i class="fa fa-check"></i><b>12.3.1</b> Identity link</a></li>
<li class="chapter" data-level="12.3.2" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#log-link"><i class="fa fa-check"></i><b>12.3.2</b> Log link</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html#other-links"><i class="fa fa-check"></i><b>12.4</b> Other links</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="glms-for-classification.html"><a href="glms-for-classification.html"><i class="fa fa-check"></i><b>13</b> GLMs for classification</a><ul>
<li class="chapter" data-level="13.1" data-path="glms-for-classification.html"><a href="glms-for-classification.html#binary-target"><i class="fa fa-check"></i><b>13.1</b> Binary target</a></li>
<li class="chapter" data-level="13.2" data-path="glms-for-classification.html"><a href="glms-for-classification.html#count-target"><i class="fa fa-check"></i><b>13.2</b> Count target</a></li>
<li class="chapter" data-level="13.3" data-path="glms-for-classification.html"><a href="glms-for-classification.html#link-functions"><i class="fa fa-check"></i><b>13.3</b> Link functions</a></li>
<li class="chapter" data-level="13.4" data-path="glms-for-classification.html"><a href="glms-for-classification.html#interpretation-of-coefficients-1"><i class="fa fa-check"></i><b>13.4</b> Interpretation of coefficients</a><ul>
<li class="chapter" data-level="13.4.1" data-path="glms-for-classification.html"><a href="glms-for-classification.html#logit"><i class="fa fa-check"></i><b>13.4.1</b> Logit</a></li>
<li class="chapter" data-level="13.4.2" data-path="glms-for-classification.html"><a href="glms-for-classification.html#probit-cauchit-cloglog"><i class="fa fa-check"></i><b>13.4.2</b> Probit, Cauchit, Cloglog</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="glms-for-classification.html"><a href="glms-for-classification.html#example-1"><i class="fa fa-check"></i><b>13.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="classification-metrics.html"><a href="classification-metrics.html"><i class="fa fa-check"></i><b>14</b> Classification metrics</a><ul>
<li class="chapter" data-level="14.1" data-path="classification-metrics.html"><a href="classification-metrics.html#area-under-the-roc-curve-auc"><i class="fa fa-check"></i><b>14.1</b> Area Under the ROC Curve (AUC)</a></li>
<li class="chapter" data-level="14.2" data-path="classification-metrics.html"><a href="classification-metrics.html#additional-reading"><i class="fa fa-check"></i><b>14.2</b> Additional reading</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html"><i class="fa fa-check"></i><b>15</b> Additional GLM topics</a><ul>
<li class="chapter" data-level="15.1" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#residuals"><i class="fa fa-check"></i><b>15.1</b> Residuals</a><ul>
<li class="chapter" data-level="15.1.1" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#raw-residuals"><i class="fa fa-check"></i><b>15.1.1</b> Raw residuals</a></li>
<li class="chapter" data-level="15.1.2" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#deviance-residuals"><i class="fa fa-check"></i><b>15.1.2</b> Deviance residuals</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#example-2"><i class="fa fa-check"></i><b>15.2</b> Example</a></li>
<li class="chapter" data-level="15.3" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#log-transforms-of-continuous-predictors"><i class="fa fa-check"></i><b>15.3</b> Log transforms of continuous predictors</a></li>
<li class="chapter" data-level="15.4" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#reference-levels"><i class="fa fa-check"></i><b>15.4</b> Reference levels</a></li>
<li class="chapter" data-level="15.5" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#interactions"><i class="fa fa-check"></i><b>15.5</b> Interactions</a></li>
<li class="chapter" data-level="15.6" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#offsets"><i class="fa fa-check"></i><b>15.6</b> Offsets</a></li>
<li class="chapter" data-level="15.7" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#tweedie-regression"><i class="fa fa-check"></i><b>15.7</b> Tweedie regression</a></li>
<li class="chapter" data-level="15.8" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#combinations-of-link-functions-and-target-distributions"><i class="fa fa-check"></i><b>15.8</b> Combinations of Link Functions and Target Distributions</a><ul>
<li class="chapter" data-level="15.8.1" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-log-link"><i class="fa fa-check"></i><b>15.8.1</b> Gaussian Response with Log Link</a></li>
<li class="chapter" data-level="15.8.2" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-inverse-link"><i class="fa fa-check"></i><b>15.8.2</b> Gaussian Response with Inverse Link</a></li>
<li class="chapter" data-level="15.8.3" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-identity-link"><i class="fa fa-check"></i><b>15.8.3</b> Gaussian Response with Identity Link</a></li>
<li class="chapter" data-level="15.8.4" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gaussian-response-with-log-link-and-negative-values"><i class="fa fa-check"></i><b>15.8.4</b> Gaussian Response with Log Link and Negative Values</a></li>
<li class="chapter" data-level="15.8.5" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gamma-response-with-log-link"><i class="fa fa-check"></i><b>15.8.5</b> Gamma Response with Log Link</a></li>
<li class="chapter" data-level="15.8.6" data-path="additional-glm-topics.html"><a href="additional-glm-topics.html#gamma-with-inverse-link"><i class="fa fa-check"></i><b>15.8.6</b> Gamma with Inverse Link</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html"><i class="fa fa-check"></i><b>16</b> GLM variable selection</a><ul>
<li class="chapter" data-level="16.1" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#stepwise-subset-selection"><i class="fa fa-check"></i><b>16.1</b> Stepwise subset selection</a></li>
<li class="chapter" data-level="16.2" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#penalized-linear-models"><i class="fa fa-check"></i><b>16.2</b> Penalized Linear Models</a></li>
<li class="chapter" data-level="16.3" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#ridge-regression"><i class="fa fa-check"></i><b>16.3</b> Ridge Regression</a></li>
<li class="chapter" data-level="16.4" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#lasso"><i class="fa fa-check"></i><b>16.4</b> Lasso</a></li>
<li class="chapter" data-level="16.5" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#elastic-net"><i class="fa fa-check"></i><b>16.5</b> Elastic Net</a></li>
<li class="chapter" data-level="16.6" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#advantages-and-disadvantages-1"><i class="fa fa-check"></i><b>16.6</b> Advantages and disadvantages</a></li>
<li class="chapter" data-level="16.7" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#example-ridge-regression"><i class="fa fa-check"></i><b>16.7</b> Example: Ridge Regression</a></li>
<li class="chapter" data-level="16.8" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#example-the-lasso"><i class="fa fa-check"></i><b>16.8</b> Example: The Lasso</a></li>
<li class="chapter" data-level="16.9" data-path="glm-variable-selection.html"><a href="glm-variable-selection.html#references"><i class="fa fa-check"></i><b>16.9</b> References</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="tree-based-models.html"><a href="tree-based-models.html"><i class="fa fa-check"></i><b>17</b> Tree-based models</a><ul>
<li class="chapter" data-level="17.1" data-path="tree-based-models.html"><a href="tree-based-models.html#decision-trees"><i class="fa fa-check"></i><b>17.1</b> Decision Trees</a><ul>
<li class="chapter" data-level="17.1.1" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-2"><i class="fa fa-check"></i><b>17.1.1</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="tree-based-models.html"><a href="tree-based-models.html#ensemble-learning"><i class="fa fa-check"></i><b>17.2</b> Ensemble learning</a><ul>
<li class="chapter" data-level="17.2.1" data-path="tree-based-models.html"><a href="tree-based-models.html#bagging"><i class="fa fa-check"></i><b>17.2.1</b> Bagging</a></li>
<li class="chapter" data-level="17.2.2" data-path="tree-based-models.html"><a href="tree-based-models.html#boosting"><i class="fa fa-check"></i><b>17.2.2</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="tree-based-models.html"><a href="tree-based-models.html#random-forests"><i class="fa fa-check"></i><b>17.3</b> Random Forests</a><ul>
<li class="chapter" data-level="17.3.1" data-path="tree-based-models.html"><a href="tree-based-models.html#example-3"><i class="fa fa-check"></i><b>17.3.1</b> Example</a></li>
<li class="chapter" data-level="17.3.2" data-path="tree-based-models.html"><a href="tree-based-models.html#variable-importance"><i class="fa fa-check"></i><b>17.3.2</b> Variable Importance</a></li>
<li class="chapter" data-level="17.3.3" data-path="tree-based-models.html"><a href="tree-based-models.html#partial-dependence"><i class="fa fa-check"></i><b>17.3.3</b> Partial dependence</a></li>
<li class="chapter" data-level="17.3.4" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-3"><i class="fa fa-check"></i><b>17.3.4</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="tree-based-models.html"><a href="tree-based-models.html#gradient-boosted-trees"><i class="fa fa-check"></i><b>17.4</b> Gradient Boosted Trees</a><ul>
<li class="chapter" data-level="17.4.1" data-path="tree-based-models.html"><a href="tree-based-models.html#adaboost"><i class="fa fa-check"></i><b>17.4.1</b> AdaBoost</a></li>
<li class="chapter" data-level="17.4.2" data-path="tree-based-models.html"><a href="tree-based-models.html#gradient-boosting"><i class="fa fa-check"></i><b>17.4.2</b> Gradient Boosting</a></li>
<li class="chapter" data-level="17.4.3" data-path="tree-based-models.html"><a href="tree-based-models.html#notation"><i class="fa fa-check"></i><b>17.4.3</b> Notation</a></li>
<li class="chapter" data-level="17.4.4" data-path="tree-based-models.html"><a href="tree-based-models.html#parameters"><i class="fa fa-check"></i><b>17.4.4</b> Parameters</a></li>
<li class="chapter" data-level="17.4.5" data-path="tree-based-models.html"><a href="tree-based-models.html#example-4"><i class="fa fa-check"></i><b>17.4.5</b> Example</a></li>
<li class="chapter" data-level="17.4.6" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-4"><i class="fa fa-check"></i><b>17.4.6</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="tree-based-models.html"><a href="tree-based-models.html#exercises-1"><i class="fa fa-check"></i><b>17.5</b> Exercises</a><ul>
<li class="chapter" data-level="17.5.1" data-path="tree-based-models.html"><a href="tree-based-models.html#rf-with-randomforest"><i class="fa fa-check"></i><b>17.5.1</b> 1. RF with <code>randomForest</code></a></li>
<li class="chapter" data-level="17.5.2" data-path="tree-based-models.html"><a href="tree-based-models.html#rf-tuning-with-caret"><i class="fa fa-check"></i><b>17.5.2</b> 2. RF tuning with <code>caret</code></a></li>
<li class="chapter" data-level="17.5.3" data-path="tree-based-models.html"><a href="tree-based-models.html#tuning-a-gbm-with-caret"><i class="fa fa-check"></i><b>17.5.3</b> 3. Tuning a GBM with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>18</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="18.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>18.1</b> Principal Component Analysis (PCA)</a><ul>
<li class="chapter" data-level="18.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-pca-on-us-arrests"><i class="fa fa-check"></i><b>18.1.1</b> Example: PCA on US Arrests</a></li>
<li class="chapter" data-level="18.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-pca-on-cancel-cells"><i class="fa fa-check"></i><b>18.1.2</b> Example: PCA on Cancel Cells</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering"><i class="fa fa-check"></i><b>18.2</b> Clustering</a><ul>
<li class="chapter" data-level="18.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>18.2.1</b> K-Means Clustering</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#hierarchical-clustering"><i class="fa fa-check"></i><b>18.3</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="18.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-clustering-cancel-cells"><i class="fa fa-check"></i><b>18.3.1</b> Example: Clustering Cancel Cells</a></li>
<li class="chapter" data-level="18.3.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#references-1"><i class="fa fa-check"></i><b>18.3.2</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="references-2.html"><a href="references-2.html"><i class="fa fa-check"></i><b>19</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predictive Analytics for Actuaries</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="unsupervised-learning" class="section level1">
<h1><span class="header-section-number"> 18</span> Unsupervised Learning</h1>
<p>Up to this point we have been trying to predict something. We use data <span class="math inline">\(X\)</span> to predict an unknown <span class="math inline">\(Y\)</span>, which is a number in the case of regression and a category in the case of classification. These problems are known as <em>supvervised</em> because we are ‚Äúsupervising‚Äù how the model learns by giving it a target.</p>
<p>Here is an easy way to remember this: If you are in kindergarten class, and the teacher provides cards with pictures and words, and then asks each kid to look at the picture and then say the word, this is supervised learning. The label is the name of the picture, and the data is the picture itself. Instead, if the teacher gives out finger paints and says ‚Äúexpress yourself!‚Äù, then this is <em>unsupervised learning</em>. There is no label but only data.</p>
<div id="principal-component-analysis-pca" class="section level2">
<h2><span class="header-section-number">18.1</span> Principal Component Analysis (PCA)</h2>
<p>Often there are a lot of columns in the data that contain redundant information. PCA is one method of simplifying.</p>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/FgakZw6K1QQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>StatQuest. ‚ÄúPrincipal Component Analysis (PCA), Step-by-Step‚Äù YouTube, Joshua Starmer, 2 Apr 2018, <a href="https://www.youtube.com/watch?v=FgakZw6K1QQ" class="uri">https://www.youtube.com/watch?v=FgakZw6K1QQ</a></p>
<p>For example, if your data consists of city traffic, such as (1) the number of cars on the road, (2) number of taxis, (3) number of pedestrians, and (4) number of Ubers, then knowing any one of these values will tell you about how busy the given road is. If there are a lot of Ubers, then there are probably also a lot of Taxis. Intuitively, we probably don‚Äôt need four variables to measure this info and we could have a single variable called ‚Äútraffic level‚Äù. This would be reducing the dimension from 4 to 1.</p>
<p>PCA is a dimensionality reduction method which reduces the number of variables needed to retain most of the information in a matrix. If there are predictor variables <span class="math inline">\(X_1, X_2, X_3, X_4, X_5\)</span>, then running PCA and choosing the first three Principal Components (PCs) will reduce the dimension from 5 to 3.</p>
<p><img src="images/PCA.png" width="377" style="display: block; margin: auto;" /></p>
<p>Each PC is a linear combination of the original <span class="math inline">\(X\)</span>s. For example, PC1 might be</p>
<p><span class="math display">\[PC_1 = 0.2X_1 + 0.3X_2 - 0.2X_3 + 0X_5 + 0.3X_5\]</span>
The weights here are also called ‚Äúloadings‚Äù or ‚Äúrotations‚Äù, and are (0.2, 0.3, -0.2, 0, 0.3). Each of the PCs can be interpreted as explaining part of the data. In the traffic example, PC1 might explain the traffic level, PC2 the weather, and PC3 the time of day.</p>
<table>
<thead>
<tr class="header">
<th>Readings</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ISLR 10.2 Principal Component Analysis</td>
<td></td>
</tr>
<tr class="even">
<td>ISLR 10.3 Clustering Methods</td>
<td></td>
</tr>
</tbody>
</table>
<div id="example-pca-on-us-arrests" class="section level3">
<h3><span class="header-section-number">18.1.1</span> Example: PCA on US Arrests</h3>
<p>In this example, we perform PCA on the <code>USArrests</code> data set, which is part of
the base <code>R</code> package. The rows of the data set contain the 50 states, in
alphabetical order:</p>
<div class="sourceCode" id="cb391"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb391-1" title="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb391-2" title="2">states=<span class="kw">row.names</span>(USArrests)</a>
<a class="sourceLine" id="cb391-3" title="3">states</a></code></pre></div>
<pre><code>##  [1] &quot;Alabama&quot;        &quot;Alaska&quot;         &quot;Arizona&quot;        &quot;Arkansas&quot;      
##  [5] &quot;California&quot;     &quot;Colorado&quot;       &quot;Connecticut&quot;    &quot;Delaware&quot;      
##  [9] &quot;Florida&quot;        &quot;Georgia&quot;        &quot;Hawaii&quot;         &quot;Idaho&quot;         
## [13] &quot;Illinois&quot;       &quot;Indiana&quot;        &quot;Iowa&quot;           &quot;Kansas&quot;        
## [17] &quot;Kentucky&quot;       &quot;Louisiana&quot;      &quot;Maine&quot;          &quot;Maryland&quot;      
## [21] &quot;Massachusetts&quot;  &quot;Michigan&quot;       &quot;Minnesota&quot;      &quot;Mississippi&quot;   
## [25] &quot;Missouri&quot;       &quot;Montana&quot;        &quot;Nebraska&quot;       &quot;Nevada&quot;        
## [29] &quot;New Hampshire&quot;  &quot;New Jersey&quot;     &quot;New Mexico&quot;     &quot;New York&quot;      
## [33] &quot;North Carolina&quot; &quot;North Dakota&quot;   &quot;Ohio&quot;           &quot;Oklahoma&quot;      
## [37] &quot;Oregon&quot;         &quot;Pennsylvania&quot;   &quot;Rhode Island&quot;   &quot;South Carolina&quot;
## [41] &quot;South Dakota&quot;   &quot;Tennessee&quot;      &quot;Texas&quot;          &quot;Utah&quot;          
## [45] &quot;Vermont&quot;        &quot;Virginia&quot;       &quot;Washington&quot;     &quot;West Virginia&quot; 
## [49] &quot;Wisconsin&quot;      &quot;Wyoming&quot;</code></pre>
<p>The columns of the data set contain four variables relating to various crimes:</p>
<div class="sourceCode" id="cb393"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb393-1" title="1"><span class="kw">glimpse</span>(USArrests)</a></code></pre></div>
<pre><code>## Observations: 50
## Variables: 4
## $ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3,...
## $ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120,...
## $ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57...
## $ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8...</code></pre>
<p>Let‚Äôs start by taking a quick look at the column means of the data.</p>
<div class="sourceCode" id="cb395"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb395-1" title="1">USArrests <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise_all</span>(mean)</a></code></pre></div>
<pre><code>##   Murder Assault UrbanPop   Rape
## 1  7.788  170.76    65.54 21.232</code></pre>
<p>We see right away the the data have <strong>vastly</strong> different means. We can also examine the variances of the four variables.</p>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb397-1" title="1">USArrests <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise_all</span>(var)</a></code></pre></div>
<pre><code>##     Murder  Assault UrbanPop     Rape
## 1 18.97047 6945.166 209.5188 87.72916</code></pre>
<p>Not surprisingly, the variables also have vastly different variances: the
<code>UrbanPop</code> variable measures the percentage of the population in each state
living in an urban area, which is not a comparable number to the number
of crimes committed in each state per 100,000 individuals. If we failed to scale the
variables before performing PCA, then most of the principal components
that we observed would be driven by the <code>Assault</code> variable, since it has by
far the largest mean and variance.</p>
<p>Thus, it is important to standardize the
variables to have mean zero and standard deviation 1 before performing
PCA. We‚Äôll perform principal components analysis using the <code>prcomp()</code> function, which is one of several functions that perform PCA. By default, this centers the variables to have mean zero. By using the option <code>scale=TRUE</code>, we scale the variables to have standard
deviation 1:</p>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb399-1" title="1">pca =<span class="st"> </span><span class="kw">prcomp</span>(USArrests, <span class="dt">scale=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<p>The output from <code>prcomp()</code> contains a number of useful quantities:</p>
<div class="sourceCode" id="cb400"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb400-1" title="1"><span class="kw">names</span>(pca)</a></code></pre></div>
<pre><code>## [1] &quot;sdev&quot;     &quot;rotation&quot; &quot;center&quot;   &quot;scale&quot;    &quot;x&quot;</code></pre>
<p>The <code>center</code> and <code>scale</code> components correspond to the means and standard
deviations of the variables that were used for scaling prior to implementing
PCA:</p>
<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb402-1" title="1">pca<span class="op">$</span>center</a></code></pre></div>
<pre><code>##   Murder  Assault UrbanPop     Rape 
##    7.788  170.760   65.540   21.232</code></pre>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb404-1" title="1">pca<span class="op">$</span>scale</a></code></pre></div>
<pre><code>##    Murder   Assault  UrbanPop      Rape 
##  4.355510 83.337661 14.474763  9.366385</code></pre>
<p>The rotation matrix provides the principal component loadings; each column
of <code>pr.out\$rotation</code> contains the corresponding principal component
loading vector:</p>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb406-1" title="1">pca<span class="op">$</span>rotation</a></code></pre></div>
<pre><code>##                 PC1        PC2        PC3         PC4
## Murder   -0.5358995  0.4181809 -0.3412327  0.64922780
## Assault  -0.5831836  0.1879856 -0.2681484 -0.74340748
## UrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773
## Rape     -0.5434321 -0.1673186  0.8177779  0.08902432</code></pre>
<p>We see that there are four distinct principal components. This is to be
expected because there are in general <code>min(n ‚àí 1, p)</code> informative principal
components in a data set with <span class="math inline">\(n\)</span> observations and <span class="math inline">\(p\)</span> variables.</p>
<p>Using the <code>prcomp()</code> function, we do not need to explicitly multiply the
data by the principal component loading vectors in order to obtain the
principal component score vectors. Rather the 50 √ó 4 matrix <span class="math inline">\(x\)</span> has as its
columns the principal component score vectors. That is, the $k^{th<code>column is the $k^{th</code> principal component score vector. We‚Äôll take a look at the first few states:</p>
<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb408-1" title="1"><span class="kw">head</span>(pca<span class="op">$</span>x)</a></code></pre></div>
<pre><code>##                   PC1        PC2         PC3          PC4
## Alabama    -0.9756604  1.1220012 -0.43980366  0.154696581
## Alaska     -1.9305379  1.0624269  2.01950027 -0.434175454
## Arizona    -1.7454429 -0.7384595  0.05423025 -0.826264240
## Arkansas    0.1399989  1.1085423  0.11342217 -0.180973554
## California -2.4986128 -1.5274267  0.59254100 -0.338559240
## Colorado   -1.4993407 -0.9776297  1.08400162  0.001450164</code></pre>
<p>We can plot the first two principal components using the <code>biplot()</code> function:</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb410-1" title="1"><span class="kw">biplot</span>(pca, <span class="dt">scale=</span><span class="dv">0</span>)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-217-1.png" width="768" /></p>
<p>The <code>scale=0</code> argument to <code>biplot()</code> ensures that the arrows are scaled to
represent the loadings; other values for <code>scale</code> give slightly different bi plots
with different interpretations.</p>
<p>The <code>prcomp()</code> function also outputs the standard deviation of each principal
component. We can access these standard deviations as follows:</p>
<div class="sourceCode" id="cb411"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb411-1" title="1">pca<span class="op">$</span>sdev</a></code></pre></div>
<pre><code>## [1] 1.5748783 0.9948694 0.5971291 0.4164494</code></pre>
<p>The variance explained by each principal component is obtained by squaring
these:</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb413-1" title="1">pca_var=pca<span class="op">$</span>sdev<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb413-2" title="2">pca_var</a></code></pre></div>
<pre><code>## [1] 2.4802416 0.9897652 0.3565632 0.1734301</code></pre>
<p>To compute the proportion of variance explained by each principal component,
we simply divide the variance explained by each principal component
by the total variance explained by all four principal components:</p>
<div class="sourceCode" id="cb415"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb415-1" title="1">pve=pca_var<span class="op">/</span><span class="kw">sum</span>(pca_var)</a>
<a class="sourceLine" id="cb415-2" title="2">pve</a></code></pre></div>
<pre><code>## [1] 0.62006039 0.24744129 0.08914080 0.04335752</code></pre>
<p>We see that the first principal component explains 62.0% of the variance
in the data, the next principal component explains 24.7% of the variance,
and so forth. We can plot the PVE explained by each component as follows:</p>
<div class="sourceCode" id="cb417"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb417-1" title="1"><span class="kw">plot</span>(pve, <span class="dt">xlab=</span><span class="st">&quot;Principal Component&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Proportion of Variance Explained&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="dt">type=</span><span class="st">&#39;b&#39;</span>)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-221-1.png" width="576" /></p>
<p>We can also use the function <code>cumsum()</code>, which computes the cumulative sum of the elements of a numeric vector, to plot the cumulative PVE:</p>
<div class="sourceCode" id="cb418"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb418-1" title="1"><span class="kw">plot</span>(<span class="kw">cumsum</span>(pve), <span class="dt">xlab=</span><span class="st">&quot;Principal Component&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Cumulative Proportion of Variance Explained&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="dt">type=</span><span class="st">&#39;b&#39;</span>)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-222-1.png" width="576" /></p>
<div class="sourceCode" id="cb419"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb419-1" title="1">a=<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">8</span>,<span class="op">-</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb419-2" title="2"><span class="kw">cumsum</span>(a)</a></code></pre></div>
<pre><code>## [1]  1  3 11  8</code></pre>
</div>
<div id="example-pca-on-cancel-cells" class="section level3">
<h3><span class="header-section-number">18.1.2</span> Example: PCA on Cancel Cells</h3>
<p>The data <code>NCI60</code> contains expression levels of 6,830 genes from 64 cancel cell lines. Cancer type is also recorded.</p>
<div class="sourceCode" id="cb421"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb421-1" title="1"><span class="kw">library</span>(ISLR)</a>
<a class="sourceLine" id="cb421-2" title="2">nci_labs=NCI60<span class="op">$</span>labs</a>
<a class="sourceLine" id="cb421-3" title="3">nci_data=NCI60<span class="op">$</span>data</a></code></pre></div>
<p>We first perform PCA on the data after scaling the variables (genes) to
have standard deviation one, although one could reasonably argue that it
is better not to scale the genes:</p>
<div class="sourceCode" id="cb422"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb422-1" title="1">pca =<span class="st"> </span><span class="kw">prcomp</span>(nci_data, <span class="dt">scale=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<p>We now plot the first few principal component score vectors, in order to
visualize the data. The observations (cell lines) corresponding to a given
cancer type will be plotted in the same color, so that we can see to what
extent the observations within a cancer type are similar to each other. We
first create a simple function that assigns a distinct color to each element
of a numeric vector. The function will be used to assign a color to each of
the 64 cell lines, based on the cancer type to which it corresponds.
We‚Äôll make use of the <code>rainbow()</code> function, which takes as its argument a positive integer,
and returns a vector containing that number of distinct colors.</p>
<div class="sourceCode" id="cb423"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb423-1" title="1">Cols=<span class="cf">function</span>(vec){</a>
<a class="sourceLine" id="cb423-2" title="2">    cols=<span class="kw">rainbow</span>(<span class="kw">length</span>(<span class="kw">unique</span>(vec)))</a>
<a class="sourceLine" id="cb423-3" title="3">    <span class="kw">return</span>(cols[<span class="kw">as.numeric</span>(<span class="kw">as.factor</span>(vec))])</a>
<a class="sourceLine" id="cb423-4" title="4">  }</a></code></pre></div>
<p>We now can plot the principal component score vectors:</p>
<div class="sourceCode" id="cb424"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb424-1" title="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb424-2" title="2"><span class="kw">plot</span>(pca<span class="op">$</span>x[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>], <span class="dt">col=</span><span class="kw">Cols</span>(nci_labs), <span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">xlab=</span><span class="st">&quot;Z1&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Z2&quot;</span>)</a>
<a class="sourceLine" id="cb424-3" title="3"><span class="kw">plot</span>(pca<span class="op">$</span>x[,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>)], <span class="dt">col=</span><span class="kw">Cols</span>(nci_labs), <span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">xlab=</span><span class="st">&quot;Z1&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Z3&quot;</span>)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-226-1.png" width="576" /></p>
<p>On the whole, cell lines corresponding to a single cancer type do tend to have similar values on the
first few principal component score vectors. This indicates that cell lines
from the same cancer type tend to have pretty similar gene expression
levels.</p>
<p>We can obtain a summary of the proportion of variance explained (PVE)
of the first few principal components using the <code>summary()</code> method for a
<code>prcomp</code> object:</p>
<div class="sourceCode" id="cb425"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb425-1" title="1"><span class="kw">summary</span>(pca)</a></code></pre></div>
<pre><code>## Importance of components:
##                            PC1      PC2      PC3      PC4      PC5      PC6
## Standard deviation     27.8535 21.48136 19.82046 17.03256 15.97181 15.72108
## Proportion of Variance  0.1136  0.06756  0.05752  0.04248  0.03735  0.03619
## Cumulative Proportion   0.1136  0.18115  0.23867  0.28115  0.31850  0.35468
##                             PC7      PC8      PC9     PC10     PC11     PC12
## Standard deviation     14.47145 13.54427 13.14400 12.73860 12.68672 12.15769
## Proportion of Variance  0.03066  0.02686  0.02529  0.02376  0.02357  0.02164
## Cumulative Proportion   0.38534  0.41220  0.43750  0.46126  0.48482  0.50646
##                            PC13     PC14     PC15     PC16     PC17     PC18
## Standard deviation     11.83019 11.62554 11.43779 11.00051 10.65666 10.48880
## Proportion of Variance  0.02049  0.01979  0.01915  0.01772  0.01663  0.01611
## Cumulative Proportion   0.52695  0.54674  0.56590  0.58361  0.60024  0.61635
##                            PC19    PC20     PC21    PC22    PC23    PC24
## Standard deviation     10.43518 10.3219 10.14608 10.0544 9.90265 9.64766
## Proportion of Variance  0.01594  0.0156  0.01507  0.0148 0.01436 0.01363
## Cumulative Proportion   0.63229  0.6479  0.66296  0.6778 0.69212 0.70575
##                           PC25    PC26    PC27   PC28    PC29    PC30    PC31
## Standard deviation     9.50764 9.33253 9.27320 9.0900 8.98117 8.75003 8.59962
## Proportion of Variance 0.01324 0.01275 0.01259 0.0121 0.01181 0.01121 0.01083
## Cumulative Proportion  0.71899 0.73174 0.74433 0.7564 0.76824 0.77945 0.79027
##                           PC32    PC33    PC34    PC35    PC36    PC37    PC38
## Standard deviation     8.44738 8.37305 8.21579 8.15731 7.97465 7.90446 7.82127
## Proportion of Variance 0.01045 0.01026 0.00988 0.00974 0.00931 0.00915 0.00896
## Cumulative Proportion  0.80072 0.81099 0.82087 0.83061 0.83992 0.84907 0.85803
##                           PC39    PC40    PC41   PC42    PC43   PC44    PC45
## Standard deviation     7.72156 7.58603 7.45619 7.3444 7.10449 7.0131 6.95839
## Proportion of Variance 0.00873 0.00843 0.00814 0.0079 0.00739 0.0072 0.00709
## Cumulative Proportion  0.86676 0.87518 0.88332 0.8912 0.89861 0.9058 0.91290
##                          PC46    PC47    PC48    PC49    PC50    PC51    PC52
## Standard deviation     6.8663 6.80744 6.64763 6.61607 6.40793 6.21984 6.20326
## Proportion of Variance 0.0069 0.00678 0.00647 0.00641 0.00601 0.00566 0.00563
## Cumulative Proportion  0.9198 0.92659 0.93306 0.93947 0.94548 0.95114 0.95678
##                           PC53    PC54    PC55    PC56    PC57   PC58    PC59
## Standard deviation     6.06706 5.91805 5.91233 5.73539 5.47261 5.2921 5.02117
## Proportion of Variance 0.00539 0.00513 0.00512 0.00482 0.00438 0.0041 0.00369
## Cumulative Proportion  0.96216 0.96729 0.97241 0.97723 0.98161 0.9857 0.98940
##                           PC60    PC61    PC62    PC63      PC64
## Standard deviation     4.68398 4.17567 4.08212 4.04124 2.148e-14
## Proportion of Variance 0.00321 0.00255 0.00244 0.00239 0.000e+00
## Cumulative Proportion  0.99262 0.99517 0.99761 1.00000 1.000e+00</code></pre>
<p>Using the <code>plot()</code> function, we can also plot the variance explained by the
first few principal components:</p>
<div class="sourceCode" id="cb427"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb427-1" title="1"><span class="kw">plot</span>(pca)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-228-1.png" width="576" /></p>
<p>Note that the height of each bar in the bar plot is given by squaring the
corresponding element of <code>pr.out\$sdev</code>. However, it is generally more informative to
plot the PVE of each principal component (i.e.¬†a <strong>scree plot</strong>) and the cumulative
PVE of each principal component. This can be done with just a
little tweaking:</p>
<div class="sourceCode" id="cb428"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb428-1" title="1">pve =<span class="st"> </span><span class="dv">100</span><span class="op">*</span>pca<span class="op">$</span>sdev<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="kw">sum</span>(pca<span class="op">$</span>sdev<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb428-2" title="2"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb428-3" title="3"><span class="kw">plot</span>(pve,  <span class="dt">type=</span><span class="st">&quot;o&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;PVE&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Principal Component&quot;</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</a>
<a class="sourceLine" id="cb428-4" title="4"><span class="kw">plot</span>(<span class="kw">cumsum</span>(pve), <span class="dt">type=</span><span class="st">&quot;o&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Cumulative PVE&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Principal Component&quot;</span>, <span class="dt">col=</span><span class="st">&quot;brown3&quot;</span>)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-229-1.png" width="576" /></p>
<p>We see that together, the first seven principal components
explain around 40% of the variance in the data. This is not a huge amount
of the variance. However, looking at the scree plot, we see that while each
of the first seven principal components explain a substantial amount of
variance, there is a marked decrease in the variance explained by further
principal components. That is, there is an <strong>elbow</strong> in the plot after approximately
the seventh principal component. This suggests that there may
be little benefit to examining more than seven or so principal components
(phew! even examining seven principal components may be difficult).</p>
</div>
</div>
<div id="clustering" class="section level2">
<h2><span class="header-section-number">18.2</span> Clustering</h2>
<p>Imagine that you are a large retailer interested in understanding the customer base. There may be several ‚Äútypes‚Äù of customers, such as those shopping for business with corporate accounts, those shopping for leisure, or debt-strapped grad students. Each of these customers would exhibit different behavior, and should be treated differently statistically. But how can a customer‚Äôs ‚Äútype‚Äù be defined? Especially for large customer data sets in the millions, one can imagine how this problem can be challenging.</p>
<p>Clustering algorithms look for groups of observations which are similar to one another. Because there is no target variable, measuring the quality of the ‚Äúfit‚Äù is much more complicated. There are many clustering algorithms, but this exam only focuses on the two that are most common.</p>
<div id="k-means-clustering" class="section level3">
<h3><span class="header-section-number">18.2.1</span> K-Means Clustering</h3>
<p>Kmeans takes continuous data and assigns observations into k clusters, or groups. In the two-dimensional example, this is the same as drawing lines around points.</p>
<iframe width="560" height="315" src="https://www.youtube.com/watch?v=4b5d3muPQmA&amp;t=268s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>StatQuest. ‚Äú K-means clustering‚Äù YouTube, Joshua Starmer, 2 Apr 2018, <a href="https://www.youtube.com/watch?v=4b5d3muPQmA&amp;t=268s" class="uri">https://www.youtube.com/watch?v=4b5d3muPQmA&amp;t=268s</a></p>
<p>Kmeans consists of the following steps:</p>
<p><strong>a)</strong> Start with two variables (<span class="math inline">\(X_1\)</span> on the X-axis, and <span class="math inline">\(X_2\)</span> on the Y-axis.)
<strong>b)</strong> Randomly assign cluster centers.
<strong>c)</strong> Put each point into the cluster that is closest.
<strong>d) - f)</strong> Move the cluster center to the mean of the points assigned to it and continue until the centers stop moving.
<strong>g)</strong> Repeated steps a) - f) a given number of times (controlled by <code>n.starts</code>). This reduces the uncertainty from choosing the initial centers randomly.</p>
<p><img src="images/kmeans.png" width="1000%" /></p>
<div id="r-example" class="section level4">
<h4><span class="header-section-number">18.2.1.1</span> R Example</h4>
<p>In <code>R</code>, the function <code>kmeans()</code> performs K-means clustering in R. We begin with
a simple simulated example in which there truly are two clusters in the
data: the first 25 observations have a mean shift relative to the next 25
observations.</p>
<div class="sourceCode" id="cb429"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb429-1" title="1"><span class="kw">set.seed</span>(<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb429-2" title="2">x =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">50</span><span class="op">*</span><span class="dv">2</span>), <span class="dt">ncol =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb429-3" title="3">x[<span class="dv">1</span><span class="op">:</span><span class="dv">25</span>,<span class="dv">1</span>] =<span class="st"> </span>x[<span class="dv">1</span><span class="op">:</span><span class="dv">25</span>,<span class="dv">1</span>]<span class="op">+</span><span class="dv">3</span></a>
<a class="sourceLine" id="cb429-4" title="4">x[<span class="dv">1</span><span class="op">:</span><span class="dv">25</span>,<span class="dv">2</span>] =<span class="st"> </span>x[<span class="dv">1</span><span class="op">:</span><span class="dv">25</span>,<span class="dv">2</span>]<span class="op">-</span><span class="dv">4</span></a></code></pre></div>
<p>We now perform K-means clustering with <code>K  =  2</code>:</p>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb430-1" title="1">km_out =<span class="st"> </span><span class="kw">kmeans</span>(x,<span class="dv">2</span>,<span class="dt">nstart =</span> <span class="dv">20</span>)</a></code></pre></div>
<p>The cluster assignments of the 50 observations are contained in
<code>km_out$cluster</code>:</p>
<div class="sourceCode" id="cb431"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb431-1" title="1">km_out<span class="op">$</span>cluster</a></code></pre></div>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2
## [39] 2 2 2 2 2 2 2 2 2 2 2 2</code></pre>
<p>The K-means clustering perfectly separated the observations into two clusters
even though we did not supply any group information to <code>kmeans()</code>. We
can plot the data, with each observation colored according to its cluster
assignment:</p>
<div class="sourceCode" id="cb433"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb433-1" title="1"><span class="kw">plot</span>(x, <span class="dt">col =</span> (km_out<span class="op">$</span>cluster<span class="op">+</span><span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;K-Means Clustering Results with K = 2&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">pch =</span> <span class="dv">20</span>, <span class="dt">cex =</span> <span class="dv">2</span>)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-234-1.png" width="576" /></p>
<p>Here the observations can be easily plotted because they are two-dimensional.
If there were more than two variables then we could instead perform PCA
and plot the first two principal components score vectors.</p>
<p>In this example, we knew that there really were two clusters because
we generated the data. However, for real data, in general we do not know
the true number of clusters. We could instead have performed K-means
clustering on this example with <code>K  =  3</code>. If we do this, K-means clustering will split up the two ‚Äúreal‚Äù clusters, since it has no information about them:</p>
<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb434-1" title="1"><span class="kw">set.seed</span>(<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb434-2" title="2">km_out =<span class="st"> </span><span class="kw">kmeans</span>(x, <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb434-3" title="3">km_out</a></code></pre></div>
<pre><code>## K-means clustering with 3 clusters of sizes 17, 23, 10
## 
## Cluster means:
##         [,1]        [,2]
## 1  3.7789567 -4.56200798
## 2 -0.3820397 -0.08740753
## 3  2.3001545 -2.69622023
## 
## Clustering vector:
##  [1] 1 3 1 3 1 1 1 3 1 3 1 3 1 3 1 3 1 1 1 1 1 3 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2
## [39] 2 2 2 2 2 3 2 3 2 2 2 2
## 
## Within cluster sum of squares by cluster:
## [1] 25.74089 52.67700 19.56137
##  (between_SS / total_SS =  79.3 %)
## 
## Available components:
## 
## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;     &quot;tot.withinss&quot;
## [6] &quot;betweenss&quot;    &quot;size&quot;         &quot;iter&quot;         &quot;ifault&quot;</code></pre>
<div class="sourceCode" id="cb436"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb436-1" title="1"><span class="kw">plot</span>(x, <span class="dt">col =</span> (km_out<span class="op">$</span>cluster<span class="op">+</span><span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;K-Means Clustering Results with K = 3&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">pch =</span> <span class="dv">20</span>, <span class="dt">cex =</span> <span class="dv">2</span>)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-235-1.png" width="576" /></p>
<p>To run the <code>kmeans()</code> function in R with multiple initial cluster assignments,
we use the <code>nstart</code> argument. If a value of <code>nstart</code> greater than one
is used, then K-means clustering will be performed using multiple random
assignments, and the <code>kmeans()</code> function will
report only the best results. Here we compare using <code>nstart = 1</code>:</p>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb437-1" title="1"><span class="kw">set.seed</span>(<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb437-2" title="2">km_out =<span class="st"> </span><span class="kw">kmeans</span>(x, <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb437-3" title="3">km_out<span class="op">$</span>tot.withinss</a></code></pre></div>
<pre><code>## [1] 97.97927</code></pre>
<p>to <code>nstart = 20</code>:</p>
<div class="sourceCode" id="cb439"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb439-1" title="1">km_out =<span class="st"> </span><span class="kw">kmeans</span>(x,<span class="dv">3</span>,<span class="dt">nstart =</span> <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb439-2" title="2">km_out<span class="op">$</span>tot.withinss</a></code></pre></div>
<pre><code>## [1] 97.97927</code></pre>
<p>Note that <code>km_out\$tot.withinss</code> is the total within-cluster sum of squares,
which we seek to minimize by performing K-means clustering. The individual within-cluster sum-of-squares are contained in the
vector <code>km_out\$withinss</code>.</p>
<p>It is generally recommended to always run K-means clustering with a large
value of <code>nstart</code>, such as 20 or 50 to avoid getting stuck in an undesirable local
optimum.</p>
<p>When performing K-means clustering, in addition to using multiple initial
cluster assignments, it is also important to set a random seed using the
<code>set.seed()</code> function. This way, the initial cluster assignments can
be replicated, and the K-means output will be fully reproducible.</p>
</div>
</div>
</div>
<div id="hierarchical-clustering" class="section level2">
<h2><span class="header-section-number">18.3</span> Hierarchical Clustering</h2>
<p>Kmeans required that we choose the number of clusters, k. Hierarchical clustering is an alternative that does not require that we choose only one value of k.</p>
<iframe width="560" height="315" src="https://www.youtube.com/watch?v=7xHsRkOdVwo&amp;t=137s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>StatQuest. ‚Äú Hierarchical Clustering‚Äù YouTube, Joshua Starmer, 20 Jun, 2017, <a href="https://www.youtube.com/watch?v=7xHsRkOdVwo&amp;t=137s" class="uri">https://www.youtube.com/watch?v=7xHsRkOdVwo&amp;t=137s</a></p>
<p>The most common type of hierarchical clustering uses a <em>bottom-up</em> approach. This starts with a single <strong>observation</strong> and then looks for others which are close and puts them into a cluster. Then it looks for other <strong>clusters</strong> that are similar and groups these together into a <strong>mega cluster</strong>. It continues to do this until all observations are in the same group.</p>
<p>This is analyzed with a graph called a dendrogram (dendro = tree, gram = graph). The height represents ‚Äúdistance‚Äù, or how similar the clusters are to one another. The clusters on the bottom, which are vertically close to one another, have similar data values; the clusters that are further apart vertically are less similar.</p>
<p>Choosing the value of the cutoff height changes the number of clusters that result.</p>
<p><img src="images/HClustering.png" width="1000%" style="display: block; margin: auto;" /></p>
<p>Certain data have a natural hierarchical structure. For example, say that the variables are City, Town, State, Country, and Continent. If we used hierarchical clustering, this pattern could be established even if we did not have labels for Cities, Towns, and so forth.</p>
<p>The <code>hclust()</code> function implements hierarchical clustering in R. In the following example we use the data from the previous section to plot the hierarchical
clustering dendrogram using complete, single, and average linkage clustering,
with Euclidean distance as the dissimilarity measure. We begin by
clustering observations using complete linkage. The <code>dist()</code> function is used
to compute the 50 <span class="math inline">\(\times\)</span> 50 inter-observation Euclidean distance matrix:</p>
<div class="sourceCode" id="cb441"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb441-1" title="1">hc_complete =<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(x), <span class="dt">method =</span> <span class="st">&quot;complete&quot;</span>)</a></code></pre></div>
<p>We could just as easily perform hierarchical clustering with average or
single linkage instead:</p>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb442-1" title="1">hc_average =<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(x), <span class="dt">method =</span> <span class="st">&quot;average&quot;</span>)</a>
<a class="sourceLine" id="cb442-2" title="2">hc_single =<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(x), <span class="dt">method =</span> <span class="st">&quot;single&quot;</span>)</a></code></pre></div>
<p>We can now plot the dendrograms obtained using the usual <code>plot()</code> function.
The numbers at the bottom of the plot identify each observation:</p>
<div class="sourceCode" id="cb443"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb443-1" title="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</a>
<a class="sourceLine" id="cb443-2" title="2"><span class="kw">plot</span>(hc_complete,<span class="dt">main =</span> <span class="st">&quot;Complete Linkage&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">sub =</span> <span class="st">&quot;&quot;</span>, <span class="dt">cex =</span> <span class="fl">.9</span>)</a>
<a class="sourceLine" id="cb443-3" title="3"><span class="kw">plot</span>(hc_average, <span class="dt">main =</span> <span class="st">&quot;Average Linkage&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">sub =</span> <span class="st">&quot;&quot;</span>, <span class="dt">cex =</span> <span class="fl">.9</span>)</a>
<a class="sourceLine" id="cb443-4" title="4"><span class="kw">plot</span>(hc_single, <span class="dt">main =</span> <span class="st">&quot;Single Linkage&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">sub =</span> <span class="st">&quot;&quot;</span>, <span class="dt">cex =</span> <span class="fl">.9</span>)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-241-1.png" width="576" /></p>
<p>To determine the cluster labels for each observation associated with a
given cut of the dendrogram, we can use the <code>cutree()</code> function:</p>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb444-1" title="1"><span class="kw">cutree</span>(hc_complete, <span class="dv">2</span>)</a></code></pre></div>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2
## [39] 2 2 2 2 2 2 2 2 2 2 2 2</code></pre>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb446-1" title="1"><span class="kw">cutree</span>(hc_average, <span class="dv">2</span>)</a></code></pre></div>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 2 2 2 2 2
## [39] 2 2 2 2 2 1 2 1 2 2 2 2</code></pre>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb448-1" title="1"><span class="kw">cutree</span>(hc_single, <span class="dv">2</span>)</a></code></pre></div>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
## [39] 1 1 1 1 1 1 1 1 1 1 1 1</code></pre>
<p>For this data, complete and average linkage generally separate the observations
into their correct groups. However, single linkage identifies one point
as belonging to its own cluster. A more sensible answer is obtained when
four clusters are selected, although there are still two singletons:</p>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb450-1" title="1"><span class="kw">cutree</span>(hc_single, <span class="dv">4</span>)</a></code></pre></div>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3
## [39] 3 3 3 4 3 3 3 3 3 3 3 3</code></pre>
<p>To scale the variables before performing hierarchical clustering of the
observations, we can use the <code>scale()</code> function:</p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb452-1" title="1">xsc =<span class="st"> </span><span class="kw">scale</span>(x)</a>
<a class="sourceLine" id="cb452-2" title="2"><span class="kw">plot</span>(<span class="kw">hclust</span>(<span class="kw">dist</span>(xsc), <span class="dt">method =</span> <span class="st">&quot;complete&quot;</span>), <span class="dt">main =</span> <span class="st">&quot;Hierarchical Clustering with Scaled Features&quot;</span>)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-244-1.png" width="576" /></p>
<p>Correlation-based distance can be computed using the <code>as.dist()</code> function, which converts an arbitrary square symmetric matrix into a form that
the <code>hclust()</code> function recognizes as a distance matrix. However, this only
makes sense for data with <strong>at least three features</strong> since the absolute correlation
between any two observations with measurements on two features is
always 1. Let‚Äôs generate and cluster a three-dimensional data set:</p>
<div class="sourceCode" id="cb453"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb453-1" title="1">x =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">30</span><span class="op">*</span><span class="dv">3</span>), <span class="dt">ncol =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb453-2" title="2">dd =<span class="st"> </span><span class="kw">as.dist</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">cor</span>(<span class="kw">t</span>(x)))</a>
<a class="sourceLine" id="cb453-3" title="3"><span class="kw">plot</span>(<span class="kw">hclust</span>(dd, <span class="dt">method =</span> <span class="st">&quot;complete&quot;</span>), <span class="dt">main =</span> <span class="st">&quot;Complete Linkage with Correlation-Based Distance&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">sub =</span> <span class="st">&quot;&quot;</span>)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-245-1.png" width="576" /></p>
<div id="example-clustering-cancel-cells" class="section level3">
<h3><span class="header-section-number">18.3.1</span> Example: Clustering Cancel Cells</h3>
<p>Unsupervised techniques are often used in the analysis of genomic data. In this example, we‚Äôll see how hierarchical and K-means clustering compare on the <code>NCI60</code> cancer cell line micro array data, which
consists of 6,830 gene expression measurements on 64 cancer cell lines:</p>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb454-1" title="1"><span class="co"># The NCI60 data</span></a>
<a class="sourceLine" id="cb454-2" title="2"><span class="kw">library</span>(ISLR)</a>
<a class="sourceLine" id="cb454-3" title="3">nci_labels =<span class="st"> </span>NCI60<span class="op">$</span>labs</a>
<a class="sourceLine" id="cb454-4" title="4">nci_data =<span class="st"> </span>NCI60<span class="op">$</span>data</a></code></pre></div>
<p>Each cell line is labeled with a cancer type. We‚Äôll ignore the
cancer types in performing clustering, as these are unsupervised
techniques. After performing clustering, we‚Äôll use this column to see the extent to which these cancer types agree with the results of these
unsupervised techniques.</p>
<p>The data has 64 rows and 6,830 columns.</p>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb455-1" title="1"><span class="kw">dim</span>(nci_data)</a></code></pre></div>
<pre><code>## [1]   64 6830</code></pre>
<p>Let‚Äôs take a look at the cancer types for the cell lines:</p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb457-1" title="1"><span class="kw">table</span>(nci_labels)</a></code></pre></div>
<pre><code>## nci_labels
##      BREAST         CNS       COLON K562A-repro K562B-repro    LEUKEMIA 
##           7           5           7           1           1           6 
## MCF7A-repro MCF7D-repro    MELANOMA       NSCLC     OVARIAN    PROSTATE 
##           1           1           8           9           6           2 
##       RENAL     UNKNOWN 
##           9           1</code></pre>
<p>We now proceed to hierarchically cluster the cell lines in the <code>NCI60</code> data,
with the goal of finding out whether or not the observations cluster into
distinct types of cancer. To begin, we standardize the variables to have
mean zero and standard deviation one. This step is
optional, and need only be performed if we want each gene to be on the
same scale:</p>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb459-1" title="1">sd_data =<span class="st"> </span><span class="kw">scale</span>(nci_data)</a></code></pre></div>
<p>We now perform hierarchical clustering of the observations using complete,
single, and average linkage. We‚Äôll use standard Euclidean distance as the dissimilarity
measure:</p>
<div class="sourceCode" id="cb460"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb460-1" title="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</a>
<a class="sourceLine" id="cb460-2" title="2">data_dist =<span class="st"> </span><span class="kw">dist</span>(sd_data)</a>
<a class="sourceLine" id="cb460-3" title="3"><span class="kw">plot</span>(<span class="kw">hclust</span>(data_dist), <span class="dt">labels =</span> nci_labels, <span class="dt">main =</span> <span class="st">&quot;Complete Linkage&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">sub =</span> <span class="st">&quot;&quot;</span>,<span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb460-4" title="4"><span class="kw">plot</span>(<span class="kw">hclust</span>(data_dist, <span class="dt">method =</span> <span class="st">&quot;average&quot;</span>), <span class="dt">labels =</span> nci_labels, <span class="dt">main =</span> <span class="st">&quot;Average Linkage&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">sub =</span> <span class="st">&quot;&quot;</span>,<span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb460-5" title="5"><span class="kw">plot</span>(<span class="kw">hclust</span>(data_dist, <span class="dt">method =</span> <span class="st">&quot;single&quot;</span>), <span class="dt">labels =</span> nci_labels,  <span class="dt">main =</span> <span class="st">&quot;Single Linkage&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">sub =</span> <span class="st">&quot;&quot;</span>,<span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-250-1.png" width="576" /></p>
<p>We see that the choice of linkage
certainly does affect the results obtained. Typically, single linkage will tend
to yield trailing clusters: very large clusters onto which individual observations
attach one-by-one. On the other hand, complete and average linkage
tend to yield more balanced, attractive clusters. For this reason, complete
and average linkage are generally preferred to single linkage. Clearly cell
lines within a single cancer type do tend to cluster together, although the
clustering is not perfect.</p>
<p>Let‚Äôs use our complete linkage hierarchical clustering
for the analysis. We can cut the dendrogram at the height that will yield a particular
number of clusters, say 4:</p>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb461-1" title="1">hc_out =<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(sd_data))</a>
<a class="sourceLine" id="cb461-2" title="2">hc_clusters =<span class="st"> </span><span class="kw">cutree</span>(hc_out,<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb461-3" title="3"><span class="kw">table</span>(hc_clusters,nci_labels)</a></code></pre></div>
<pre><code>##            nci_labels
## hc_clusters BREAST CNS COLON K562A-repro K562B-repro LEUKEMIA MCF7A-repro
##           1      2   3     2           0           0        0           0
##           2      3   2     0           0           0        0           0
##           3      0   0     0           1           1        6           0
##           4      2   0     5           0           0        0           1
##            nci_labels
## hc_clusters MCF7D-repro MELANOMA NSCLC OVARIAN PROSTATE RENAL UNKNOWN
##           1           0        8     8       6        2     8       1
##           2           0        0     1       0        0     1       0
##           3           0        0     0       0        0     0       0
##           4           1        0     0       0        0     0       0</code></pre>
<p>There are some clear patterns. All the leukemia cell lines fall in cluster 3,
while the breast cancer cell lines are spread out over three different clusters.
We can plot the cut on the dendrogram that produces these four clusters using the <code>abline()</code> function, which draws a straight line on top of any existing plot in R:</p>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb463-1" title="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb463-2" title="2"><span class="kw">plot</span>(hc_out, <span class="dt">labels =</span> nci_labels)</a>
<a class="sourceLine" id="cb463-3" title="3"><span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">139</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-252-1.png" width="576" /></p>
<p>Printing the output of <code>hclust</code> gives a useful brief summary of the object:</p>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb464-1" title="1">hc_out</a></code></pre></div>
<pre><code>## 
## Call:
## hclust(d = dist(sd_data))
## 
## Cluster method   : complete 
## Distance         : euclidean 
## Number of objects: 64</code></pre>
<p>We claimed earlier that K-means clustering and hierarchical
clustering with the dendrogram cut to obtain the same number
of clusters can yield <strong>very</strong> different results. How do these <code>NCI60</code> hierarchical
clustering results compare to what we get if we perform K-means clustering
with <code>K  =  4</code>?</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb466-1" title="1"><span class="kw">set.seed</span>(<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb466-2" title="2">km_out =<span class="st"> </span><span class="kw">kmeans</span>(sd_data, <span class="dv">4</span>, <span class="dt">nstart =</span> <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb466-3" title="3">km_clusters =<span class="st"> </span>km_out<span class="op">$</span>cluster</a></code></pre></div>
<p>We can use a confusion matrix to compare the differences in how the two methods assigned observations to clusters:</p>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb467-1" title="1"><span class="kw">table</span>(km_clusters,hc_clusters)</a></code></pre></div>
<pre><code>##            hc_clusters
## km_clusters  1  2  3  4
##           1 11  0  0  9
##           2 20  7  0  0
##           3  9  0  0  0
##           4  0  0  8  0</code></pre>
<p>We see that the four clusters obtained using hierarchical clustering and Kmeans
clustering are somewhat different. Cluster 2 in K-means clustering is
identical to cluster 3 in hierarchical clustering. However, the other clusters
differ: for instance, cluster 4 in K-means clustering contains a portion of
the observations assigned to cluster 1 by hierarchical clustering, as well as
all of the observations assigned to cluster 2 by hierarchical clustering.</p>
</div>
<div id="references-1" class="section level3">
<h3><span class="header-section-number">18.3.2</span> References</h3>
<p>These examples are an adaptation of p.¬†404-407, 410-413 of ‚ÄúIntroduction to
Statistical Learning with Applications in R‚Äù by Gareth James, Daniela Witten, Trevor Hastie and Robert
Tibshirani. Adapted by R. Jordan Crouser at Smith College for SDS293: Machine Learning (Spring 2016), and re-implemented in Fall 2016 in <code>tidyverse</code> format by Amelia McNamara and R. Jordan Crouser at Smith College.</p>
<p>Used with permission from Jordan Crouser at Smith College, and to the following contributors on github:</p>
<ul>
<li>github.com/jcrouser</li>
<li>github.com/AmeliaMN</li>
<li>github.com/mhusseinmidd</li>
<li>github.com/rudeboybert</li>
<li>github.com/ijlyttle</li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tree-based-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references-2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/sdcastillo/PA-R-Study-Manual/edit/master/07-unsupervised-learning.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Exam-PA-Study-Manual.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
