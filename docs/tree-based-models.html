<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 9 Tree-based models | The Predictive Analytics R Study Manual</title>
  <meta name="description" content="This will help you pass these exams" />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content=" 9 Tree-based models | The Predictive Analytics R Study Manual" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This will help you pass these exams" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 9 Tree-based models | The Predictive Analytics R Study Manual" />
  
  <meta name="twitter:description" content="This will help you pass these exams" />
  

<meta name="author" content="Sam Castillo" />


<meta name="date" content="2019-10-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-models.html"/>
<link rel="next" href="a-mini-exam-example.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Exam PA Study Manual</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome</a></li>
<li class="chapter" data-level="2" data-path="the-exam.html"><a href="the-exam.html"><i class="fa fa-check"></i><b>2</b> The Exam</a></li>
<li class="chapter" data-level="3" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>3</b> What is machine learning?</a></li>
<li class="chapter" data-level="4" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>4</b> Getting started</a><ul>
<li class="chapter" data-level="4.1" data-path="getting-started.html"><a href="getting-started.html#download-islr"><i class="fa fa-check"></i><b>4.1</b> Download ISLR</a></li>
<li class="chapter" data-level="4.2" data-path="getting-started.html"><a href="getting-started.html#installing-r"><i class="fa fa-check"></i><b>4.2</b> Installing R</a></li>
<li class="chapter" data-level="4.3" data-path="getting-started.html"><a href="getting-started.html#installing-rstudio"><i class="fa fa-check"></i><b>4.3</b> Installing RStudio</a></li>
<li class="chapter" data-level="4.4" data-path="getting-started.html"><a href="getting-started.html#set-the-r-library"><i class="fa fa-check"></i><b>4.4</b> Set the R library</a></li>
<li class="chapter" data-level="4.5" data-path="getting-started.html"><a href="getting-started.html#download-the-data"><i class="fa fa-check"></i><b>4.5</b> Download the data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="r-programming.html"><a href="r-programming.html"><i class="fa fa-check"></i><b>5</b> R programming</a><ul>
<li class="chapter" data-level="5.1" data-path="r-programming.html"><a href="r-programming.html#notebook-chunks"><i class="fa fa-check"></i><b>5.1</b> Notebook chunks</a></li>
<li class="chapter" data-level="5.2" data-path="r-programming.html"><a href="r-programming.html#basic-operations"><i class="fa fa-check"></i><b>5.2</b> Basic operations</a></li>
<li class="chapter" data-level="5.3" data-path="r-programming.html"><a href="r-programming.html#lists"><i class="fa fa-check"></i><b>5.3</b> Lists</a></li>
<li class="chapter" data-level="5.4" data-path="r-programming.html"><a href="r-programming.html#functions"><i class="fa fa-check"></i><b>5.4</b> Functions</a></li>
<li class="chapter" data-level="5.5" data-path="r-programming.html"><a href="r-programming.html#data-frames"><i class="fa fa-check"></i><b>5.5</b> Data frames</a></li>
<li class="chapter" data-level="5.6" data-path="r-programming.html"><a href="r-programming.html#pipes"><i class="fa fa-check"></i><b>5.6</b> Pipes</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-manipulation.html"><a href="data-manipulation.html"><i class="fa fa-check"></i><b>6</b> Data manipulation</a><ul>
<li class="chapter" data-level="6.1" data-path="data-manipulation.html"><a href="data-manipulation.html#look-at-the-data"><i class="fa fa-check"></i><b>6.1</b> Look at the data</a></li>
<li class="chapter" data-level="6.2" data-path="data-manipulation.html"><a href="data-manipulation.html#transform-the-data"><i class="fa fa-check"></i><b>6.2</b> Transform the data</a></li>
<li class="chapter" data-level="6.3" data-path="data-manipulation.html"><a href="data-manipulation.html#exercises"><i class="fa fa-check"></i><b>6.3</b> Exercises</a></li>
<li class="chapter" data-level="6.4" data-path="data-manipulation.html"><a href="data-manipulation.html#answers-to-exercises"><i class="fa fa-check"></i><b>6.4</b> Answers to exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i><b>7</b> Visualization</a><ul>
<li class="chapter" data-level="7.1" data-path="visualization.html"><a href="visualization.html#create-a-plot-object-ggplot"><i class="fa fa-check"></i><b>7.1</b> Create a plot object (ggplot)</a></li>
<li class="chapter" data-level="7.2" data-path="visualization.html"><a href="visualization.html#add-a-plot"><i class="fa fa-check"></i><b>7.2</b> Add a plot</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>8</b> Linear Models</a><ul>
<li class="chapter" data-level="8.1" data-path="linear-models.html"><a href="linear-models.html#model-notation"><i class="fa fa-check"></i><b>8.1</b> Model Notation</a></li>
<li class="chapter" data-level="8.2" data-path="linear-models.html"><a href="linear-models.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>8.2</b> Ordinary least squares (OLS)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="linear-models.html"><a href="linear-models.html#example"><i class="fa fa-check"></i><b>8.2.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="linear-models.html"><a href="linear-models.html#generalized-linear-models-glms"><i class="fa fa-check"></i><b>8.3</b> Generalized linear models (GLMs)</a><ul>
<li class="chapter" data-level="8.3.1" data-path="linear-models.html"><a href="linear-models.html#example-1"><i class="fa fa-check"></i><b>8.3.1</b> Example</a></li>
<li class="chapter" data-level="8.3.2" data-path="linear-models.html"><a href="linear-models.html#reference-levels"><i class="fa fa-check"></i><b>8.3.2</b> Reference levels</a></li>
<li class="chapter" data-level="8.3.3" data-path="linear-models.html"><a href="linear-models.html#interactions"><i class="fa fa-check"></i><b>8.3.3</b> Interactions</a></li>
<li class="chapter" data-level="8.3.4" data-path="linear-models.html"><a href="linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>8.3.4</b> Poisson Regression</a></li>
<li class="chapter" data-level="8.3.5" data-path="linear-models.html"><a href="linear-models.html#tweedie-regression"><i class="fa fa-check"></i><b>8.3.5</b> Tweedie regression</a></li>
<li class="chapter" data-level="8.3.6" data-path="linear-models.html"><a href="linear-models.html#stepwise-subset-selection"><i class="fa fa-check"></i><b>8.3.6</b> Stepwise subset selection</a></li>
<li class="chapter" data-level="8.3.7" data-path="linear-models.html"><a href="linear-models.html#advantages-and-disadvantages"><i class="fa fa-check"></i><b>8.3.7</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="linear-models.html"><a href="linear-models.html#penalized-linear-models"><i class="fa fa-check"></i><b>8.4</b> Penalized Linear Models</a><ul>
<li class="chapter" data-level="8.4.1" data-path="linear-models.html"><a href="linear-models.html#ridge-regression"><i class="fa fa-check"></i><b>8.4.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="8.4.2" data-path="linear-models.html"><a href="linear-models.html#lasso"><i class="fa fa-check"></i><b>8.4.2</b> Lasso</a></li>
<li class="chapter" data-level="8.4.3" data-path="linear-models.html"><a href="linear-models.html#elastic-net"><i class="fa fa-check"></i><b>8.4.3</b> Elastic Net</a></li>
<li class="chapter" data-level="8.4.4" data-path="linear-models.html"><a href="linear-models.html#advantages-and-disadvantages-1"><i class="fa fa-check"></i><b>8.4.4</b> Advantages and disadvantages</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="tree-based-models.html"><a href="tree-based-models.html"><i class="fa fa-check"></i><b>9</b> Tree-based models</a><ul>
<li class="chapter" data-level="9.1" data-path="tree-based-models.html"><a href="tree-based-models.html#decision-trees"><i class="fa fa-check"></i><b>9.1</b> Decision Trees</a></li>
<li class="chapter" data-level="9.2" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-2"><i class="fa fa-check"></i><b>9.2</b> Advantages and disadvantages</a></li>
<li class="chapter" data-level="9.3" data-path="tree-based-models.html"><a href="tree-based-models.html#ensemble-learning"><i class="fa fa-check"></i><b>9.3</b> Ensemble learning</a><ul>
<li class="chapter" data-level="9.3.1" data-path="tree-based-models.html"><a href="tree-based-models.html#bagging"><i class="fa fa-check"></i><b>9.3.1</b> Bagging</a></li>
<li class="chapter" data-level="9.3.2" data-path="tree-based-models.html"><a href="tree-based-models.html#boosting"><i class="fa fa-check"></i><b>9.3.2</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="tree-based-models.html"><a href="tree-based-models.html#random-forests"><i class="fa fa-check"></i><b>9.4</b> Random Forests</a></li>
<li class="chapter" data-level="9.5" data-path="tree-based-models.html"><a href="tree-based-models.html#gradient-boosted-trees"><i class="fa fa-check"></i><b>9.5</b> Gradient Boosted Trees</a><ul>
<li class="chapter" data-level="9.5.1" data-path="tree-based-models.html"><a href="tree-based-models.html#variable-importance"><i class="fa fa-check"></i><b>9.5.1</b> Variable importance</a></li>
<li class="chapter" data-level="9.5.2" data-path="tree-based-models.html"><a href="tree-based-models.html#partial-dependence"><i class="fa fa-check"></i><b>9.5.2</b> Partial dependence</a></li>
<li class="chapter" data-level="9.5.3" data-path="tree-based-models.html"><a href="tree-based-models.html#exercises-1"><i class="fa fa-check"></i><b>9.5.3</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="a-mini-exam-example.html"><a href="a-mini-exam-example.html"><i class="fa fa-check"></i><b>10</b> A Mini-Exam Example</a><ul>
<li class="chapter" data-level="10.1" data-path="a-mini-exam-example.html"><a href="a-mini-exam-example.html#project-statement"><i class="fa fa-check"></i><b>10.1</b> Project Statement</a><ul>
<li class="chapter" data-level="10.1.1" data-path="a-mini-exam-example.html"><a href="a-mini-exam-example.html#describe-the-data-1-point"><i class="fa fa-check"></i><b>10.1.1</b> Describe the data (1 point)</a></li>
<li class="chapter" data-level="10.1.2" data-path="a-mini-exam-example.html"><a href="a-mini-exam-example.html#create-a-histogram-of-the-claims-and-comment-on-the-shape-1-point"><i class="fa fa-check"></i><b>10.1.2</b> Create a histogram of the claims and comment on the shape (1 point)</a></li>
<li class="chapter" data-level="10.1.3" data-path="a-mini-exam-example.html"><a href="a-mini-exam-example.html#fit-a-linear-model-1-point"><i class="fa fa-check"></i><b>10.1.3</b> Fit a linear model (1 point)</a></li>
<li class="chapter" data-level="10.1.4" data-path="a-mini-exam-example.html"><a href="a-mini-exam-example.html#describe-the-relationship-between-age-sex-and-claim-costs-1-point"><i class="fa fa-check"></i><b>10.1.4</b> Describe the relationship between age, sex, and claim costs (1 point)</a></li>
<li class="chapter" data-level="10.1.5" data-path="a-mini-exam-example.html"><a href="a-mini-exam-example.html#write-a-summary-of-steps-1-4-in-non-technical-language-1-point"><i class="fa fa-check"></i><b>10.1.5</b> Write a summary of steps 1-4 in non-technical language (1 point)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="practice-exam.html"><a href="practice-exam.html"><i class="fa fa-check"></i><b>11</b> Practice Exam</a></li>
<li class="chapter" data-level="12" data-path="prior-exams.html"><a href="prior-exams.html"><i class="fa fa-check"></i><b>12</b> Prior Exams</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Predictive Analytics R Study Manual</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tree-based-models" class="section level1">
<h1><span class="header-section-number"> 9</span> Tree-based models</h1>
<div id="decision-trees" class="section level2">
<h2><span class="header-section-number">9.1</span> Decision Trees</h2>
<p>Decision trees can be used for either classification or regression problems. The model structure is a series of yes/no questions. Depending on how each observation answers these questions, a prediction is made.</p>
<p>The below example shows how a single tree can predict health claims.</p>
<ul>
<li>For non-smokers, the predicted annual claims are 8,434. This represents 80% of the observations</li>
<li>For smokers with a <code>bmi</code> of less than 30, the predicted annual claims are 21,000. 10% of patients fall into this bucket.</li>
<li>For smokers with a <code>bmi</code> of more than 30, the prediction is 42,000. This bucket accounts for 11% of patients.</li>
</ul>
<div class="figure"><span id="fig:unnamed-chunk-101"></span>
<img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-101-1.png" alt="Decision tree of health costs" width="672" />
<p class="caption">
Figure 9.1: Decision tree of health costs
</p>
</div>
<p>We can cut the data set up into these groups and look at the claim costs. From this grouping, we can see that <code>smoker</code> is the most important variable as the difference in average claims is about 20,000.</p>
<table>
<thead>
<tr class="header">
<th align="left">smoker</th>
<th align="left">bmi_30</th>
<th align="left">mean_claims</th>
<th align="right">percent</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">no</td>
<td align="left">bmi &lt; 30</td>
<td align="left">$7,977.03</td>
<td align="right">0.38</td>
</tr>
<tr class="even">
<td align="left">no</td>
<td align="left">bmi &gt;= 30</td>
<td align="left">$8,842.69</td>
<td align="right">0.42</td>
</tr>
<tr class="odd">
<td align="left">yes</td>
<td align="left">bmi &lt; 30</td>
<td align="left">$21,363.22</td>
<td align="right">0.10</td>
</tr>
<tr class="even">
<td align="left">yes</td>
<td align="left">bmi &gt;= 30</td>
<td align="left">$41,557.99</td>
<td align="right">0.11</td>
</tr>
</tbody>
</table>
<p>This was a very simple example because there were only two variables. If we have more variables, the tree will get large very quickly. This will result in overfitting; there will be good performance on the training data but poor performance on the test data.</p>
<p>The step-by-step process of building a tree is</p>
<p><strong>Step 1: Choose a variable at random.</strong></p>
<p>This could be any variable in <code>age</code>, <code>children</code>, <code>charges</code>, <code>sex</code>, <code>smoker</code>, <code>age_bucket</code>, <code>bmi</code>, or <code>region</code>.</p>
<p><strong>Step 2: Find the split point which best seperates observations out based on the value of <span class="math inline">\(y\)</span>. A good split is one where the <span class="math inline">\(y\)</span>’s are very different. * </strong></p>
<p>In this case, <code>smoker</code> was chosen. Then we can only split this in one way: <code>smoker = 1</code> or <code>smoker = 0</code>.</p>
<p>Then for each of these groups, smokers and non-smokers, choose another variable at random. In this case, for no-smokers, <code>age</code> was chosen. To find the best cut point of <code>age</code>, look at all possible age cut points from 18, 19, 20, 21, …, 64 and choose the one which best separates the data.</p>
<p>There are three ways of deciding where to split</p>
<ul>
<li><em>Entropy</em> (aka, information gain)</li>
<li><em>Gini</em></li>
<li><em>Classification error</em></li>
</ul>
<p>Of these, only the first two are commonly used. The exam is not going to ask you to calculate either of these. Just know that neither method will work better on all data sets, and so the best practice is to test both and compare the performance.</p>
<p><strong>Step 3: Continue doing this until a stopping criteria is reached. For example, the minimum number of observations is 5 or less.</strong></p>
<p>As you can see, this results in a very deep tree.</p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb211-1" title="1">tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="dt">formula =</span> charges <span class="op">~</span><span class="st">  </span>., <span class="dt">data =</span> health_insurance,</a>
<a class="sourceLine" id="cb211-2" title="2">              <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">cp =</span> <span class="fl">0.003</span>))</a>
<a class="sourceLine" id="cb211-3" title="3"><span class="kw">rpart.plot</span>(tree, <span class="dt">type =</span> <span class="dv">3</span>)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-103-1.png" width="672" /></p>
<p><strong>Step 4: Apply cost comlexity pruning to simplify the tree</strong></p>
<p>Intuitively, we know that the above model would perform poorly due to overfitting. We want to make it simpler by removing nodes. This is very similar to how in linear models we reduce complexity by reducing the number of coefficients.</p>
<p>A measure of the depth of the tree is the <em>complexity</em>. A simple way of measuring this from the number of terminal nodes, called <span class="math inline">\(|T|\)</span>. This is similar to the “degrees of freedom” in a linear model. In the above example, <span class="math inline">\(|T| = 8\)</span>. The amount of penalization is controlled by <span class="math inline">\(\alpha\)</span>. This is very similar to <span class="math inline">\(\lambda\)</span> in the Lasso.</p>
<p>Intuitively, merely only looking at the number of nodes by itself is too simple because not all data sets will have the same characteristics such as <span class="math inline">\(n\)</span>, <span class="math inline">\(p\)</span>, the number of categorical variables, correlations between variables, and so fourth. In addition, if we just looked at the error (squared error in this case) we would overfit very easily. To address this issue, we use a cost function which takes into account the error as well as <span class="math inline">\(|T|\)</span>.</p>
<p>To calculate the cost of a tree, number the terminal nodes from <span class="math inline">\(1\)</span> to <span class="math inline">\(|T|\)</span>, and let the set of observations that fall into the <span class="math inline">\(mth\)</span> bucket be <span class="math inline">\(R_m\)</span>. Then add up the squared error over all terminal nodes to the penalty term.</p>
<p><span class="math display">\[
\text{Cost}_\alpha(T) = \sum_{m=1}^{|T|} \sum_{R_m}(y_i - \hat{y}_{Rm})^2 + \alpha |T|
\]</span></p>
<p><strong>Step 5: Use cross-validation to select the best alpha</strong></p>
<p>The cost is controlled by the <code>CP</code> parameter. In the above example, did you notice the line <code>rpart.control(cp = 0.003)</code>? This is telling <code>rpart</code> to continue growing the tree until the CP reaches 0.003. At each subtree, we can measure the cost <code>CP</code> as well as the cross-validation error <code>xerror</code>.</p>
<p>This is stored in the <code>cptable</code></p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb212-1" title="1">tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="dt">formula =</span> charges <span class="op">~</span><span class="st">  </span>., <span class="dt">data =</span> health_insurance,</a>
<a class="sourceLine" id="cb212-2" title="2">              <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">cp =</span> <span class="fl">0.0001</span>))</a>
<a class="sourceLine" id="cb212-3" title="3">cost &lt;-<span class="st"> </span>tree<span class="op">$</span>cptable <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb212-4" title="4"><span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb212-5" title="5"><span class="st">  </span><span class="kw">select</span>(nsplit, CP, xerror) </a>
<a class="sourceLine" id="cb212-6" title="6"></a>
<a class="sourceLine" id="cb212-7" title="7">cost <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>()</a></code></pre></div>
<pre><code>## # A tibble: 6 x 3
##   nsplit      CP xerror
##    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1      0 0.620    1.00 
## 2      1 0.144    0.382
## 3      2 0.0637   0.240
## 4      3 0.00967  0.178
## 5      4 0.00784  0.171
## 6      5 0.00712  0.169</code></pre>
<p>As more splits are added, the cost continues to decrease, reaches a minimum, and then begins to increase.</p>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-105-1.png" width="480" /></p>
<p>To optimize performance, choose the number of splits which has the lowest error. Often, though, the goal of using a decision tree is to create a simple model. In this case, we can err or the side of a lower <code>nsplit</code> so that the tree is shorter and more interpretable. All of the questions on so far have only used decision trees for interpretability, and a different model method has been used when predictive power is needed.</p>
<p>You will typically be given the below code, which does this automatically. To get full credit on decision tree questions, mention that you used cross-validation to select the number of splits.</p>
<p>Once we have selected <span class="math inline">\(\alpha\)</span>, the tree is pruned to be shorter.</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb214-1" title="1">pruned_tree &lt;-<span class="st"> </span><span class="kw">prune</span>(tree,<span class="dt">cp =</span> tree<span class="op">$</span>cptable[<span class="kw">which.min</span>(tree<span class="op">$</span>cptable[, <span class="st">&quot;xerror&quot;</span>]), <span class="st">&quot;CP&quot;</span>])</a>
<a class="sourceLine" id="cb214-2" title="2"><span class="kw">rpart.plot</span>(pruned_tree, <span class="dt">type=</span><span class="dv">3</span>)</a></code></pre></div>
</div>
<div id="advantages-and-disadvantages-2" class="section level2">
<h2><span class="header-section-number">9.2</span> Advantages and disadvantages</h2>
<p><strong>Advantages</strong></p>
<ul>
<li>Easy to interpret</li>
<li>Captures interaction effects</li>
<li>Captures non-linearities</li>
<li>Handles continuous and categorical data</li>
<li>Handles missing values</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li>Is a “weak learner” because of low predictive power</li>
<li>Does not work on small data sets</li>
<li>Is often a simplification of the underlying process because all observations at terminal nodes have equal predicted values</li>
<li>Is biased towards selecting high-cardinality features because more possible split points for these features tend to lead to overfitting</li>
<li>High variance (which can be alleviated with stricter parameters) leads the “easy to interpret results” to change upon retraining
Unable to predict beyond the range of the training data for regression (because each predicted value is an average of training samples)</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Readings</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ISLR 8.1.1 Basics of Decision Trees</td>
<td></td>
</tr>
<tr class="even">
<td>ISLR 8.1.2 Classification Trees</td>
<td></td>
</tr>
<tr class="odd">
<td><a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf">rpart Documentation (Optional)</a></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div id="ensemble-learning" class="section level2">
<h2><span class="header-section-number">9.3</span> Ensemble learning</h2>
<p>The “wisdom of crowds” says that often many are smater than the few. In the context of modeling, the models which we have looked at so far have been single guesses; however, often the underlying process is more complex than any single model can explain. If we build separate models and then combine them, known as <em>ensembling</em>, performance can be improved. Instead of trying to create a single perfect model, many simple models, known as <em>weak learners</em> are combined into a <em>meta-model</em>.</p>
<p>The two main ways that models are combined are through <em>bagging</em> and <em>boosting</em>.</p>
<div id="bagging" class="section level3">
<h3><span class="header-section-number">9.3.1</span> Bagging</h3>
<p>To start, we create many “copies” of the training data by sampling with replacement. Then we fit a simple model, typically a decision tree or linear model, to each of the data sets. Because each model is looking at different areas of the data, the predictions are different. The final model is a weighted average of each of the individual models.</p>
</div>
<div id="boosting" class="section level3">
<h3><span class="header-section-number">9.3.2</span> Boosting</h3>
<p>Boosting always uses the original training data and iteratively fits models to the error of the prior models. These weak learners are ineffective by themselves but powerful when added together. Unlike with bagging, the computer must train these weak learners <em>sequentially</em> instead of in parallel.</p>
</div>
</div>
<div id="random-forests" class="section level2">
<h2><span class="header-section-number">9.4</span> Random Forests</h2>
<p>A random forest is the most common example of bagging. As the name implies, a forest is made up of <em>trees</em>. Seperate trees are fit to sampled datasets. For random forests, there is one minor modification: in order to make each model even more different, each tree selects a <em>random subset of variables</em>.</p>
<ol style="list-style-type: decimal">
<li>Assume that the underlying process, <span class="math inline">\(Y\)</span>, has some signal within the data <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li>Introduce randomness (variance) to capture the signal.</li>
<li>Remove the variance by taking an average.</li>
</ol>
<p>When using only a single tree, there can only be as many predictions as there are terminal nodes. In a random forest, predictions can be more granular due to the contribution of each of the trees.</p>
<p>The below graph illustrates this. A single tree (left) has stair-like, step-wise predictions whereas a random forest is free to predict any value. The color represents the predicted value (yellow = highest, black = lowest).</p>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-107-1.png" width="672" /></p>
<p>In most applications, only the <code>mtry</code> parameter needs to be tuned. Tuning the <code>ntrees</code> parameter is not required; however, the soa may still ask you to.</p>
<p>Using the basic <code>randomForest</code> package we fit a model with 500 trees.</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb215-1" title="1">rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(charges <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>bmi, <span class="dt">data =</span> health_insurance, <span class="dt">ntree =</span> <span class="dv">500</span>)</a>
<a class="sourceLine" id="cb215-2" title="2"><span class="kw">plot</span>(rf)</a></code></pre></div>
<p><img src="Exam-PA-Study-Manual_files/figure-html/unnamed-chunk-108-1.png" width="672" /></p>
<p><strong>Advantages</strong></p>
<ul>
<li>Resilient to overfitting due to bagging</li>
<li>Only one parameter to tune (mtry, the number of features considered at each split)</li>
<li>Very good a multi-class prediction</li>
<li>Nonlinearities</li>
<li>Interaction effects</li>
<li>Deal with unbalanced and missing data*Usually requires over/undersamplin</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li>Does not work on small data sets</li>
<li>Weaker performance than other methods (GBM, NN)</li>
<li>Unable to predict beyond training data for regression</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Readings</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ISLR 8.1.1 Basics of Decision Trees</td>
<td></td>
</tr>
<tr class="even">
<td>ISLR 8.1.2 Classification Trees</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div id="gradient-boosted-trees" class="section level2">
<h2><span class="header-section-number">9.5</span> Gradient Boosted Trees</h2>
<p>Another ensemble learning method is <em>gradient boosting</em>, also known as the Gradient Boosted Machine (GBM). Although this is unlikely to get significant attention on the PA exam due to the complexity, this is the most widely-used and powerful machine learning algorithms that are in use today.</p>
<p>We start with an initial model, which is just a constant prediction of the mean.</p>
<p><span class="math display">\[f = f_0(\mathbf{x_i}) = \frac{1}{n}\sum_{i=1}^ny_i\]</span></p>
<p>Then we update the target (what the model is predicting) by subtracting off the previously predicted value.</p>
<p><span class="math display">\[ \hat{y_i} \leftarrow y_i - f_0(\mathbf{x_i})\]</span></p>
<p>This <span class="math inline">\(\hat{y_i}\)</span> is called the <em>residual</em>. In our example, instead of predicting <code>charges</code>, this would be predicting the residual of <span class="math inline">\(\text{charges}_i - \text{Mean}(\text{charges})\)</span>. We now use this model for the residuals to update the prediction.</p>
<p>If we updated each prediction with the prior residual directly, the algorithm would be unstable. To make this process more gradual, we use a <em>learning rate</em> parameter.</p>
<p>At step 2, we have</p>
<p><span class="math display">\[f = f_0 + \alpha f_1\]</span></p>
<p>Then we go back and fit another weak learner to this residual and repeat.</p>
<p><span class="math display">\[f = f_0 + \alpha f_1 + \alpha f_2\]</span></p>
<p>We then iterate through this process hundreds or thousands of times, slowly improving the prediction.</p>
<p>Because each new tree is fit to <em>residuals</em> instead of the response itself, the process continuously improves the prediction. As the prediction improves, the residuals get smaller and smaller. In random forests, or other bagging algorithms, the model performance is more limited by the individual trees because each only contributes to the overall average.</p>
<p>Similarly to how GLMs can be used for classification problems through a logit transform (aka logistic regression), GBMs can also be used for classification.</p>
<p><strong>Parameters</strong></p>
<p>For random forests, the individual tree parameters do not get tuned. For GBMs, however, these parameters can make a significant difference in model performance.</p>
<p>Here are the common parameters as in the documentation for <code>?gbm</code>. In general, you should never need to memorize info that is in the documentation.</p>
<ul>
<li><p><code>n.trees</code>: Integer specifying the total number of trees to fit. This is equivalent to the number of iterations and the number of basis functions in the additive expansion. Default is 100.</p></li>
<li><p><code>shrinkage</code>: a shrinkage parameter applied to each tree in the expansion. Also known as the learning rate or step-size reduction; 0.001 to 0.1 usually work, but a smaller learning rate typically requires more trees. Default is 0.1.</p></li>
<li><p><code>interaction.depth</code>: Integer specifying the maximum depth of each tree (i.e., the highest level of variable interactions allowed). A value of 1 implies an additive model, a value of 2 implies a model with up to 2-way interactions, etc. Default is 1.</p></li>
<li><p><code>n.minobsinnode</code>: Integer specifying the minimum number of observations in the terminal nodes of the trees. Note that this is the actual number of observations, not the total weight.</p></li>
</ul>
<p>GBMs are easy to overfit, and the parameters need to be carefully tuned using cross-validation. In the Examples section we go through how to do this.</p>
<p><strong>Advantages</strong></p>
<ul>
<li>High prediction accuracy</li>
<li>Closest model to a “silver bullet” that exists</li>
<li>Nonlinearities, interaction effects, resilient to outliers, corrects for missing values</li>
<li>Deals with class imbalance directly by weighting observations</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li>Requires large sample size</li>
<li>Longer training time</li>
<li>Does not detect linear combinations of features. These must be engineered
Can overfit if not tuned correctly</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Readings</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ISLR 8.1.1 Basics of Decision Trees</td>
<td></td>
</tr>
<tr class="even">
<td>ISLR 8.1.2 Classification Trees</td>
<td></td>
</tr>
</tbody>
</table>
<div id="variable-importance" class="section level3">
<h3><span class="header-section-number">9.5.1</span> Variable importance</h3>
</div>
<div id="partial-dependence" class="section level3">
<h3><span class="header-section-number">9.5.2</span> Partial dependence</h3>
</div>
<div id="exercises-1" class="section level3">
<h3><span class="header-section-number">9.5.3</span> Exercises</h3>
<p>Run this code on your computer to answer these exercises.</p>
<p><strong>1. RF with <code>randomForest</code></strong></p>
<p>(Part 1 of 2)</p>
<p>The below code is set up to fit a random forest to the <code>soa_mortality</code> data set to predict <code>actual_cnt</code>.</p>
<p>There is a problem: all of the predictions are coming out to be 1. Find out why this is happening and fix it.</p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb216-1" title="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb216-2" title="2"><span class="co">#For the sake of this example, only take 20% of the records</span></a>
<a class="sourceLine" id="cb216-3" title="3">df &lt;-<span class="st"> </span>soa_mortality <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb216-4" title="4"><span class="st">  </span><span class="kw">sample_frac</span>(<span class="fl">0.2</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb216-5" title="5"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">target =</span> <span class="kw">as.factor</span>(<span class="kw">ifelse</span>(actual_cnt <span class="op">==</span><span class="st"> </span><span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>))) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb216-6" title="6"><span class="st">  </span><span class="kw">select</span>(target, prodcat, distchan, smoker, sex, issage, uwkey) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb216-7" title="7"><span class="st">  </span><span class="kw">mutate_if</span>(is.character, <span class="op">~</span><span class="kw">as.factor</span>(.x))</a>
<a class="sourceLine" id="cb216-8" title="8"></a>
<a class="sourceLine" id="cb216-9" title="9"><span class="co">#check that the target has 0&#39;s and 1&#39;s</span></a>
<a class="sourceLine" id="cb216-10" title="10">df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(target)</a></code></pre></div>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb217-1" title="1">index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> df<span class="op">$</span>target, <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list =</span> F)</a>
<a class="sourceLine" id="cb217-2" title="2"></a>
<a class="sourceLine" id="cb217-3" title="3">train &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(index)</a>
<a class="sourceLine" id="cb217-4" title="4">test &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>index)</a>
<a class="sourceLine" id="cb217-5" title="5"></a>
<a class="sourceLine" id="cb217-6" title="6">k =<span class="st"> </span><span class="fl">0.1</span></a>
<a class="sourceLine" id="cb217-7" title="7">cutoff=<span class="kw">c</span>(k,<span class="dv">1</span><span class="op">-</span>k) </a>
<a class="sourceLine" id="cb217-8" title="8"></a>
<a class="sourceLine" id="cb217-9" title="9">model &lt;-<span class="st"> </span><span class="kw">randomForest</span>(</a>
<a class="sourceLine" id="cb217-10" title="10">  <span class="dt">formula =</span> target <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb217-11" title="11">  <span class="dt">data =</span> train,</a>
<a class="sourceLine" id="cb217-12" title="12">  <span class="dt">ntree =</span> <span class="dv">100</span>,</a>
<a class="sourceLine" id="cb217-13" title="13">  <span class="dt">cutoff =</span> cutoff</a>
<a class="sourceLine" id="cb217-14" title="14">  )</a>
<a class="sourceLine" id="cb217-15" title="15"></a>
<a class="sourceLine" id="cb217-16" title="16">pred &lt;-<span class="st"> </span><span class="kw">predict</span>(model, test)</a>
<a class="sourceLine" id="cb217-17" title="17"><span class="kw">confusionMatrix</span>(pred, test<span class="op">$</span>target)</a></code></pre></div>
<p>(Part 2 of 2)</p>
<p>Downsample the majority class and refit the model, and then choose between the original data and the downsampled data based on the model performance. Use your own judgement when choosing how to evaluate the model based on accuracy, sensitivity, specificity, and Kappa.</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb218-1" title="1">down_train &lt;-<span class="st"> </span><span class="kw">downSample</span>(<span class="dt">x =</span> train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>target),</a>
<a class="sourceLine" id="cb218-2" title="2">                         <span class="dt">y =</span> train<span class="op">$</span>target)</a>
<a class="sourceLine" id="cb218-3" title="3"></a>
<a class="sourceLine" id="cb218-4" title="4">down_test &lt;-<span class="st"> </span><span class="kw">downSample</span>(<span class="dt">x =</span> test <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>target),</a>
<a class="sourceLine" id="cb218-5" title="5">                         <span class="dt">y =</span> test<span class="op">$</span>target)</a>
<a class="sourceLine" id="cb218-6" title="6"></a>
<a class="sourceLine" id="cb218-7" title="7">down_train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(Class)</a></code></pre></div>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb219-1" title="1">model &lt;-<span class="st"> </span><span class="kw">randomForest</span>(</a>
<a class="sourceLine" id="cb219-2" title="2">  <span class="dt">formula =</span> Class <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb219-3" title="3">  <span class="dt">data =</span> down_train,</a>
<a class="sourceLine" id="cb219-4" title="4">  <span class="dt">ntree =</span> <span class="dv">100</span>,</a>
<a class="sourceLine" id="cb219-5" title="5">  <span class="dt">cutoff =</span> cutoff</a>
<a class="sourceLine" id="cb219-6" title="6">  )</a>
<a class="sourceLine" id="cb219-7" title="7"></a>
<a class="sourceLine" id="cb219-8" title="8">down_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(model, down_test)</a>
<a class="sourceLine" id="cb219-9" title="9"><span class="kw">confusionMatrix</span>(down_pred, down_test<span class="op">$</span>Class)</a></code></pre></div>
<p>Now up-sample the minority class and repeat the same procedure.</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb220-1" title="1">up_train &lt;-<span class="st"> </span><span class="kw">upSample</span>(<span class="dt">x =</span> train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>target),</a>
<a class="sourceLine" id="cb220-2" title="2">                         <span class="dt">y =</span> train<span class="op">$</span>target)</a>
<a class="sourceLine" id="cb220-3" title="3"></a>
<a class="sourceLine" id="cb220-4" title="4">up_test &lt;-<span class="st"> </span><span class="kw">upSample</span>(<span class="dt">x =</span> test <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>target),</a>
<a class="sourceLine" id="cb220-5" title="5">                         <span class="dt">y =</span> test<span class="op">$</span>target)</a>
<a class="sourceLine" id="cb220-6" title="6"></a>
<a class="sourceLine" id="cb220-7" title="7">up_train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(Class)</a></code></pre></div>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb221-1" title="1">model &lt;-<span class="st"> </span><span class="kw">randomForest</span>(</a>
<a class="sourceLine" id="cb221-2" title="2">  <span class="dt">formula =</span> Class <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb221-3" title="3">  <span class="dt">data =</span> up_train,</a>
<a class="sourceLine" id="cb221-4" title="4">  <span class="dt">ntree =</span> <span class="dv">100</span>,</a>
<a class="sourceLine" id="cb221-5" title="5">  <span class="dt">cutoff =</span> cutoff</a>
<a class="sourceLine" id="cb221-6" title="6">  )</a>
<a class="sourceLine" id="cb221-7" title="7"></a>
<a class="sourceLine" id="cb221-8" title="8">up_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(model, up_test)</a>
<a class="sourceLine" id="cb221-9" title="9"><span class="kw">confusionMatrix</span>(up_pred, up_test<span class="op">$</span>Class)</a></code></pre></div>
<p><strong>2. RF tuning with <code>caret</code></strong></p>
<p>The best practice of tuning a model is with cross-validation. This can only be done in the <code>caret</code> library. If the SOA asks you to use <code>caret</code>, they will likely ask you a question related to cross validation as below.</p>
<p>An actuary has trained a predictive model and chosen the best hyperparameters, cleaned the data, and performed feature engineering. They have one problem, however: the error on the training data is far lower than on new, unseen test data. Read the code below and determine their problem. Find a way to lower the error on the test data <em>without changing the model or the data.</em> Explain the rational behind your method.</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb222-1" title="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb222-2" title="2">data &lt;-<span class="st"> </span>health_insurance <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample_n</span>(<span class="dv">1000</span>) <span class="co">#Uncomment this when completing this exercise</span></a>
<a class="sourceLine" id="cb222-3" title="3"></a>
<a class="sourceLine" id="cb222-4" title="4">index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> data<span class="op">$</span>charges, <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list =</span> F) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.numeric</span>()</a>
<a class="sourceLine" id="cb222-5" title="5">train &lt;-<span class="st">  </span>health_insurance <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(index)</a>
<a class="sourceLine" id="cb222-6" title="6">test &lt;-<span class="st"> </span>health_insurance <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>index)</a>
<a class="sourceLine" id="cb222-7" title="7"></a>
<a class="sourceLine" id="cb222-8" title="8">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(</a>
<a class="sourceLine" id="cb222-9" title="9">  <span class="dt">method=</span><span class="st">&#39;boot&#39;</span>, </a>
<a class="sourceLine" id="cb222-10" title="10">  <span class="dt">number=</span><span class="dv">2</span>, </a>
<a class="sourceLine" id="cb222-11" title="11">  <span class="dt">p =</span> <span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb222-12" title="12"></a>
<a class="sourceLine" id="cb222-13" title="13">tunegrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">.mtry=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>))</a>
<a class="sourceLine" id="cb222-14" title="14">rf &lt;-<span class="st"> </span><span class="kw">train</span>(charges <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb222-15" title="15">            <span class="dt">data =</span> train,</a>
<a class="sourceLine" id="cb222-16" title="16">            <span class="dt">method=</span><span class="st">&#39;rf&#39;</span>, </a>
<a class="sourceLine" id="cb222-17" title="17">            <span class="dt">tuneGrid=</span>tunegrid, </a>
<a class="sourceLine" id="cb222-18" title="18">            <span class="dt">trControl=</span>control)</a>
<a class="sourceLine" id="cb222-19" title="19"></a>
<a class="sourceLine" id="cb222-20" title="20">pred_train &lt;-<span class="st"> </span><span class="kw">predict</span>(rf, train)</a>
<a class="sourceLine" id="cb222-21" title="21">pred_test &lt;-<span class="st"> </span><span class="kw">predict</span>(rf, test)</a>
<a class="sourceLine" id="cb222-22" title="22"></a>
<a class="sourceLine" id="cb222-23" title="23">get_rmse &lt;-<span class="st"> </span><span class="cf">function</span>(y, y_hat){</a>
<a class="sourceLine" id="cb222-24" title="24">  <span class="kw">sqrt</span>(<span class="kw">mean</span>((y <span class="op">-</span><span class="st"> </span>y_hat)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb222-25" title="25">}</a>
<a class="sourceLine" id="cb222-26" title="26"></a>
<a class="sourceLine" id="cb222-27" title="27"><span class="kw">get_rmse</span>(pred_train, train<span class="op">$</span>charges)</a>
<a class="sourceLine" id="cb222-28" title="28"><span class="kw">get_rmse</span>(pred_test, test<span class="op">$</span>charges)</a></code></pre></div>
<div id="tuning-a-gbm-with-caret" class="section level4">
<h4><span class="header-section-number">9.5.3.1</span> Tuning a GBM with <code>caret</code></h4>
<p><a href="https://rpubs.com/phamdinhkhanh/389752" class="uri">https://rpubs.com/phamdinhkhanh/389752</a></p>
<p>If the SOA asks you to tune a GBM, they will need to give you starting hyperparameters which are close to the “best” values due to how slow the Prometric computers are. Another possibility is that they pre-train a GBM model object and ask that you use it.</p>
<p>This example looks at 27 combinations of hyper parameters.</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb223-1" title="1">tunegrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(</a>
<a class="sourceLine" id="cb223-2" title="2">    <span class="dt">interaction.depth =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>),</a>
<a class="sourceLine" id="cb223-3" title="3">                    <span class="dt">n.trees =</span> <span class="kw">c</span>(<span class="dv">20</span>, <span class="dv">40</span>, <span class="dv">100</span>), </a>
<a class="sourceLine" id="cb223-4" title="4">                    <span class="dt">shrinkage =</span> <span class="kw">c</span>(<span class="fl">0.01</span>, <span class="fl">0.001</span>, <span class="fl">0.0001</span>),</a>
<a class="sourceLine" id="cb223-5" title="5">                    <span class="dt">n.minobsinnode =</span> <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb223-6" title="6"><span class="kw">nrow</span>(tunegrid)</a></code></pre></div>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb224-1" title="1">gbm &lt;-<span class="st"> </span><span class="kw">train</span>(charges <span class="op">~</span><span class="st"> </span>bmi <span class="op">+</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>sex <span class="op">+</span><span class="st"> </span>region,</a>
<a class="sourceLine" id="cb224-2" title="2">            <span class="dt">data =</span> health_insurance_train,</a>
<a class="sourceLine" id="cb224-3" title="3">            <span class="dt">method=</span><span class="st">&#39;gbm&#39;</span>, </a>
<a class="sourceLine" id="cb224-4" title="4">            <span class="dt">tuneGrid=</span>tunegrid, </a>
<a class="sourceLine" id="cb224-5" title="5">            <span class="dt">trControl=</span>control,</a>
<a class="sourceLine" id="cb224-6" title="6">            <span class="dt">verbose =</span> FALSE<span class="co">#Switch this to TRUE to see useful output</span></a>
<a class="sourceLine" id="cb224-7" title="7">            )</a></code></pre></div>
<p>The output shows the RMSE for each of the 27 models tested.</p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb225-1" title="1"><span class="kw">trellis.par.set</span>(<span class="kw">caretTheme</span>())</a>
<a class="sourceLine" id="cb225-2" title="2"><span class="kw">plot</span>(gbm)</a></code></pre></div>
<p>The <code>summary.gbm</code> function, which R understands from the <code>summary</code> function, shows the variable importance. The most predictive feature is <code>age</code>, followed by <code>bmi</code>.</p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb226-1" title="1"><span class="kw">summary</span>(gbm, <span class="dt">plotit =</span> F) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tibble</span>()</a></code></pre></div>
</div>
<div id="solutions" class="section level4">
<h4><span class="header-section-number">9.5.3.2</span> Solutions</h4>
<ol style="list-style-type: decimal">
<li><p>Part 1 of 2: Set the cutoff to be lower (i.e., 0.1)</p></li>
<li><p>Part 2 of 2: The Accuracy and Kappa are higher (better) for the undersampled vs. the original data.The up-sampled model is best. With a cutoff of 0.1, accuracy is 0.66 and Kapp 0.32</p></li>
<li><p>(Hint) Look at <code>?trainControl</code></p></li>
</ol>
<p>A bootstrap sample is when samples are taken of the observations with replacement. The default code was using <code>method = boot</code>, which is using a boot strap. It is strange that this only repeated twice, and each time the data is only train on <code>p = 0.2</code> percent (twenty percent!) of the training data. Because the training data is small, <code>n</code> is effectively smaller, which makes the model worse. Additionally, because there are only two bootstrap replications, there is high variance in this estimate.</p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb227-1" title="1">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(</a>
<a class="sourceLine" id="cb227-2" title="2">  <span class="dt">method=</span><span class="st">&#39;boot&#39;</span>, </a>
<a class="sourceLine" id="cb227-3" title="3">  <span class="dt">number=</span><span class="dv">2</span>, </a>
<a class="sourceLine" id="cb227-4" title="4">  <span class="dt">p =</span> <span class="fl">0.2</span>)</a></code></pre></div>
<p>This is what the default error is</p>
<p>TRAIN 2581.203
TEST 5284.176</p>
<p>A good though is to increase the number of bootstrap samples to 10 and to increase the training percentage size to the default of 0.75. This, however, results in <strong>worse</strong> performance on the test set.</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb228-1" title="1">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(</a>
<a class="sourceLine" id="cb228-2" title="2">  <span class="dt">method=</span><span class="st">&#39;boot&#39;</span>, </a>
<a class="sourceLine" id="cb228-3" title="3">  <span class="dt">number=</span><span class="dv">10</span>, </a>
<a class="sourceLine" id="cb228-4" title="4">  <span class="dt">p =</span> <span class="fl">0.75</span>)</a></code></pre></div>
<p>TRAIN 2580.682
TEST 5286.089</p>
<p>The <strong>best</strong> method is to use repeated cross-validation. This makes the most use of the data by training over 9-10ths of the observations and evaluating the error on the remaining 1/10th. <strong>This estimate of the error is important because it’s how <code>caret</code> selects the optimal number of trees to use in the random forest.</strong></p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb229-1" title="1">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(</a>
<a class="sourceLine" id="cb229-2" title="2">  <span class="dt">method=</span><span class="st">&#39;repeatedcv&#39;</span>, </a>
<a class="sourceLine" id="cb229-3" title="3">  <span class="dt">number=</span><span class="dv">10</span>, </a>
<a class="sourceLine" id="cb229-4" title="4">  <span class="dt">p =</span> <span class="fl">0.8</span>)</a></code></pre></div>
<p>TRAIN 2600.809
TEST 5280.758</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="a-mini-exam-example.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/06-tree-based-models.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["Exam-PA-Study-Manual.pdf", "Exam-PA-Study-Manual.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
